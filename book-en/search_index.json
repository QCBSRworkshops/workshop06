[["index.html", "Workshop 6: Generalized linear models QCBS R Workshop Series Preface 0.1 Code of conduct 0.2 Contributors 0.3 Contributing", " Workshop 6: Generalized linear models QCBS R Workshop Series Developed and maintained by the contributors of the QCBS R Workshop Series1 2023-01-13 16:43:19 Preface The QCBS R Workshop Series is a series of 10 workshops that walks participants through the steps required to use R for a wide array of statistical analyses relevant to research in biology and ecology. These open-access workshops were created by members of the QCBS both for members of the QCBS and the larger community. The content of this workshop has been peer-reviewed by various QCBS members. If you would like to suggest modifications, please contact the current series coordinators, listed on the main Github page. 0.1 Code of conduct The QCBS R Workshop Series and the QCBS R Symposium are venues dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. Participants, presenters and organizers of the workshop series and other related activities accept this Code of Conduct when being present at any workshop-related activities. We do not tolerate behaviour that is disrespectful or that excludes, intimidates, or causes discomfort to others. We do not tolerate discrimination or harassment based on characteristics that include, but are not limited to, gender identity and expression, sexual orientation, disability, physical appearance, body size, citizenship, nationality, ethnic or social origin, pregnancy, familial status, genetic information, religion or belief (or lack thereof), membership of a national minority, property, age, education, socio-economic status, technical choices, and experience level. It applies to all spaces managed by or affiliated with the workshop, including, but not limited to, workshops, email lists, and online forums such as GitHub, Slack and Twitter. 0.1.1 Expected behaviour All participants are expected to show respect and courtesy to others. All interactions should be professional regardless of platform: either online or in-person. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all workshop events and platforms: Use welcoming and inclusive language; Be respectful of different viewpoints and experiences; Gracefully accept constructive criticism; Focus on what is best for the community; Show courtesy and respect towards other community members. 0.1.2 Unacceptable behaviour Examples of unacceptable behaviour by participants at any workshop event/platform include: written or verbal comments which have the effect of excluding people on the - basis of membership of any specific group; causing someone to fear for their safety, such as through stalking or intimidation; violent threats or language directed against another person; the display of sexual or violent images; unwelcome sexual attention; nonconsensual or unwelcome physical contact; insults or put-downs; sexist, racist, homophobic, transphobic, ableist, or exclusionary jokes; incitement to violence, suicide, or self-harm; continuing to initiate interaction (including photography or recording) with - someone after being asked to stop; publication of private communication without consent. 0.2 Contributors Originally developed by: Cédric Frenette Dussault, Vincent Fugère, Thomas Lamy, Zofia Taranu Contributed with changes to the presentation and/or the written material: 2022 - 2021 - 2020 Laurie Maynard Pedro Henrique P. Braga Katherine Hébert Alex Arkilanian Mathieu Vaillancourt Esteban Góngora 2019 - 2018 - 2017 Azenor Bideault Willian Vieira Pedro Henrique P. Braga Marie Hélène Brice Kevin Cazelles 2016 - 2015 - 2014 Cédric Frenette Dussault Thomas Lamy Zofia Taranu Vincent Fugère Contributed by reporting issues and suggesting modifications: 0.3 Contributing The QCBS R Workshop Series provides a venue for graduate students and postdoctoral fellows to improve their teaching and learning abilities through the instruction and the development of statistical and programming workshops. We immensely value the contributions provided to the series, and we rely on the QCBS community’s engagement to constantly improve our workshops. To encourage community contributions, we keep our workshops openly accessible, reproducible, and encourage collaboration between contributors. The QCBS offers Learning and Development Awards (LeaDA) to support graduates student and post-doctoral QCBS members who instruct or develop workshops. We also welcome voluntary community feedback to keep the workshops up to date and as helpful as possible! For more information about contributing, view our Presentation and Development Protocol. The QCBS R Workshop Series is part of the Québec Centre for Biodiversity Science, and is maintained by the series coordinators and graduent student, postdoctoral, and research professional members. The contributors for this workshop can be accessed here.↩︎ "],["learning-objectives.html", "Chapter 1 Learning objectives", " Chapter 1 Learning objectives A significant limitation of general linear models is that they cannot accommodate response variables that do not have a normal error distribution - a situation that is very common when analyzing biological data. In this workshop, you will learn how to use generalized linear models, which are powerful tools to overcome some of the distributional assumptions of linear models. Specifically, we will: Distinguish generalized linear models from general linear models (including many of their equations!). Identify situations for when the use of generalized linear models is appropriate. Test assumptions for generalized linear models. Implement and execute generalized linear models in binary, proportion and count data. Validate, interpret and visualise results of generalized linear models. "],["preparing-for-the-workshop.html", "Chapter 2 Preparing for the workshop", " Chapter 2 Preparing for the workshop To prepare for this workshop, you must download and install the earliest RStudio and R versions. You must also download the data we will use during this workshop: Mites Faramea This workshop requires the following R packages: ggplot2 MASS vcdExtra bbmle DescTools To install them from CRAN, run: install.packages(c(&#39;ggplot2&#39;, &#39;MASS&#39;, &#39;ggpmisc&#39;, &#39;vcdExtra&#39;, &#39;bbmle&#39;, &#39;DescTools&#39;, &#39;GlmSimulatoR&#39;, &#39;cplm&#39;) ) ## also installing the dependencies &#39;timechange&#39;, &#39;ggpp&#39;, &#39;confintr&#39;, &#39;polynom&#39;, &#39;lmodel2&#39;, &#39;splus2R&#39;, &#39;lubridate&#39; To load these packages, run: library(ggplot2) library(MASS) library(vcdExtra) library(bbmle) library(DescTools) library(GlmSimulatoR) library(cplm) library(ggpmisc) This workshop will build up your knowledge on models along side with some of their important equations. Do not run to the mountains! This will be achieved progressively and you will feel proud of yourself after! "],["reviewing-linear-models.html", "Chapter 3 Reviewing linear models 3.1 General linear models 3.2 An example with general linear models", " Chapter 3 Reviewing linear models Much of our research focuses on investigating how patterns we observe can be explained by predictive variables. We are often looking for a function \\(f\\) that can explain a response variable ( \\(Y\\) ) in terms of one ( \\(X_1\\) ) or many other predictors ( \\(X_2\\), \\(X_3\\), \\(...\\) , \\(X_n\\) ): \\[Y = f(X_1)\\] The combination of predictive variables we have sampled will never fully explain \\(Y\\). Because of this, there is always unpredictable disturbance in our models, i.e. the error \\(\\epsilon\\). As such, the error is an irrevocable part of our function: \\[Y = f(X_1, \\epsilon)\\] In Workshop 4, we have learned how to use general linear models as \\(f(\\cdot)\\) to describe the relationship between variables. They were: the \\(t\\)-test, the analysis of variance (or, ANOVA), the linear regression (both simple, with one predictor, and multiple, with more than one predictor), and the analysis of covariance (ANCOVA). 3.1 General linear models 3.1.1 Definition The general form of our function \\(Y = f(X_1)\\) as a linear function can be represented by: \\[Y = \\beta_0 + \\beta_1X_i + \\varepsilon\\] where: \\(Y_i\\) is the predicted value of a response variable \\(\\beta_0\\) is the unknown coefficient intercept \\(\\beta_1\\) is the unknown coefficient slope \\(X_i\\) is the value for the explanatory variable \\(\\varepsilon_i\\) is the model residual drawn from a normal distribution with a varying mean but a constant variance. 3.1.2 Assumptions Linear models only produce unbiased estimators (i.e. are only reliable) if they follow certain assumptions. Most importantly: 1. The population can be described by a linear relationship: \\[Y = \\beta_0 + \\beta_1X_i + \\varepsilon\\] 2. The error term \\(\\varepsilon\\) has the same variance given any value of the explanatory variable (i.e. homoskedasticity), and the error terms are not correlated across observations (i.e. no autocorrelation): \\[\\mathbb{V}{\\rm ar} (\\epsilon_i | \\mathbf{X} ) = \\sigma^2_\\epsilon,\\ \\forall i = 1,..,N\\] and, \\[\\mathbb{C}{\\rm ov} (\\epsilon_i, \\epsilon_j) = 0,\\ i \\neq j\\] 3. And, the residuals are normal: \\[\\boldsymbol{\\varepsilon} | \\mathbf{X} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\sigma^2_\\epsilon \\mathbf{I} \\right)\\] The estimations of general linear models as in \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\) assumes that data is generated following these assumptions. 3.2 An example with general linear models Let us simulate 250 observations which satisfies our assumptions: \\(\\epsilon_i \\sim \\mathcal{N}(0, 2^2), i = 1,...,250\\). nSamples &lt;- 250 ID &lt;- factor(c(seq(1:nSamples))) PredVar &lt;- runif(nSamples, min = 0, max = 50) simNormData &lt;- data.frame( ID = ID, PredVar = PredVar, RespVar = (2*PredVar + rnorm(nSamples, mean = 0, sd = 2) ) ) # We have learned how to use lm() lm.simNormData &lt;- lm(RespVar ~ PredVar, data = simNormData) We can plot the result of our lm() model to produce diagnostic figures for our model: layout( matrix(c(1, 2, 3, 4), 2, 2) ) plot(lm.simNormData) These graphs allow one to assess how assumptions of linearity and homoscedasticity are being met. Briefly, : The Q-Q plot allows the comparison of the residuals to “ideal” normal observations; The scale-location plot (square rooted standardized residual vs. predicted value) is useful for checking the assumption of homoscedasticity; Cook’s Distance, which is a measure of the influence of each observation on the regression coefficients and helps identify outliers. Residuals are \\(Y-\\widehat{Y}\\), or the observed value minus the predicted value. Outliers are observations \\(Y\\) with large residuals, i.e. the observed value \\(Y\\) for a point \\(X\\) is very different from the one predicted by the regression model \\(\\widehat{Y}\\). A leverage point is defined as an observation \\(Y\\) that has a value of \\(x\\) that is far away from the mean of \\(x\\). An influential observation is defined as an observation \\(Y\\) that changes the slope of the line \\(\\beta_1\\). Thus, influential points have a large influence on the fit of the model. One method to find influential points is to compare the fit of the model with and without each observation. "],["example-with-real-data.html", "Chapter 4 Example with real data", " Chapter 4 Example with real data Let us use our prior knowledge on general linear models to explore the relationship between variables within the Oribatid mite data set. Let us begin by loading this data set into R: # Use setwd() to set your working directory mites &lt;- read.csv(&#39;data/mites.csv&#39;, stringsAsFactors = TRUE) The dataset that you just loaded is a subset from the classic Oribatid mites (Acari,Oribatei), which has been used in numerous texts (e.g. Borcard, Gillet &amp; Legendre, Numerical Ecology with R), and which is available in the vegan library. The Oribatid mite dataset has 70 observations with moss and mite samples collected at the Station de Biologie from the Université de Montréal. ], within the municipality of Saint-Hippolyte, Québec (Canada). Each sample includes 5 variables of environmental measurements and abundance for Galumna sp. for each site. We can peek into the structure and the first six rows of the dataset using the head() and str() functions: head(mites) ## Galumna pa totalabund prop SubsDens WatrCont Substrate Shrub Topo ## 1 8 1 140 0.057142857 39.18 350.15 Sphagn1 Few Hummock ## 2 3 1 268 0.011194030 54.99 434.81 Litter Few Hummock ## 3 1 1 186 0.005376344 46.07 371.72 Interface Few Hummock ## 4 1 1 286 0.003496503 48.19 360.50 Sphagn1 Few Hummock ## 5 2 1 199 0.010050251 23.55 204.13 Sphagn1 Few Hummock ## 6 1 1 209 0.004784689 57.32 311.55 Sphagn1 Few Hummock str(mites) ## &#39;data.frame&#39;: 70 obs. of 9 variables: ## $ Galumna : int 8 3 1 1 2 1 1 1 2 5 ... ## $ pa : int 1 1 1 1 1 1 1 1 1 1 ... ## $ totalabund: int 140 268 186 286 199 209 162 126 123 166 ... ## $ prop : num 0.05714 0.01119 0.00538 0.0035 0.01005 ... ## $ SubsDens : num 39.2 55 46.1 48.2 23.6 ... ## $ WatrCont : num 350 435 372 360 204 ... ## $ Substrate : Factor w/ 7 levels &quot;Barepeat&quot;,&quot;Interface&quot;,..: 4 3 2 4 4 4 4 2 3 4 ... ## $ Shrub : Factor w/ 3 levels &quot;Few&quot;,&quot;Many&quot;,&quot;None&quot;: 1 1 1 1 1 1 1 2 2 2 ... ## $ Topo : Factor w/ 2 levels &quot;Blanket&quot;,&quot;Hummock&quot;: 2 2 2 2 2 2 2 1 1 2 ... Our first glance into the dataset already allows us to separate potential response variables from potential predictors: Response variables: Occurrence: pa Abundance: Galumna Relative Frequency or Proportion: prop Predictive variables: Substract Density: SubsDens Water Content: WatrCont Substrate: Substrate Shrubs Nearby: Shrub Topography: Topo We can also already elaborate an initial question: Could the abundance, occurrence or proportion of Galumna sp. be predicted by environmental features? To answer this question, we can think of a variety of functions: \\(\\text{Abundance} = f(\\text{Water content}, \\epsilon)\\) \\(\\text{Proportion} = f(\\text{Water content}, \\epsilon)\\) \\(\\text{Occurrence} = f(\\text{Substrate}, \\epsilon)\\) \\(\\text{Abundance} = f(\\text{Topography}, \\epsilon)\\) \\(\\text{Occurrence} = f(\\text{Shrubs Nearby}, \\epsilon)\\) \\(\\text{Relative Frequency} = f(\\text{Topography}, \\epsilon)\\) \\(\\text{Occurrence} = f(\\text{Substract Density}, \\epsilon)\\) \\(\\text{Abundance} = f(\\text{Substrate}, \\epsilon)\\) Can we see a relationship between Galumna and any of the five environmental variables? Let us attempt to be more specific and ask whether he composition of Galumna’s communities (abundance, occurrence and relative frequency) vary as a function of water content?. We can begin by representing all three response variables against the predictor: plot(Galumna ~ WatrCont, data = mites, xlab = &#39;Water content&#39;, ylab = &#39;Abundance&#39;) boxplot(WatrCont ~ pa, data = mites, xlab=&#39;Presence/Absence&#39;, ylab = &#39;Water content&#39;) plot(prop ~ WatrCont, data = mites, xlab = &#39;Water content&#39;, ylab = &#39;Proportion&#39;) Indeed, Galumna seems to vary negatively as a function of WatrCont, i.e. Galumna sp. seems to prefer dryer sites. We can go step further and fit general linear models to test whether Galumna, pa, or prop vary as a function of WatrCont using the lm() function: # Fit the models ## # Abundance model lm.abund &lt;- lm(Galumna ~ WatrCont, data = mites) ## # Presence-absence model lm.pa &lt;- lm(pa ~ WatrCont, data = mites) ## # Proportion model lm.prop &lt;- lm(prop ~ WatrCont, data = mites) Then, we can check the model output to verify whether these relationships are statistically significant: # Check the model output with the summary() function summary(lm.abund) ## ## Call: ## lm(formula = Galumna ~ WatrCont, data = mites) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7210 -0.8236 -0.3270 0.3910 6.6772 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.439349 0.555825 6.188 3.98e-08 *** ## WatrCont -0.006045 0.001280 -4.723 1.21e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.514 on 68 degrees of freedom ## Multiple R-squared: 0.247, Adjusted R-squared: 0.2359 ## F-statistic: 22.31 on 1 and 68 DF, p-value: 1.206e-05 summary(lm.pa) ## ## Call: ## lm(formula = pa ~ WatrCont, data = mites) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61320 -0.30889 -0.05498 0.30247 0.80073 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1892142 0.1431306 8.309 6.03e-12 *** ## WatrCont -0.0020263 0.0003296 -6.148 4.68e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3897 on 68 degrees of freedom ## Multiple R-squared: 0.3573, Adjusted R-squared: 0.3478 ## F-statistic: 37.8 on 1 and 68 DF, p-value: 4.677e-08 summary(lm.prop) ## ## Call: ## lm(formula = prop ~ WatrCont, data = mites) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.010208 -0.004927 -0.002056 0.003240 0.049252 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.020e-02 3.294e-03 6.133 4.98e-08 *** ## WatrCont -3.516e-05 7.586e-06 -4.635 1.67e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.008971 on 68 degrees of freedom ## Multiple R-squared: 0.2401, Adjusted R-squared: 0.2289 ## F-statistic: 21.49 on 1 and 68 DF, p-value: 1.665e-05 # Extracting the Pr(&gt;|t|) summary(lm.abund)$coefficients[, 4] ## (Intercept) WatrCont ## 3.981563e-08 1.206117e-05 summary(lm.pa)$coefficients[, 4] ## (Intercept) WatrCont ## 6.030252e-12 4.676755e-08 summary(lm.prop)$coefficients[, 4] ## (Intercept) WatrCont ## 4.977432e-08 1.665437e-05 For now, yes, there is a strong and significant relationship for all 3 response variables! But, wait a minute… We are forgetting something important here! What about the assumptions of linear models? "],["recalling-linear-models-assumptions.html", "Chapter 5 Recalling linear models: assumptions", " Chapter 5 Recalling linear models: assumptions Let us validate these models to confirm that the assumptions of linear models are being respected, starting with the abundance model. # Plot the abundance model plot(Galumna ~ WatrCont, data = mites) abline(lm.abund) The model does not fit well. It predicts negative abundance values when WatrCont exceeds 600, which does not make any sense. Also, the model does poorly at predicting high abundance values at low values of WatrCont. We can also check the model diagnostic plots: # Diagnostic plots plot(lm.abund) Diagnostic plots show that the data/model violate assumptions of homogeneity of variance (the graph on the left shows that residuals are larger at higher fitted values) and normality (the graph on the right indicates that residuals are not distributed as expected from a normal distribution, i.e. many points are far from predicted values given by the dotted line). Therefore, we need to reject this model, and cannot use it to conclude that Galumna abundances vary as a function of water content. The diagnostic plots for the presence-absence model and the proportion model also indicate that these models are also inappropriate: # Plot the proportion model plot(prop ~ WatrCont, data = mites) abline(lm.prop) # Diagnostic plots plot(lm.prop) # Plot the presence/absence model plot(pa ~ WatrCont, data = mites) abline(lm.pa) # Diagnostic plots plot(lm.pa) Let us take a step back here and review the assumptions of linear models, and where they come from. Remember our simple linear model? \\[Y_i = \\beta_0 + \\beta_1X_i + \\varepsilon\\] The last entry \\(\\varepsilon_i\\) is important. This is where assumptions of normality and homoscedasticity originate. In linear models, the residuals \\(\\varepsilon_i\\) (the distance between each observation and the regression line) can be predicted by drawing random values from a normal distribution. Recall that all normal distributions have two parameters, \\(\\mu\\) (the mean of the distribution) and \\(\\sigma^2\\) (the variance of the distribution). In a linear model, \\(\\mu\\) changes based on values of \\(X\\) (the predictor variable), but \\(\\sigma^2\\) has the same value for all values of \\(Y\\). Our simple linear can also be written as this: \\[Y_i \\sim N(\\mu = \\beta_0 + \\beta_1 X_i +\\varepsilon, \\sigma^2)\\] with \\(N(\\cdot)\\) meaning that \\(Y_i\\) is drawn from a normal distribution with parameters \\(\\mu\\) (mean; which depends on \\(x_i\\)) and \\(\\sigma\\) (variance; which has the same value for all \\(Y_i\\)s). Let us take a look at what happens to the distribution when we vary the parameters \\(\\mu\\) and \\(\\sigma\\). Varying \\(\\mu\\) while \\(\\sigma = 5\\) shifts the mean of the distribution. If we keep \\(\\mu = 25\\), varying \\(\\sigma\\) changes the shape of the distribution, where smaller \\(\\sigma\\) (low variance) means there is higher probability around the mean, while larger \\(\\sigma\\) spreads out the probabilities across the full range of values. "],["model-prediction.html", "Chapter 6 Model prediction", " Chapter 6 Model prediction When the assumptions of the linear model are not met, model prediction becomes problematic. Let us work through an example to demonstrate some of the problems that arise from a poorly fitted model. Remember that when we predict, we aim at estimating the unknown coefficients \\(\\beta_0\\) and \\(\\beta_1\\) so that a line effectively predicting every value of \\(Y\\) as a function of \\(X\\) can be drawn! \\[Y_i \\sim N(\\mu = \\beta_0 + \\beta_1 X_i +\\varepsilon, \\sigma^2)\\] Let us predict Galumna abundance at a water content = 300 using the linear model we fit above. What are the parameters of the normal distribution used to model \\(Y\\) when water content is \\(300\\)? Let us begin by obtaining the parameters \\(\\mu\\) and \\(\\sigma^2\\) for a normal distribution corresponding to our equation. To obtain the coefficients from our models, we can use the function coef(): # Extract model coefficients coef(lm.abund) ## (Intercept) WatrCont ## 3.439348672 -0.006044788 These coefficients would allow us to predict Galumna abundance if there was no error. However, we know that error is an irrevocable part of our model. To get our predicted values, we therefore also need to add . This is where we use the normal distribution! For \\(X\\) = 300, our model predicts that should follow a normal distribution with mean = 1.63. We can extract the variance (\\(\\sigma^2\\)) from our model summary: # Extract variance from the model summary summary(lm.abund)$sigma ## [1] 1.513531 We can plug these values into the equation we just discussed above: \\[Y_i \\sim N(\\mu = \\beta_0 + \\beta_1 X_i +\\varepsilon, \\sigma^2)\\] \\(\\mu = 3.44 + (-0.006 \\times 300) = 1.63\\) \\(\\sigma^2 = 1.51\\) This tells us that randomly drawn \\(Y\\) values when water content is \\(300\\) should be on average \\(1.63\\) and have a variance of \\(1.51\\). In other words, at \\(x = 300\\), residuals should follow a normal distribution with \\(\\mu = 1.63\\) and \\(\\sigma^2 = 1.51\\). At \\(x = 400\\), we get \\(\\mu = 1.02\\) and \\(\\sigma^2 = 1.51\\), etc. At a water content of 400, residuals should follow a normal distribution with parameters \\(\\mu = 3.44 + (-0.006 x 400) = 1.02\\) and \\(\\sigma^2 = 1.51\\), and thereon. Each \\(Y\\) value is modeled using a normal distribution with a mean that depends on \\(X_i\\), but with a variance that is constant \\(\\sigma^2 = 1.51\\) across all \\(X_i\\) values. We can represent these expectations like this: The four normal distributions on this graph represent the probability of observing a given Galumna abundance for 4 different water content values. The mean of the normal distribution varies as a function of water content (hence \\(\\mu\\) decreases with water content), but \\(\\sigma^2\\) is always 1.51 (i.e. the variance is homogeneous across all values of \\(X\\)). Our model thus expects observations to fall within the following shaded areas. However, what happens if we add our observations to this plot? This model seems inappropriate (!) for at least two reasons: 1. Values are on average further from the regression line at low water content values. That is, there is more residual variance around the predicted values for low values of \\(X\\), such that \\(\\varepsilon\\) varies as a function of \\(X\\), thus violating the assumption of homoscedasticity. It makes no sense to use a constant value of \\(\\sigma^2\\): the normal distributions used to predict \\(Y\\) at low values of \\(X\\) should ideally be wider (have a larger \\(\\sigma^2\\)) than normal distributions used to predict \\(Y\\) at large \\(X\\) values, but linear models do not permit this. 2. The residuals do not follow a normal distribution with constant variance across all values of \\(X\\). The variance of the residuals’ distribution changes as a function of \\(X\\) (see the spread of the data points around the trend line!). 3. The predicted values do not make sense, given the observations. Our response variable is abundance, which can only take integer values. Yet, when water content is 300, the abundance value that our model predicts to be the most probable is 1.63! We know that the probability of observing 1.63 individuals at water content = 300 is actually zero, as is the probability of observing any fraction (non-integers). Our predicted values should be modelled using a distribution that only predicts integers, rather than a continuous distribution like the normal distribution. This is a very common problem, as biological data often follows one of the myriad other statistical distributions besides the normal distribution. 6.0.0.1 So, what do we do now? Transform our data? Very often, data will not “behave” and will violate the assumptions we have seen, showing evidence for non-normality and/or heteroskedasticity. We have been told to transform our data using logarithmic, square-root, and cosine transformations to get around these problems. Unfortunately, transformations not always work and come with a few drawbacks: 1. They change the response variable (!), making interpretation challenging; 2. They may not simultaneously improve linearity and homogeneity of variance; 3. The boundaries of the sample space change. For instance, our simple linear model: \\[Y_i = \\beta_0 + \\beta_1X_i + \\varepsilon\\] looks like this after the log-transformation of \\(Y\\): \\[E(\\log{Y_i}) = \\beta_0 + \\beta_1X_i\\] It is, of course, much less intuitive to interpret that for every \\(300\\) units increase in water content, Galumna abundance takes the form of \\(\\log(1.63)\\)… Thankfully, the normal distribution is not our only option! "],["the-distributions-of-biological-data.html", "Chapter 7 The distributions of biological data", " Chapter 7 The distributions of biological data Statisticians have described a multitude of distributions that correspond to different types of data. A distribution provides the probability of observing each possible outcome of an experiment or survey (for example, abundance = 8 Galumna is one such “outcome” of a survey). Discrete distributions have a range that only includes integers, while continuous distributions have a range that also includes fractions (the normal distribution is an example of a continuous distribution). All distributions have parameters that dictate the shape of the distribution (for example \\(\\mu\\) and \\(\\sigma^2\\) for the normal distribution). For a good overview of statistical distributions, we recommend that you refer to Chapter 4 in Ben Bolker’s Ecological Models and Data in R. Here, we briefly discuss a few distributions that are useful for ecologists and generalized linear modeling. We have already seen that our response variable “Galumna abundance” can only take integer values. Abundance, therefore, follows a discrete distribution, with no fractions in its range. A useful distribution to model abundance data is the Poisson distribution, named after Siméon Denis Poisson. The Poisson distribution is a discrete distribution with a single parameter, \\(\\lambda\\) (lambda), which defines both the mean and the variance of the distribution (i.e. the mean and the variance of a Poisson distribution are equal). Here are 3 examples of Poisson distributions with different values of \\(\\lambda\\), corresponding in this case to the mean number of Galumna observed in a fictive set of samples: # examples of Poisson distributions with different values of lambda par(cex = 2) x = seq(1, 50, 1) plot(x, dpois(x, lambda = 1), type = &quot;h&quot;, lwd = 3, xlab = &quot;Frequency of Galumna&quot;, ylab = &quot;Probability&quot;, main = &quot;lambda = 1&quot;) plot(x, dpois(x, lambda = 10), type = &quot;h&quot;, lwd = 3, xlab = &quot;Frequency of Galumna&quot;, ylab = &quot;Probability&quot;, main = &quot;lambda = 10&quot;) plot(x, dpois(x, lambda = 30), type = &quot;h&quot;, lwd = 3, xlab = &quot;Frequency of Galumna&quot;, ylab = &quot;Probability&quot;, main = &quot;lambda = 30&quot;) Note that at low \\(\\lambda\\) values (when the mean is close to zero), the distribution is skewed to the right, while at large \\(\\lambda\\) values (large mean) the distribution is symmetrical. The variance increases with the mean, predicted values are always integers, and the range of a Poisson distribution is always strictly positive; all of these properties are useful to model count data, for example abundance of a given taxon, number of seeds in a plot. Our variable mites $Galumna seems to follow a Poisson distribution with a low value of \\(\\lambda\\) (indeed, if we calculate the mean abundance of Galumna across all samples using the function mean(), we find that it is close to zero): # Explore the dataset hist(mites$Galumna) mean(mites$Galumna) ## [1] 0.9571429 Our variable $pa (presence-absence) takes yet another form. It consists of only zeros and ones, such that a Poisson distribution would not be appropriate to model this variable. hist(mites$pa) We need a distribution with a range that only includes two possible outcomes: zero or one. The Bernoulli distribution is such a distribution. It is often the first distribution that students of statistics are introduced to, for example when discussing the probability of obtaining the outcome “heads” when flipping a coin. The Bernoulli distribution has only one parameter, \\(p\\), the probability of success (i.e. the probability of obtaining heads on a coin flip). If we consider that each of our samples is equivalent to a coin toss, then we can use the Bernoulli distribution to calculate the probability of obtaining the outcome “Galumna present” (1) vs. “Galumna absent” (0). Here are some examples of Bernoulli distributions with various probabilities of presence (\\(p\\)): We can calculate the number of sites where Galumna is present out of the total number of sites to get an idea of what \\(p\\) might be in our case: # Calculate probabilities of presence of Galumna (p) sum(mites$pa) / nrow(mites) ## [1] 0.3571429 \\(p\\) for the variable mites $pa is more or less 0.36, such that roughly twice as many sites have the outcome “Galumna absent” (0) than the outcome “Galumna present” (1). When there are multiple trials/coin tosses, the Bernoulli distribution expands into the binomial distribution, which has the additional parameter \\(n\\), corresponding to the number of trials. The binomial distribution predicts the probability of observing a given proportion of successes, \\(p\\), out of a known total number of trials, \\(n\\). “Successes” can be anything from taxon occurrence, number of surviving individuals out of a sample, etc. Imagine that instead of only working in the Laurentians, we took 50 mite samples at each of 30 regions across Canada. In each sample from each region, we determine if Galumna is present or absent. We could model this data using a binomial distribution with \\(n\\) = 50 samples (i.e. “trials” or coin flips where Galumna can be either present or absent) and \\(p\\) = the average proportion of samples in which Galumna is present. We would have 30 data points, corresponding to the 30 regions. Here are some examples of binomial distributions with \\(n\\) = 50 and 3 different values of \\(p\\): Note that the binomial distribution is right-skewed at low \\(p\\) values but left-skewed at high \\(p\\) values. This is the main difference with the Poisson distribution: the binomial distribution has an upper limit to its range, corresponding to the number of trials, \\(n\\). Consequently, the binomial distribution is often used to model data where the number of successes are integers and where the number of trials is known. For example, we could use the binomial distribution to model our proportion data, where each individual mite in a sample could be considered a trial, and if the mite is a Galumna individual then the trial is a success. In this case, the number of trials \\(n\\) varies among our 70 samples based on the total number of individuals in the sample, while \\(p\\), the probability of success, is given by the proportion of Galumna in each sample. Why are we discussing all of these distributions? Because they can be used to replace the normal distribution when calculating predicted values in a linear model. For example, we could use the Poisson distribution and model our abundance data with the following equation: \\[Y_i \\sim Poisson(\\lambda = \\beta_0 + \\beta_1X_i)\\] Note that \\(\\lambda\\) varies as a function of \\(X\\) (water content), meaning that the residual variance will also vary with \\(X\\). This means that we just relaxed the homogeneity of variance assumption! Also, predicted values will now be integers instead of fractions because they will all be drawn from Poisson distributions with different \\(\\lambda\\) values. The model will never predict negative values because Poisson distributions have strictly positive ranges. By simply switching the distribution of error terms (\\(\\varepsilon\\)) from normal to Poisson, we solved most of the problems of our abundance linear model. This new model is almost a Poisson generalized linear model, which basically looks like this: Probabilities of observations/predicted values (in orange, as for the lm() model above) are now integers, and that both the variance and the mean of the distribution decline as \\(\\lambda\\) decreases with increasing water content. Why is the fitted line of predicted values curved? Why is this called a “generalized” linear model? "],["generalized-linear-models.html", "Chapter 8 Generalized linear models", " Chapter 8 Generalized linear models “Generalized” linear model (GLIM or GLM) refer to a larger class of models popularized by McCullagh and Nelder (1982). These models come in handy because the response variable \\(y\\) can follow an exponential family distribution with mean \\(\\mu\\), which is assumed to follow a (often) nonlinear function. In GLM, we need to specify: 1. a statistical distribution for the residuals of the model 2. a link function for the predicted valued of the model. For a linear regression with a continuous response variable distributed normally, the predicted values of Y is explained by this equation : \\(μ = βx\\) where: - \\(μ\\) is the predicted value of the response variable - \\(x\\) is the model matrix (i.e. predictor variables) - \\(β\\) is the estimated parameters from the data (i.e. intercept and slope). \\(βx\\) is called the linear predictor. In math terms, it is the matrix product of the model’s matrix \\(x\\) and the vector of the estimated parameters \\(β\\). For a linear regression with a continuous response variable distributed normally, the linear predictor \\(βx\\) is equivalent to the expected values of the response variable. When the response varaible is not normally distributed, this statement is not true. I nthis situation, we need to transform the predicted values \\(μ\\) with a link function to obtain a linear relationship with the linear predictor: \\[g(\\mu_i) = ηi\\] where \\(g(μ)\\) is the link function to the predicted values. Therefore it removes the restriction on the residuals. The \\(μ / 1-μ\\) ratio represent the odds of an event Y to occur. It transforms our predicted value to a scale of 0 to +Inf. For instance the probability to fail this course 0.6, thus the odds of failing is \\(0.6/(1 − 0.6) = 1.5\\). It indicates that the probability of me failing this class is 1.5 higher than the probability that I succeed (which are \\(1.5 × 0.4 = 0.6\\)). To complete our model, we also need a variance function which describe how the variance \\(\\text{var}(Y_i)\\) depends on the mean: \\[\\text{var}(Y_i) = \\phi V(\\mu)\\] where the dispersion parameter \\(\\phi\\) (phi) is constant. With the linear predictor, our generalized linear model would be: \\[\\eta_i = \\underbrace{g(\\mu)}_{Link~~function} = \\underbrace{\\beta_0 + \\beta_1X_1~+~...~+~\\beta_pX_p}_{Linear~component}\\] Ageneral linear model with \\(\\epsilon ∼ N(0, σ^2)\\) and \\(Y_i \\sim{} N(\\mu_i, \\sigma^2)\\) has: an identity link function: \\[g(\\mu_i) = \\mu_i\\] and a variance function: \\[V(\\mu_i) = 1\\] which result in: \\[\\underbrace{g(\\mu)}_{Link~~function} = \\underbrace{\\beta_0 + \\beta_1X_1~+~...~+~\\beta_pX_p}_{Linear~component}\\] When our response variable is binary, the link function is a logit function: \\[\\eta_i = \\text{logit}(\\mu_i) = \\log(\\frac{\\mu_i}{1-\\mu_i})\\] The log transformation allows the values to be distributed from -Inf to +Inf. We can now link the predicted values with the linear predictor \\(βx\\). This is the reason we still qualify this model as linear despite the relationship not being a «straight line». In R, we can fit generalized linear models using the glm() function, which is similar to the lm() function, with the family argument taking the names of the link function (inside link) and, the variance function: # This is what the glm() function syntax looks like (don&#39;t run this) glm(formula, family = gaussian(link = &quot;identity&quot;), data, ...) This approach is extendable to other distribution families, such as the ones below: Distribution of \\(Y\\) Link function name Link function Model R Normal Identity \\(g(\\mu) = \\mu\\) \\(\\mu = \\mathbf{X} \\boldsymbol{\\beta}\\) gaussian(link=\"identity\") Binomial Logit \\(g(\\mu) = \\log\\left(\\dfrac{\\mu}{1-\\mu}\\right)\\) \\(\\log\\left(\\dfrac{\\mu}{1-\\mu}\\right) = \\mathbf{X} \\boldsymbol{\\beta}\\) binomial(link=\"logit\") Poisson Log \\(g(\\mu) = \\log(\\mu)\\) \\(-\\mu^{-1} = \\mathbf{X} \\boldsymbol{\\beta}\\) poisson(link=\"log\") Exponential Negative Inverse \\(g(\\mu) = -\\mu^{-1}\\) \\(\\log(\\mu) = \\mathbf{X} \\boldsymbol{\\beta}\\) Gamma(link=\"inverse\") "],["binomial-glm.html", "Chapter 9 Binomial GLM 9.1 GLM with binomial data: logit link 9.2 Example 9.3 Challenge 1 9.4 Interpreting the output of a logistic regression 9.5 Predictive power and goodness-of-fit 9.6 Visual representation of results", " Chapter 9 Binomial GLM A common response variable in ecological data sets is the binary variable: we observe a phenomenon \\(Y\\) or its “absence”. For example, species presence/absence is frequently recorded in ecological monitoring studies. We usually wish to determine whether a species’ presence is affected by some environmental variables. Other examples include the presence/absence of a disease within a wild population, the success/failure to record a specific behaviour, and the survival/death of organisms. Usually, we are interested in questions such as: how do species occurrences vary in function of the environment? \\[Occurrences = f(Environment)\\] Under a linear model, expected values can be out of the [0, 1] range with lm(): # set up some binary data Pres &lt;- c(rep(1, 40), rep(0, 40)) rnor &lt;- function(x) rnorm(1, mean = ifelse(x == 1, 12.5, 7.5), sd = 2) ExpVar &lt;- sapply(Pres, rnor) # linear model with binary data... lm(Pres ~ ExpVar) When we predict the probability to observe a phenomenon \\(Y\\) with a binary variable, the predicted value has to be between 0 and 1: it is the possible range of probability! 9.1 GLM with binomial data: logit link As we have seen in the previous section, a regression that has a binary response variable is one of many generalized linear models and is called a logistic regression or a logit model. A generalized linear model is made of a linear predictor: \\[\\underbrace{g(\\mu_i)}_{Link~~function} = \\underbrace{\\beta_0 + \\beta_1X_1~+~...~+~\\beta_pX_p}_{Linear~component}\\] Consider that \\(Y_i ∼ B(n_i, p_i)\\), and that we want to model the proportions of \\(Y_i^{}/n_i\\). As such: \\(E(Y_i^{}/n_i) = p_i\\) and \\(\\text{var}(Y_i^{}/n_i) = \\frac{1}{n_i}p_i(1-p_i)\\) , so that \\(V(\\mu_i) = \\mu_i(1-\\mu_i)\\).] We first move the probabilities \\(\\mu_i\\) to the odds: \\[\\text{odds}_i = \\frac{\\mu_i}{1-\\mu_i}\\] The odds puts our expected values on a 0 to +Inf scale. We then take logarithms, calculating the logit or log-odds: \\[\\eta_i = \\text{logit}(\\mu_i) = \\log(\\frac{\\mu_i}{1-\\mu_i})\\] with \\(\\mu\\) being the expected values (probability that \\(Y = 1\\) ), and with the expected values now ranging from -Inf to +Inf. In R, presence (or success, survival…) is usually coded as 1 and absence (or failure, death…) as 0. A logistic regression (or any other generalized linear model) is performed with the glm() function. This function is different from the basic lm() as it allows one to specify a statistical distribution other than the normal distribution. glm(formula, family = ???, # this argument allows us to set a probability distribution! data, ...) We have seen that binary variables are not normally distributed (i.e. we observe a peak at 0 and a peak at 1, but nothing in between). In ‘R’, we can specify the statistical distribution of our model with the argument ‘family’. For a logistic regression, we indicate ‘family = binomial’. Distribution of \\(Y\\) Link function name Link function Model R Normal Identity \\(g(\\mu) = \\mu\\) \\(\\mu = \\mathbf{X} \\boldsymbol{\\beta}\\) gaussian(link=\"identity\") Binomial Logit \\(g(\\mu) = \\log\\left(\\dfrac{\\mu}{1-\\mu}\\right)\\) \\(\\log\\left(\\dfrac{\\mu}{1-\\mu}\\right) = \\mathbf{X} \\boldsymbol{\\beta}\\) binomial(link=\"logit\") Poisson Log \\(g(\\mu) = \\log(\\mu)\\) \\(-\\mu^{-1} = \\mathbf{X} \\boldsymbol{\\beta}\\) poisson(link=\"log\") Exponential Negative Inverse \\(g(\\mu) = -\\mu^{-1}\\) \\(\\log(\\mu) = \\mathbf{X} \\boldsymbol{\\beta}\\) Gamma(link=\"inverse\") In R, we can therefore build a binomial GLM with a logit link as follows: # This is the syntax for a binomial GLM with a logit link glm(formula, family = binomial(link = &quot;logit&quot;), # this is also known as logistic data, ...) 9.2 Example Let’s build our first generalized linear model! Here, we want to build a logistic regression model using the mites data # Exercise 1 - our first GLM! # setwd(&#39;...&#39;) mites &lt;- read.csv(&quot;data/mites.csv&quot;, header = TRUE) str(mites) ## &#39;data.frame&#39;: 70 obs. of 9 variables: ## $ Galumna : int 8 3 1 1 2 1 1 1 2 5 ... ## $ pa : int 1 1 1 1 1 1 1 1 1 1 ... ## $ totalabund: int 140 268 186 286 199 209 162 126 123 166 ... ## $ prop : num 0.05714 0.01119 0.00538 0.0035 0.01005 ... ## $ SubsDens : num 39.2 55 46.1 48.2 23.6 ... ## $ WatrCont : num 350 435 372 360 204 ... ## $ Substrate : chr &quot;Sphagn1&quot; &quot;Litter&quot; &quot;Interface&quot; &quot;Sphagn1&quot; ... ## $ Shrub : chr &quot;Few&quot; &quot;Few&quot; &quot;Few&quot; &quot;Few&quot; ... ## $ Topo : chr &quot;Hummock&quot; &quot;Hummock&quot; &quot;Hummock&quot; &quot;Hummock&quot; ... We can fit the logistic regression model of the presence of Galumna sp. as a function of water content and topography as follows, using the glm() function and the family argument: logit.reg &lt;- glm(pa ~ WatrCont + Topo, data = mites, family = binomial(link = &quot;logit&quot;)) To see the model output, we run: summary(logit.reg) ## ## Call: ## glm(formula = pa ~ WatrCont + Topo, family = binomial(link = &quot;logit&quot;), ## data = mites) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0387 -0.5589 -0.1594 0.4112 2.0252 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.464402 1.670622 2.672 0.007533 ** ## WatrCont -0.015813 0.004535 -3.487 0.000489 *** ## TopoHummock 2.090757 0.735348 2.843 0.004466 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 91.246 on 69 degrees of freedom ## Residual deviance: 48.762 on 67 degrees of freedom ## AIC: 54.762 ## ## Number of Fisher Scoring iterations: 6 Doesn’t this structure resembles the one from summary.lm()? IT does, right? However you might notice that there are some special differences (e.g. dispersion parameter) we will discuss further in this book! 9.3 Challenge 1 Using the bacteria dataset (from the MASS package), model the presence of H. influenzae as a function of treatment and week of test. Start with a full model and reduce it to the most parsimonious model. Load the MASS package and the bacteria dataset: ## &#39;data.frame&#39;: 220 obs. of 6 variables: ## $ y : Factor w/ 2 levels &quot;n&quot;,&quot;y&quot;: 2 2 2 2 2 2 1 2 2 2 ... ## $ ap : Factor w/ 2 levels &quot;a&quot;,&quot;p&quot;: 2 2 2 2 1 1 1 1 1 1 ... ## $ hilo: Factor w/ 2 levels &quot;hi&quot;,&quot;lo&quot;: 1 1 1 1 1 1 1 1 2 2 ... ## $ week: int 0 2 4 11 0 2 6 11 0 2 ... ## $ ID : Factor w/ 50 levels &quot;X01&quot;,&quot;X02&quot;,&quot;X03&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ trt : Factor w/ 3 levels &quot;placebo&quot;,&quot;drug&quot;,..: 1 1 1 1 3 3 3 3 2 2 ... This dataset was made to test the presence of the bacteria H. influenzae in children with otitis media in the Northern Territory of Australia. Dr A. Leach tested the effects of a drug on 50 children with a history of otitis media in the Northern Territory of Australia. The children were randomized to the drug or a placebo. The presence of H. influenzae was checked at weeks 0, 2, 4, 6 and 11: 30 of the checks were missing and are not included in this data frame. Click here to see the solution! # Challenge 1 - Solution # Fit models (full to most parsimonious) model.bact1 &lt;- glm(y ~ trt * week, data = bacteria, family = binomial) model.bact2 &lt;- glm(y ~ trt + week, data = bacteria, family = binomial) model.bact3 &lt;- glm(y ~ week, data = bacteria, family = binomial) # Let&#39;s compare these models using a likelihood ratio test (LRT). anova(model.bact1, model.bact2, model.bact3, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: y ~ trt * week ## Model 2: y ~ trt + week ## Model 3: y ~ week ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 214 203.12 ## 2 216 203.81 -2 -0.6854 0.70984 ## 3 218 210.91 -2 -7.1026 0.02869 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Which model is the best candidate? Based on these results, we select model #2 as the best candidate to model these data. 9.4 Interpreting the output of a logistic regression The output of our logistic regression indicates that both water content (WatrCont) and topography (‘Topo’) are significant: # Extracting model coefficients summary(logit.reg)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.46440199 1.670622482 2.672299 0.0075333598 ## WatrCont -0.01581255 0.004535069 -3.486728 0.0004889684 ## TopoHummock 2.09075654 0.735348234 2.843220 0.0044660283 But how do we interpret the slope coefficients? Remember that we applied a transformation on our expected values (i.e. the probability that \\(Y = 1\\) so we have to use an inverse function to properly interpret the results. 9.4.1 An example using the identity link If we were to use the identity link function, the interpretation is much easier. Assuming we have a binary outcome \\(y\\) and two covariates \\(x_1\\) and \\(x_2\\) and a constant, the probability of a successful outcome ( \\(y = 1\\) ) is given by: \\[Pr(y_i = 1) = p = g^{-1(\\beta_0 + x_{1i}\\beta_1 + x_{2i}\\beta_2)}\\] where \\(g^{-1}()\\) is the inverse link function. For the identity link, the interpretation of the \\(\\beta_1\\) coefficient is therefore straighforward: For one-unit increase in \\(x_1\\), \\(\\beta_1\\) dictates a constant difference in the outcome. \\[\\Delta{y_i} = (\\beta_0 + \\beta_1(\\color{red}{x_{1i} + 1}) + \\beta_2x_{2i}) - (\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})\\] \\[\\Delta{y_i} = \\beta_1\\] 9.4.2 Interpreting the coefficients using the logit link In a linear logistic model with two covariates \\(x_1\\) and \\(x_2\\), we have: \\[log({\\dfrac{p}{1-p}})=\\beta_0 + \\beta_1x_{1i} +\\beta_2x_{2i}\\] This corresponds to a log odds ratio! We can the use an exponential function to rewrite that model to get the odds ratio: \\[\\dfrac{p}{1-p}=exp(\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})\\] If we want to convert the odds into probability, given a coefficient \\(\\alpha\\) we would use the inverse logit link function (also known as the logistic function): \\[ Pr(y_i = 1) = logit^{-1}(\\alpha) = \\dfrac{1}{1 + exp(-\\alpha)} = (\\dfrac{1}{1 + exp(-\\alpha)}) * (\\dfrac{exp(\\alpha)}{exp(\\alpha)}) = \\dfrac{exp(\\alpha)}{exp(\\alpha) + 1}\\] Now going back to our model, this gives us: \\[Pr(y_i = 1) = \\dfrac{exp(\\beta_0 +\\beta_1 x_{1i} + \\beta_2x_{2i})}{1 + exp{(\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})}}\\] Since the inverse link is nonlinear, it is difficult to interpret the coefficient. However, we can look what happens to the differences for a one-unit change to \\(x_1\\): \\[\\Delta{y_i} = \\dfrac{\\exp(\\beta_0 + \\beta_1(\\color{red}{x_{1i} + 1}) + \\beta_2x_{2i})}{1 + \\exp{(\\beta_0 + \\beta_1(\\color{red}{x_{1i} + 1}) + \\beta_2x_{2i}})} - \\dfrac{\\exp(\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})}{1 + \\exp{(\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})}}\\] \\[\\Delta{y_i} = \\exp(\\beta_1)\\ \\] As \\(x_1\\) increases by one unit, the odds increase by a factor of \\(\\exp(\\beta_1)\\). Note that the odds values here are considered when all other parameters are kept constant. With this, we can now interpret the results of our model: # model output logit.reg ## ## Call: glm(formula = pa ~ WatrCont + Topo, family = binomial(link = &quot;logit&quot;), ## data = mites) ## ## Coefficients: ## (Intercept) WatrCont TopoHummock ## 4.46440 -0.01581 2.09076 ## ## Degrees of Freedom: 69 Total (i.e. Null); 67 Residual ## Null Deviance: 91.25 ## Residual Deviance: 48.76 AIC: 54.76 For a one-unit increase (or decrease) in our coefficients, we can obtain the odds for the presence of mites. # odds for the presence of mites exp(logit.reg$coefficient[2:3]) ## WatrCont TopoHummock ## 0.9843118 8.0910340 When the odds value is smaller than 1, interpretation is a little bit more complicated. When this is the case, we have to take the inverse value (i.e. 1 divided by the odds) to facilitate interpretation. The interpretation is then how LESS likely it is to observe the event of interest. For water content, the odds is 0.984. The inverse is: \\[\\dfrac{1}{0.984} = 1.0159\\]. This means that a one-unit increase in water content decreases the likelihood of observing Galumna sp. by 1.0159. We can also subtract 1 from the odds value to obtain a percentage: \\((1.0159 - 1) * 100 = 1.59%\\). So there is a 1.59% decrease in probability of observing Galumna sp. with a one-unit increase in water content. To convince ourselves that it is an appropriate interpretation, we can plot the presence of Galumna sp. as a function of water content. We see that, on average, Galumna sp. presence is higher at lower water content than its “absence”. When an estimated parameter is between 0 and 1 on the odds’ scale, the relationship between the response variable and the predictors is negative. If it is greater than 1, the relationship will be positive. If the confidence interval include 1 on the odds’ scale, the relationship is not significant. Keep in mind that a value of 1 on the odds’ scale indicates that the probability to observe the event Y is equal to the probability of not observing the event (i.e. \\(p\\) = 0.5, 0.5/(1-0.5) = 1). Let us try with topography: exp(logit.reg$coefficients[1]) ## (Intercept) ## 86.86907 The estimated parameter for topography is 2.091 on the odds’ scale. Thus, the probability is: \\(1/(1+1/exp(2.091)) = 0.89\\) equivalent to \\(1/(1+1/8.09)\\). Note that the odds for a predictor is calculated when all other variables are constant. Topography has an odd of 8.09. Thus the probability of observing Galumna sp. is 8.09 greater when the topography is hummock instead of blanket. We can calculate the odds without the function exp(): Let us start with the odds for topography from logit.reg: \\(µ/ (1 - µ) = 8.09\\) We rearrange to isolate \\(µ\\) : \\(µ = 8.09(1 - µ) = 8.09 - 8.09µ\\) \\(8.09µ + µ = 8.09\\) \\(µ(8.09 + 1) = 8.09\\) \\(µ = 8.09 / (8.09 + 1)\\) \\(µ = 1 / (1 + (1 / 8.09)) = 0.89\\) We found the same results ! 9.5 Predictive power and goodness-of-fit An easy and intuitive way to evaluate the predictive power of your model is to compare its deviance to the deviance of a null model. Deviance can be understood as a generalisation of the residual sum of squares when models are estimated by maximum likelihood (i.e. it is how parameters are estimated in GLM). This allows us to compute a pseudo-R2 statistic, which is analogous to the coefficient of determination R2 in ordinary least square regression (i.e. the basic method for linear models). The generic formula to compute a pseudo-R2 is given by: \\[\\text{pseudo-R}^2 = \\dfrac{\\text{null deviance} - \\text{residual deviance}}{\\text{null deviance}}\\] where “null deviance” is the deviance of the null model and “residual deviance” is the deviance of the model of interest. The difference is divided by the null deviance so that the result is bound between 0 and 1. A null model is a model without any predictor. In R, we create a null model this way : null.model &lt;- glm(Response.variable ~ 1, family = binomial). The unit deviance is a measure of distance between \\(y\\) and \\(μ\\). \\[{\\displaystyle d(y,y)=0}\\] \\[{\\displaystyle d(y,\\mu )&gt;0\\quad \\forall y\\neq \\mu }\\] The total deviance \\({\\displaystyle D(\\mathbf {y} ,{\\hat {\\boldsymbol {\\mu }}})}\\) of a model with predictions \\({\\hat {\\boldsymbol {\\mu }}}\\) of the observation \\(\\mathbf {y}\\) is the sum of its unit deviances: \\[{\\displaystyle D(\\mathbf {y} ,{\\hat {\\boldsymbol {\\mu }}})=\\sum _{i}d(y_{i},{\\hat {\\mu }}_{i})}\\] Now, the deviance of a model that has estimates \\({\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]\\) can be defined by its likelihood: \\[D(y,{\\hat {\\mu }})=2{\\Big (}\\log {\\big (}p(y\\mid {\\hat {\\theta }}_{s}){\\big )}-\\log {\\big (}p(y\\mid {\\hat {\\theta }}_{0}){\\big )}{\\Big )}\\] with \\(\\hat \\theta_0\\) denoting the fitted values of the parameters in the reduced model, while \\({\\displaystyle {\\hat {\\theta }}_{s}}\\hat \\theta_s\\) denotes the fitted parameters for the saturated model. The residual deviance is defined as 2 times the log-likelihood ratio of the full model compared to the reduced model: \\[D(y,{\\hat {\\mu }})=2{\\Big (}\\log {\\big (}p(\\text{saturated model}){\\big )}-\\log {\\big (}p(\\text{reduced model}){\\big )}{\\Big )}\\] And, the null deviance is defined 2 times the log-likelihood ratio of the full model compared to the null model (i.e. predictors are set to 1). \\[D(y,{\\hat {\\mu }})=2{\\Big (}\\log {\\big (}p(\\text{saturated model}){\\big )}-\\log {\\big (}p(\\text{null model}){\\big )}{\\Big )}\\] Now we can run this in R. Let us compare the deviance of your model (residual deviance) to the deviance of a null model (null deviance). The null model is a model without any explanatory variable and it looks like this: null.model &lt;- glm(response.variable ~ 1, family = binomial) The saturated (or full) deviance model is a model with all explanatory variables: full.model &lt;- glm(response.variable ~ ., family = binomial) Residual and null deviances are already stored in the glm object: # extract residual and null deviances objects(logit.reg) ## [1] &quot;aic&quot; &quot;boundary&quot; &quot;call&quot; ## [4] &quot;coefficients&quot; &quot;contrasts&quot; &quot;control&quot; ## [7] &quot;converged&quot; &quot;data&quot; &quot;deviance&quot; ## [10] &quot;df.null&quot; &quot;df.residual&quot; &quot;effects&quot; ## [13] &quot;family&quot; &quot;fitted.values&quot; &quot;formula&quot; ## [16] &quot;iter&quot; &quot;linear.predictors&quot; &quot;method&quot; ## [19] &quot;model&quot; &quot;null.deviance&quot; &quot;offset&quot; ## [22] &quot;prior.weights&quot; &quot;qr&quot; &quot;R&quot; ## [25] &quot;rank&quot; &quot;residuals&quot; &quot;terms&quot; ## [28] &quot;weights&quot; &quot;xlevels&quot; &quot;y&quot; We can then use these deviance values to calculate the pseudo-R2 value: # calculate the pseudo-R2 pseudoR2 &lt;- (logit.reg$null.deviance - logit.reg$deviance) / logit.reg$null.deviance pseudoR2 ## [1] 0.4655937 Hence, the model explains 46.6% of the variability in the data. An adjusted McFadden’s pseudo-R2, which penalizes for the number of predictors, can be calculated as below: \\[ R^2_{adj} = 1 - \\frac{logL(M)-K}{logL(M_{null})} \\] where K corresponds to the additional number of predictors in relation to the null model. The goodness-of-fit of logistic regression models can be expressed by variants of \\(pseudo-R^2\\) statistics, such as Maddala (1983) or Cragg and Uhler (1970) measures. When talking about logistic regressions, low R2 values are common. The R function DescTools::PseudoR2() makes it possible to calculate many types of \\(pseudo-R^2\\). By specifying which = all, calculate all of them at once. # Calculate many pseudo-R2! logit.reg &lt;- glm(pa ~ WatrCont + Topo, data = mites, family = binomial(link = &quot;logit&quot;)) DescTools::PseudoR2(logit.reg, which = &quot;all&quot;) ## McFadden McFaddenAdj CoxSnell Nagelkerke AldrichNelson ## 0.4655937 0.3998373 0.4549662 0.6245898 0.3776866 ## VeallZimmermann Efron McKelveyZavoina Tjur AIC ## 0.6674318 0.5024101 0.7064093 0.5114661 54.7623962 ## BIC logLik logLik0 G2 ## 61.5078819 -24.3811981 -45.6229593 42.4835224 9.5.1 Challenge 2 Assess goodness-of-fit and predictive power of the model.bact2 model. How can you improve the predictive power of this model? Click here to see the solution! # Challenge 2 - Solution # Extract null and residual deviance null.d &lt;- model.bact2$null.deviance resid.d &lt;- model.bact2$deviance # Calculate pseudo-R2 bact.pseudoR2 &lt;- (null.d - resid.d) / null.d bact.pseudoR2 ## [1] 0.0624257 This is very low! Adding informative explanatory variables could increase the explanatory power of the model. But, do not be afraid of non-significant results! 9.6 Visual representation of results Once we have validated our model, we would likely want to visually represent our results. Here is an example with ggplot2. See workshop 3for a reminder for this package. ggplot(mites, aes(x = WatrCont, y = pa)) + geom_point() + stat_smooth(method = &quot;glm&quot;, method.args = list(family=binomial), se = TRUE) + xlab(&quot;Water content&quot;) + ylab(&quot;Probability of presence&quot;) + ggtitle(&quot;Probability of presence of Galumna sp. against the water content&quot;)+theme_classic() "],["binomial-glm-and-proportions.html", "Chapter 10 Binomial GLM and proportions", " Chapter 10 Binomial GLM and proportions Sometimes, proportion data are more similar to logistic regression than you think. In discrete counts, we can, for instance, measure the number of presence of individuals in relation to the total number of populations sampled. We will thus obtain a proportional number of “success” in observing individuals by dividing the counts by the total counts. In glm(), we have to provide prior weights if the response variable is the proportion of successes. Proportions can be modelled by providing both the number of “successes” and prior weights in the function prop.reg &lt;- glm(cbind(Galumna, totalabund - Galumna) ~ Topo + WatrCont, data = mites, family = binomial) summary(prop.reg) ## ## Call: ## glm(formula = cbind(Galumna, totalabund - Galumna) ~ Topo + WatrCont, ## family = binomial, data = mites) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4808 -0.9699 -0.6327 -0.1798 4.1688 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.288925 0.422109 -7.792 6.61e-15 *** ## TopoHummock 0.578332 0.274928 2.104 0.0354 * ## WatrCont -0.005886 0.001086 -5.420 5.97e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 140.702 on 69 degrees of freedom ## Residual deviance: 85.905 on 67 degrees of freedom ## AIC: 158.66 ## ## Number of Fisher Scoring iterations: 5 The weights can also be set explicitly in glm(): prop.reg2 &lt;- glm(prop ~ Topo + WatrCont, data = mites, family = binomial, weights = totalabund) # provide prior weights summary(prop.reg2) ## ## Call: ## glm(formula = prop ~ Topo + WatrCont, family = binomial, data = mites, ## weights = totalabund) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4808 -0.9699 -0.6327 -0.1798 4.1688 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.288925 0.422109 -7.792 6.61e-15 *** ## TopoHummock 0.578332 0.274928 2.104 0.0354 * ## WatrCont -0.005886 0.001086 -5.420 5.97e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 140.702 on 69 degrees of freedom ## Residual deviance: 85.905 on 67 degrees of freedom ## AIC: 158.66 ## ## Number of Fisher Scoring iterations: 5 "],["what-can-we-do-with-count-data.html", "Chapter 11 What can we do with count data? 11.1 Poisson GLMs 11.2 The problem of overdispersion 11.3 Quasi-Poisson GLMs 11.4 Negative binomial GLMs 11.5 Plotting the final GLM to the data 11.6 Conclusion on GLMs with count data", " Chapter 11 What can we do with count data? Count data is characterized by: Positive values: you do not count -7 individuals Integer values: you do not count 7.56 individuals Exhibits larger variance for large values To illustrate count data we will use a new dataset called faramea. # Load the dataset faramea &lt;- read.csv(&quot;faramea.csv&quot;, header = TRUE) In this dataset, the number of trees of the species Faramea occidentalis was measured in 43 quadrants in Barro Colorado Island in Panama. For each quadrant, environmental characteristics were also recorded such as elevation or precipitation. Let us take a look at the number of Faramea occidentalis found at each quadrant. # Histogram of F. occidentalis count data hist(faramea$Faramea.occidentalis, breaks=seq(0,45,1), xlab=expression(paste(&quot;Number of &quot;, italic(Faramea~occidentalis))), ylab=&quot;Frequency&quot;, main=&quot;&quot;, col=&quot;grey&quot;) We can see that there are only positive and integer values. In this example, we want to test whether elevation (a continuous explanatory variable) influences Faramea occidentalis abundance. Hence, a Poisson GLM (i.e. a simple Poisson regression) seems to be a good choice to model the number of Faramea occidentalis as a function of elevation. Poisson GLMs are usually a good way to start modeling count data. 11.1 Poisson GLMs 11.1.1 The Poisson distribution The Poisson distribution specifies the probability of a discrete random variable \\(Y\\) and is given by: \\[f(y, \\,\\mu)\\, =\\, Pr(Y = y)\\, =\\, \\frac{\\mu^y \\times e^{-\\mu}}{y!}\\] \\[E(Y)\\, =\\, Var(Y)\\, =\\, \\mu\\] where \\(\\mu\\) is the parameter of the Poisson distribution The Poisson distribution is particularly relevant to model count data because it: specifies the probability only for integer values \\(P(y&lt;0) = 0\\), hence the probability of any negative value is null \\(Var(Y) = \\mu\\) (the mean-variance relationship) allows for heterogeneity (e.g. when variance generally increases with the mean) A Poisson GLM will model the value of \\(\\mu\\) as a function of different explanatory variables: Step 1. We assume \\(Y_i\\) follows a Poisson distribution with mean and variance \\(\\mu_i\\). \\[Y_i ∼ Poisson(\\mu_i)\\] \\[E(Y_i) = Var(Y_i) = \\mu_i\\] \\[f(y_i, \\, \\mu_i) = \\frac{\\mu^{y_i}_i \\times e^{-\\mu_i}}{y!}\\] \\(\\mu_i\\) corresponds to the expected number of individuals. Step 2. We specify the linear predictor of the model just as in a linear model. \\[\\underbrace{\\alpha}_\\text{Intercept} + \\underbrace{\\beta_1}_\\text{slope of &#39;Elevation&#39;} \\times \\text{Elevation}_i\\] Step 3. The link between the mean of \\(Y_i\\) and linear predictor is a logarithmic function can be written as: \\[log(\\mu_i) = \\alpha + \\beta_1 \\times \\text{Elevation}_i \\] It can also be written as: \\[\\mu_i = e^{ \\alpha + \\beta \\times \\text{Elevation}_i}\\] This shows that the impact of each explanatory variable is multiplicative. Increasing Elevation by one increases μ by factor of exp( \\(\\beta_\\text{Elevation}\\) ). We can also write it as: \\[\\mu_i = e^{\\alpha} \\times e^{\\beta_1^{\\text{Elevation}_i}}\\] If \\(β_j = 0\\) then \\(exp(β_j) = 1\\) and \\(μ\\) is not related to \\(x_j\\). If \\(β_j &gt; 0\\) then \\(μ\\) increases if \\(x_j\\) increases; if \\(β_j &lt; 0\\) then \\(μ\\) decreases if \\(x_j\\) increases. 11.1.2 Poisson GLMs in R Fitting a Poisson GLM in R requires only setting family = poisson in the glm() function. By default the link function is log. # Fit a Poisson GLM glm.poisson = glm(Faramea.occidentalis ~ Elevation, data = faramea, family = poisson) # this is what makes it a Poisson GLM! Note the default link is log. summary(glm.poisson) ## ## Call: ## glm(formula = Faramea.occidentalis ~ Elevation, family = poisson, ## data = faramea) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3319 -2.7509 -1.5451 0.1139 11.3995 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.7687001 0.1099136 16.092 &lt; 2e-16 *** ## Elevation -0.0027375 0.0006436 -4.253 2.11e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 414.81 on 42 degrees of freedom ## Residual deviance: 388.12 on 41 degrees of freedom ## AIC: 462.01 ## ## Number of Fisher Scoring iterations: 10 Intercept = \\(\\alpha\\) Elevation = \\(\\beta\\) The output is similar to a ‘lm’ output (seeWorkshop 4) and gives us the parameter estimates which can also be retrieved using other functions: # intercept summary(glm.poisson)$coefficients[1,1] ## [1] 1.7687 # slope of elevation summary(glm.poisson)$coefficients[2,1] ## [1] -0.002737509 Now we can estimate the residual and null deviances. In our model the unknown parameters are the intercept (\\(\\alpha\\)) and the slope of elevation (\\(\\beta\\)): \\[log(\\mu_i) = 1.769 - 0.0027 \\times \\text{Elevation}_i\\] which we can also write as: \\[\\mu_i = e^{1.769 - 0.0027 \\times \\text{Elevation}_i}\\] Remember that to estimate the unknown parameter, maximum likelihood estimation is used. The residual deviance is defined as twice the difference between the log-likelihood of a model that provides a perfect fit and the log-likelihood of our model. \\[\\text{residual deviance} = 2 \\, log(L(y;\\,y)) - 2 \\, log(L(y;\\, \\mu))\\] In a Poisson GLM, the residual deviance should be close to the residual degrees of freedom. However, our residual deviance is much higher than the degrees of freedom of our model! \\[388.12 &gt;&gt; 41\\] 11.2 The problem of overdispersion An important aspect of the summary can be found in the last lines. # Null deviance: 414.81 on 42 degrees of freedom # Residual deviance: 388.12 on 41 degrees of freedom Remember that ML estimation is used to estimate the parameters. In the goodness-of-fit section we mentioned that the deviance was a ML equivalent of the sum of squares in linear models. Here, the null deviance and the residual deviance are equivalent to the total sum of squares and the residual sum of squares respectively. The residual deviance is defined as twice the difference between the log likelihood of a model that provides a perfect fit to the data (a saturated model) and the log likelihood of the model. If our model is correct the asymptotic distribution of the residual deviance is approximated using χ² distribution with \\(n\\)-\\(p\\)-1 degrees of freedom (computed as \\(n\\)-\\(p\\)-1, where \\(n\\) is the number of observations and \\(p\\) the number of covariates). This implies that residual deviance should be equal to the residual degrees of freedom. In our example, the residual deviance equals 388.12 while we have 41 (43-1-1) degrees of freedom. This former is greater to the former by 9.5 times, the model is then overdispersed. Overdispersion As a consequence overdispersion can be computed for any model using the parameter φ: φ = residual deviance / residual degrees of freedom * φ &lt; 1 indicates underdispersion * φ = 1 indicates no overdispersion * φ &gt; 1 indicates overdispersion Why does a Poisson GLM exhibit overdispersion? This arises when the variance of the data is higher than expected from the Poisson distribution. This frequently occurs when data includes many zeros and/or many very high values. Looking back at the distribution of our data (above) suggests that our data contains many zeroes preventing us to use a Poisson GLM. Overdispersion may also result from missing covariates, missing interaction terms or presence of strong outliers, preventing us from using a Poisson GLM. The Poisson distribution can account only partially for heterogeneity in the data due to the mean variance relationship, but in some cases variance increases even higher than the mean. Computing the mean and the variance of our dataset suggests this is occurring: mean(faramea$Faramea.occidentalis) var(faramea$Faramea.occidentalis) In practice, Poissons GLM are useful for describing the mean µi but underestimates the variance in the data, making all model-based tests too liberal! There are two ways of dealing with overdispersion and will be developed below: correct for it by doing a quasi-Poisson GLM choose another distribution such as the negative binomial 11.3 Quasi-Poisson GLMs The principle behind a quasi-Poisson GLM is very simple; the overdispersion parameter (φ) is added to the expected variance equation: \\[E(Y_i) = \\mu_i\\] \\[Var(Y_i) = φ.\\mu_i\\] The linear predictor and the link function remains the same. The difference is that \\(φ\\) will first be estimated to correct the model. Parameter Estimates will be the same but the standard errors of the parameters are multiplied by \\(√φ\\), in other terms, some marginally significant p-values may no longer hold. In R, The quasipoisson family object can be used to deal with count data exhibiting overdispersion (the quasibinomial family object can do the same for binomial data). The fitted φ value will be returned in the summary of the GLM. There are two ways to perform this GLM: # Option 1, fit a new quasi-Poisson GLM glm.quasipoisson = glm(Faramea.occidentalis~Elevation, data=faramea, family=quasipoisson) # Option 2, build from the previous model and update it: glm.quasipoisson = update(glm.poisson,family=quasipoisson) # output summary(glm.quasipoisson) ## ## Call: ## glm(formula = Faramea.occidentalis ~ Elevation, family = quasipoisson, ## data = faramea) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3319 -2.7509 -1.5451 0.1139 11.3995 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.768700 0.439233 4.027 0.000238 *** ## Elevation -0.002738 0.002572 -1.064 0.293391 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 15.96936) ## ## Null deviance: 414.81 on 42 degrees of freedom ## Residual deviance: 388.12 on 41 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 10 If you look at the summary of the model you will see that \\(φ\\) is estimated as 15.97. We then made a good choice by updating the model to account for overdispersion. However if we look at P-values we can note that elevation is no longer significant. Yet, 15.97 is quite a lot of overdispersion, and in general quasi-Poisson GLMs will be favoured when \\(φ\\) is included between 1 and 15 though these limits are arbitrary. When overdispersion is higher than 15-20 we recommend moving to the negative binomial. Two other points are important to keep in mind when using quasi-Poisson GLMs and dealing with overdispersion: Quasi-Poisson GLMs do not have AIC scores. An important aspect is that quasi-Poisson GLMs do not correspond to models with fully specified likelihoods and rely on quasi-ML estimation (i.e. pseudolikelihood). One consequence is that quasi-Poisson GLMs do not have AIC scores for model comparisons. However, variants of AIC have been developed to deal with this situation (e.g. quasi-AIC). Overdispersion affects model comparison. Indeed overdispersion also influences the comparison of two nested models and has to be taken into account when φ is known. For instance, let us assume that we want to compare GLM1, with \\(p_1\\) parameters to GLM2, with \\(p_2\\) parameters, such that GLM1 is nested within GLM2 and \\(p_2&gt;p_1\\). Model comparison is achieved based on a generalized likelihood ratio test, which can be written as a function of the difference of deviances between the two GLMs, \\(D_1\\) and \\(D_2\\) respectively. If Overdispersion is known, deviances have to be scaled (i.e. corrected) accordingly as \\(D^* = D/φ\\). The final test will be based on the criterion \\(D^*_1 - D^*_2\\) which is assumed to follow a \\(χ²\\) distribution with \\(p_1-p_2\\) degrees of freedom when GLM1 is correct. In some cases \\(φ\\) is not known. For instance, this occurs when you run a GLM with a normal error distribution. In that case, \\(φ\\) can be estimated a posteriori using the residual deviance of the larger model so the criterion becomes: \\[\\frac{(D_1-D_2)/(p_2-p_1)}{D_2(n-p_2)}\\] and is assumed to follow a F distribution with \\(p_1-p_2\\) and \\(n-p_2\\) degrees of freedom. Try also deviance analysis to test for the effect of Elevation null.model &lt;- glm(Faramea.occidentalis ~ 1, data = faramea, family = quasipoisson) anova(null.model, glm.quasipoisson, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: Faramea.occidentalis ~ 1 ## Model 2: Faramea.occidentalis ~ Elevation ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 42 414.81 ## 2 41 388.12 1 26.686 0.1961 Dispersion parameter: 11.4 Negative binomial GLMs GLM with a negative binomial (NB) distribution are favored when overdispersion is extreme. The NB distribution contains an additional parameter \\(k\\), particularly handy for count data containing a preponderance of zeros. Before we go into R stuff, we should see what lies behind a negative binomial GLM. A NB distribution is actually a combination of two distributions: a Poisson distribution and a gamma distribution. The NB distribution first assumes that a discrete random variable is Poisson distributed but its mean, \\(µ\\) is assumed to follow a gamma distribution. The mixture between the Poisson and gamma distributions can be simplified into a density function specific to the NB which has two parameters \\(µ\\) and \\(k\\). \\[Y \\sim NB(µ, k)\\] \\[E(Y) = µ~and~Var(Y) = µ + µ²/k\\] Here, we can see how overdispersion will be accounted for by NB distribution in GLMs. The second term of the variance determines the amount of overdispersion. In fact, it is indirectly determined by \\(k\\), where \\(k\\) is also called the dispersion parameter. If \\(k\\) is large (relative to \\(μ²\\)), the second term, \\(µ²/k\\) approximates 0, and the variance of Y is \\(μ\\). In such cases the NB converges to the Poisson distribution and you might as well use a Poisson distribution. The smaller \\(k\\), the larger the overdispersion. Just like with others GLMs, a NB GLM is specified following the fundamental three steps. It first assumes that Yi is negative binomial distributed with mean \\(μ_i\\) and parameter \\(k\\). \\[Y_i \\sim NB(µ_i, k)\\] \\[E(Y_i) = µ_i~and~Var(Y_i) = µ_i + µ_i²/k\\] The two last steps define the systematic part and the link function between the mean of \\(Y_i\\) and the predictor function. In NB GLMs the link function is logarithmic ensuring that fitted values are always positive. \\[log(µ_i) = β_0 + βX_i\\] or \\[µ_i = exp(β_0 + βX_i)\\] The negative binomial GLM can be built using the glm.nb() function from the MASS package: glm.negbin = glm.nb(Faramea.occidentalis~Elevation, data=faramea) summary(glm.negbin) ## ## Call: ## glm.nb(formula = Faramea.occidentalis ~ Elevation, data = faramea, ## init.theta = 0.2593107955, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.36748 -1.17564 -0.51338 -0.05226 2.25716 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.369226 0.473841 5.00 5.73e-07 *** ## Elevation -0.007038 0.002496 -2.82 0.00481 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(0.2593) family taken to be 1) ## ## Null deviance: 41.974 on 42 degrees of freedom ## Residual deviance: 36.343 on 41 degrees of freedom ## AIC: 182.51 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.2593 ## Std. Err.: 0.0755 ## ## 2 x log-likelihood: -176.5090 The summary is similar to other GLMs summaries (e.g. Poisson GLMs), though we now have a parameter theta, which stands for parameter \\(k\\) in the variance of the NB distribution. Its standard error is also provided, but care is needed with its use as the interval is not symmetric and we are testing on the boundary. 11.5 Plotting the final GLM to the data The NB GLMs appear to be the best fit to our data. We can plot the relationship between the abundance of Faramea occidentalis and elevation. Use summary to get the parameters. summary(glm.negbin)$coefficients[1, 1] ## [1] 2.369226 summary(glm.negbin)$coefficients[2, 1] ## [1] -0.007038124 Use the standard errors to build the confidence envelope. summary(glm.negbin)$coefficients[1, 2] ## [1] 0.4738409 summary(glm.negbin)$coefficients[2, 2] ## [1] 0.002496143 # Make model predicitions pp &lt;- predict(glm.negbin, newdata = data.frame(Elevation = 1:800), se.fit = TRUE) linkinv &lt;- family(glm.negbin)$linkinv # inverse-link function # Prepare to plot model results pframe &lt;- as.data.frame(pp$fit) names(pframe) &lt;- &quot;pred0&quot; pframe$pred &lt;- linkinv(pp$fit) sc &lt;- abs(qnorm((1-0.95)/2)) # Normal approx. to likelihood pframe &lt;- transform(pframe, lwr = linkinv(pred0-sc*pp$se.fit), upr = linkinv(pred0+sc*pp$se.fit)) # Plot! plot(faramea$Elevation, faramea$Faramea.occidentalis, ylab = &#39;Number of F. occidentalis&#39;, xlab = &#39;Elevation(m)&#39;) lines(pframe$pred, lwd = 2) # predicted values lines(pframe$upr, col = 2, lty = 3, lwd = 2) # show error bounds lines(pframe$lwr, col = 2, lty = 3, lwd = 2) We can see that the number of Faramea occidentalis significantly decreases with elevation. However, the confidence envelope of the NB model is large at low elevation. 11.5.1 Challenge 3 Use the mites dataset! Model the abundance of the species Galumna as a function of the substrate characteristics (water content WatrCont and density SubsDens) Do you need to account for overdispersion? Which covariates have a significant effect? Select the best model! # Challenge 3 mites &lt;- read.csv(&quot;data/mites.csv&quot;, header = TRUE) Drop each term in turn and compare the full model with a nested model using the command: # This is how to do model comparison by dropping terms in turn drop1(MyGLM, test = &quot;Chi&quot;) Specify manually a nested model, call it for example MyGLM2, and use the command: # You can also manually specify a nested model and compare it to your full model with this command: anova(MyGLM, MyGLM2, test = &quot;Chi&quot;) Click here to see the solution! # Poisson GLM glm.p = glm(Galumna~WatrCont+SubsDens, data=mites, family=poisson) # quasi-Poisson GLM glm.qp = update(glm.p,family=quasipoisson) # model selection drop1(glm.qp, test = &quot;Chi&quot;) ## Single term deletions ## ## Model: ## Galumna ~ WatrCont + SubsDens ## Df Deviance scaled dev. Pr(&gt;Chi) ## &lt;none&gt; 101.49 ## WatrCont 1 168.10 31.711 1.789e-08 *** ## SubsDens 1 108.05 3.125 0.07708 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # or glm.qp2 = glm(Galumna~WatrCont, data=mites, family=quasipoisson) anova(glm.qp2, glm.qp, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: Galumna ~ WatrCont ## Model 2: Galumna ~ WatrCont + SubsDens ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 68 108.05 ## 2 67 101.49 1 6.5657 0.07708 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.6 Conclusion on GLMs with count data All the GLMs introduced (Poisson, quasi-Poisson and NB) to model count data use the same log-linear mean function (\\(log(µ) = βx\\)), but make different assumptions about the remaining likelihood. Quasi-Poisson and NB are favored to deal with overdispersion. However, in some cases the data may contains too many zeros and zero-augmented models can be useful as they extend the mean function by modifying (typically, increasing) the likelihood of zero counts (e.g. zero-inflated Poisson [ZIP]). "],["other-distributions.html", "Chapter 12 Other distributions", " Chapter 12 Other distributions When the response variable consists of percentages or proportions that do not arise from successes and failures from \\(n\\) yes/no experiments (Bernoulli experiment), it is not possible to use the binomial distribution. In this case, it is often advised to perform a logit transformation of the data and use a lm(m). See this interesting article. For data that can be appear normally distributed after a log-transformation, it can be advisable to use a log-normal distribution in a GLM instead of log-transforming the data. A Gamma distribution can also be used. It is similar to a log-normal distribution, but is more versatile. The Tweedie distribution is a versatile family of distributions that is useful for data with a mix of zeros and positive values (not necessarily counts). See the R Tweedie package. - When the data comprise an excess number of zeros, that arise from a different process than the process that generates the counts, the zero-inflated Poisson or zero-inflated negative binomial distributions should be used. These methods are available, in the glmmADMB package, among others. "],["summary.html", "Chapter 13 Summary", " Chapter 13 Summary GLMs are a powerful statistical technique for data with non-normal distributions as binomial, abundance and proportion data. These models are, howerver, not without many challenges and do not always offer the perfect solution to problems in data distribution. In the next workshop, you will see general and generalized linear mixed models. These models allow to overcome a number of limitations associated with traditional linear models by modelling part of the variation with a random variable. In that workshop, you will learn when it is important to use a mixed effects model to analyze your data. We will walk you through the steps to conduct a linear mixed model analysis, check its assumptions, report results, and visually represent your model in R. "],["additional-resources.html", "Chapter 14 Additional resources", " Chapter 14 Additional resources Books: B. Bolker (2009) Ecological Models and Data in R. Princeton University Press. A. Zuur et al. (2009) Mixed Effects Models and Extensions in Ecology with R. Springer. Articles : Harrison et al. (2018), PeerJ, DOI 10.7717/peerj.4794 Websites: GLMM for Ecologists A great website on GLMM with a Q&amp;A section! "],["references.html", "Chapter 15 References", " Chapter 15 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
