# Preface: Reviewing linear models

Much of our research focuses on investigating how patterns we observe can be explained by predictive variables. In other words, we are often looking for a function $f$ that can explain a response variable ( $Y$ ) *in terms of* one ( $X_1$ ) or many other predictors ( $X_2$, $X_3$, $...$ , $X_n$ ):

$$Y = f(X_1)$$

The combination of predictive variables we have sampled will *never* fully explain $Y$. Because of this, there is always *unpredictable disturbance* in our models, i.e. the error $\epsilon$. As such, **error** is an irrevocable part of our function:

$$Y = f(X_1, \epsilon)$$
In [Workshop 4](https://qcbsrworkshops.github.io/workshop04/book-en/index.html), we learned how to use **general linear models** ($t$-test, ANOVA, linear regression, ANVOCA) as $f(\cdot)$ to describe the relationship between variables. The general form of our function $Y = f(X_1)$ as a linear function can be expressed as:

$$Y = \beta_0 + \beta_1X_i + \varepsilon$$
where:
$Y_i$ is thepredicted value of a response variable
$\beta_0$ is the *unknown coefficient* **intercept**
$\beta_1$ is the *unknown coefficient* **slope**
$X_i$ is the value for the explanatory variable
$\varepsilon_i$ is the model residual drawn from a normal distribution with a varying mean but a constant variance.

We also learned that **linear models** only produce unbiased estimators (i.e. are only reliable) if they follow certain assumptions. Most importantly:

1. The population can be described by a **linear relationship**:

$$Y = \beta_0 + \beta_1X_i + \varepsilon$$

2. The error term $\varepsilon$ has the same variance given any value of the explanatory variable (i.e. **homoskedasticity**), and the error terms are not correlated across observations (i.e. **no autocorrelation**):

$$\mathbb{V}{\rm ar} (\epsilon_i | \mathbf{X} ) = \sigma^2_\epsilon,\ \forall i = 1,..,N$$

$$\mathbb{C}{\rm ov} (\epsilon_i, \epsilon_j) = 0,\ i \neq j$$

3. And, the **residuals are normally distributed**:

$$\boldsymbol{\varepsilon} | \mathbf{X} \sim \mathcal{N} \left( \mathbf{0}, \sigma^2_\epsilon \mathbf{I} \right)$$
This means that our data needs to look like this in order to use a general linear model, such as a linear regression:

```{r echo = FALSE}
# Set the coefficients:
N = 50
beta_0 = 1
beta_1 = 0.5
# Generate sample data:
x <- 0:N
#
e <- rnorm(mean = 0, sd = 1.5, n = length(x))
y <- beta_0 + beta_1 * x + e
# Plot the data
#
#
plot(x, y)

# The unknown DGP:
y_dgp <- beta_0 + beta_1 * x

# Plot the Unknown Population regression:
lines(x = x, y = y_dgp, col = "darkgreen", lty = 2)

legend(x = 0, y = 25,
       legend = c(expression(paste("Y = ", beta[0] + beta[1] * X))),
       lty = c(2, 1), lwd = c(1, 1), pch = c(NA, NA), col = c("darkgreen", "blue"))
```

## An example with general models

To illustrate why generalized linear models are incredibly useful, it is
best to first try to understand the limitations of linear models. Let us use our prior knowledge on general linear models to explore the relationship between variables within the *Oribatid mite dataset*.

```{r, echo = TRUE, eval = TRUE}
mites <- read.csv('data/mites.csv',
                  stringsAsFactors = TRUE)
```

And, exploring it using:
```{r echo = TRUE, eval = TRUE}
str(mites)
head(mites)
```

This dataset is a subset of the classic *Oribatid mite dataset* from the `vegan` library, which has been used in numerous texts (e.g. Borcard, Gillet & Legendre, *Numerical Ecology with R*). The dataset consists of **70 moss and mite samples** collected at University of Montreal's Station de Biologie des Laurentides, Saint-Hippolyte, QC. Each sample includes measurements for **5 environmental variables** and **abundance data for 35 mite taxa**.

In the reduced dataset that we will be using throughout this workshop, we only included the 5 environmental measurements and the abundance of a single mite taxon, *Galumna sp.* We also created a presence/absence variable and a proportional abundance variable for *Galumna*, to demonstrate different ways to model abundance, occurrence (presence/absence), and proportion data.

For each sample, the following data is provided:

Response variables:
1. `Galumna`: Abundance of *Galumna*
2. `pa`: Occurrence of *Galumna* (presence = 1 or absence = 0)
3. `prop`: Relative Frequency or Proportion of *Galumna* in the mite community

Predictive variables:
1. `SubsDens`: Substrate density
2. `WatrCont`: Water content
3. `Substrate`: type of substrate
4. `Shrub`: abundance of shrubs nearby
5. `Topo`: microtopography (blanket or hummock)

### So, what questions can we ask about this dataset?

**Question**: *Could the abundance, occurrence or proportion of Galumna sp. be predicted by environmental features?*

To answer our question, we can develop models! *So many options!*

$\text{Abundance} = f(\text{Water content}, \epsilon)$  

$\text{Proportion} = f(\text{Water content}, \epsilon)$  

$\text{Occurrence} = f(\text{Substrate}, \epsilon)$  

$\text{Abundance} = f(\text{Topography}, \epsilon)$  

$\text{Occurrence} = f(\text{Shrubs Nearby}, \epsilon)$  

$\text{Relative Frequency} = f(\text{Topography}, \epsilon)$  

$\text{Occurrence} = f(\text{Substrate Density}, \epsilon)$  

$\text{Abundance} = f(\text{Substrate}, \epsilon)$  


To begin, can we see any relationship between *Galumna* and environmental variables?

```{r, echo = TRUE}
plot(mites)
```

There seems to be a negative relationship between Galumna abundance and
water content (`WatrCont`). Occurence (`pa`) and proportion (`prop`) also seem to be negatively correlated with `WatrCont`. This brings us to ask the following question: *Do Galumna's community values (abundance, occurrence and relative frequency) vary as a function of water content?*

We can take a closer look by specifically plotting those 3 response
variables as a function of `WatrCont`:

```{r, echo = TRUE, eval = TRUE}
plot(Galumna ~ WatrCont,
     data = mites,
     xlab = 'Water content',
     ylab = 'Abundance')
```

```{r, echo = TRUE, eval = TRUE}
boxplot(WatrCont ~ pa,
        data = mites,
        xlab='Presence/Absence',
        ylab = 'Water content')
```

```{r, echo = TRUE, eval = TRUE}
plot(prop ~ WatrCont,
     data = mites,
     xlab = 'Water content',
     ylab = 'Proportion')
```

Indeed, *Galumna* seems to vary negatively as a function of `WatrCont`, i.e.
*Galumna* seems to prefer dryer sites.

Let us use (general) linear models to test whether `Galumna`, `pa`, and/or `prop` vary as a function of `WatrCont`:

$\text{Galumna} = f(\text{Water content}, \epsilon)$
$\text{Occurrence} = f(\text{Water content}, \epsilon)$
$\text{Proportion} = f(\text{Water content}, \epsilon)$

We can fit these models in `R`:

```{r, echo = TRUE, eval = TRUE}
lm.abund <- lm(Galumna ~ WatrCont, data = mites)

lm.pa <- lm(pa ~ WatrCont, data = mites)

lm.prop <- lm(prop ~ WatrCont, data = mites)
```

Then, we can check the model output to verify whether these relationships are statistically significant:

```{r, echo = TRUE, eval = TRUE}
summary(lm.abund)
```

```{r, echo = TRUE, eval = TRUE}
summary(lm.pa)
```

```{r, echo = TRUE, eval = TRUE}
summary(lm.prop)
```

Yes, there is a strong and significant relationship for all 3 response variables! **Wait a minute...** We are forgetting something important here! What about the assumptions of linear models?

## Recalling linear models: assumptions

Let's validate these models to make sure that we
respect assumptions of linear models, starting with the abundance model.

```{r, echo = TRUE, eval = TRUE}
plot(Galumna ~ WatrCont, data = mites)
abline(lm.abund)
```

The model does not fit well. It predicts negative abundance values when
`WatrCont` exceeds `600`, which does not make any sense. Also, the model
does poorly at predicting high abundance values at low values of `WatrCont`.

We can also check the model diagnostic plots:

```{r, echo = TRUE, eval = TRUE, fig.show="hold", out.width = "50%", out.height = "50%"}
plot(lm.abund)
```

Diagnostic plots show that the data/model violate assumptions of
homogeneity of variance (the graph on the left shows that residuals are
larger at higher fitted values) and normality (the graph on the right
indicates that residuals are not distributed as expected from a normal
distribution, i.e. many points are far from predicted values given by
the dotted line). Therefore, we need to reject this model, and can't
use it to conclude that Galumna abundance varies as a function of water
content.

Looking at diagnostic plots for the presence-absence model and
the proportion model also indicate that these models are also inappropriate:

```{r, echo = TRUE, eval = TRUE}
# Plot the proportion model
plot(prop ~ WatrCont, data = mites)
abline(lm.prop)
```

```{r, echo = TRUE, eval = TRUE, fig.show = "hold", out.width="50%", out.height="50%"}
# Diagnostic plots
plot(lm.prop)
```

```{r, echo = TRUE, eval = TRUE}
# Plot the presence/absence model
plot(pa ~ WatrCont, data = mites)
abline(lm.pa)
```

```{r, echo = TRUE, eval = TRUE, fig.show = "hold", out.width="50%", out.height="50%"}
# Diagnostic plots
plot(lm.pa)
```

Let's take a step back here and review the assumptions of linear models, and where they come from. Remember our simple linear model?

$$Y_i = \beta_0 + \beta_1X_i + \varepsilon$$

where:

- $Y_i$ is the predicted value of a response variable
- $\beta_0$ is the *unknown coefficient* **intercept**
- \beta_1$ is the *unknown coefficient* **slope**
- $X_i$ is the value of the explanatory variable
- $\varepsilon_i$ is the model residual drawn from a normal distribution with a varying mean but a constant variance.


This last point about $\varepsilon_i$ is important. This is where assumptions of
normality and homoscedasticity originate. **It means that the residuals
(the distance between each observation and the regression line) can be
predicted by drawing random values from a normal distribution.**

Recall
that all normal distributions have two parameters, $\mu$ (the mean of the distribution) and $\sigma^2$ (the variance of the distribution). In a linear model, $\mu$ changes based on values of $X$ (the predictor
variable), but $\sigma^2$ has the same value for all values of $Y$. Our simple linear can also be written as this:

$$Y_i \sim N(\mu = \beta_0 + \beta_1 X_i +\varepsilon, \sigma^2)$$

with $N(\cdot)$ meaning that $Y_i$ is drawn from a **normal distribution** with parameters $\mu$ (mean; which depends on $x_i$) and $\sigma$ (variance; which has the same value for all $Y_i$s).

Let's take a look at what happens to the distribution when we vary the parameters $\mu$ and $\sigma$.

Varying $\mu$ while $\sigma = 5$ shifts the mean of the distribution.

```{r, echo = FALSE}
x = seq(1, 50, 0.1)
plot(x, dnorm(x, mean = 20, sd = 5),
type = 'l', lwd = 3,
xlab = '# galumna', ylab = 'Probability')
points(x, dnorm(x, mean = 25, sd = 5),
type = 'l', lwd = 3, col = 2)
points(x, dnorm(x, mean = 30, sd = 5), type = 'l', lwd = 3, col = 4)
legend('topleft', legend = c('20', '25', '30'), lty = 1, col = c(1,2,4), bty = 'n', lwd = 2, cex = 1.1)
```

If we keep $\mu = 25$, varying $\sigma$ changes the shape of the distribution, where smaller $\sigma$ (low variance) means there is higher probability around the mean, while larger $\sigma$ spreads out the probabilities across the full range of values.

```{r, echo=FALSE}
x = seq(1, 50, 0.1)
plot(x, dnorm(x, mean = 25, sd = 5), type = 'l', lwd = 3, xlab = '# galumna', ylab = 'Probability')
points(x, dnorm(x, mean = 25, sd = 7.5), type = 'l', lwd = 3, col = 2)
points(x, dnorm(x, mean = 25, sd = 10), type = 'l', lwd = 3, col = 4)
legend('topleft', legend = c('5', '7.5', '10'), lty = 1, col = c(1,2,4), bty = 'n', lwd = 2, cex = 1.1)
```

### Model prediction

When the assumptions of the linear model are not met, model prediction becomes problematic. Let's work through an example to demonstrate some of the problems that arise from a poorly fitted model.

Remember that when we predict, we aim at estimating the *unknown coefficients* $\beta_0$ and $\beta_1$ so that a line effectively predicting every value of $Y$ as a function of $X$ can be drawn!

$$Y_i \sim N(\mu = \beta_0 + \beta_1 X_i +\varepsilon, \sigma^2)$$

Let's predict *Galumna* abundance at a water content = 300 using the linear model we fit above. **What are the parameters of the normal distribution used to model $Y$ when water content is $300$?**

Let's begin by obtaining the parameters $\mu$ and $\sigma^2$ for a normal distribution corresponding to our equation. To obtain the coefficients from our models, we can use the function `coef()`:

```{r, echo = TRUE}
coef(lm.abund)
```

These coefficients would allow us to predict *Galumna* abundance if there was no error. However, we know that error is an **irrevocable** part of our model. To get our predicted values, we therefore also need to add \varepsilon. This is where we use the normal
distribution! For $X$ = 300, our model predicts that \varepsilon should follow a normal distribution with mean = 1.63. We can extract the variance ($\sigma^2$) from our model summary:

```{r, echo = TRUE}
summary(lm.abund)$sigma
```

We can plug these values into the equation we just discussed above:
$$Y_i \sim N(\mu = \beta_0 + \beta_1 X_i +\varepsilon, \sigma^2)$$
$\mu = 3.44 + (-0.006 \times 300) = 1.63$  

$\sigma^2 = 1.51$

This tells us that randomly drawn $Y$ values when water content is $300$ should be on average $1.63$ and have a variance of $1.51$. In other words, at $x = 300$, residuals should follow a normal distribution with $\mu = 1.63$ and $\sigma^2 = 1.51$. At $x = 400$, we get $\mu = 1.02$ and $\sigma^2 = 1.51$, etc.


At a water content of 400, residuals \varepsilon should follow a normal
distribution with parameters $\mu = 3.44 + (-0.006 x 400) = 1.02$ and $\sigma^2 = 1.51$, etc. Each $Y$ value is modeled using a normal distribution with a mean that depends on $X_i$, but with a variance that is constant $\sigma^2 = 1.51$ across all $X_i$ values. Graphically, this looks like this:

![](images/modelPredic.png)

The four normal distributions on this graph represent the probability of
observing a given Galumna abundance for 4 different water content
values. The mean of the normal distribution varies as a function of
water content (hence $\mu$  decreases with water content), but $\sigma^2$ is always 1.51 (i.e. the variance is homogeneous across all values of $X$).

This model is inappropriate for at least two reasons:

**1.  Values are on average further from the regression line at low water content values.** That is, there is more residual variance around the predicted values for low values of $X$, such that $\varepsilon$ varies as a function of $X$, thus violating the assumption of homoscedasticity. It makes no sense to use a constant value of $\sigma^2$: the normal distributions used to predict $Y$ at low values of $X$ should ideally be wider (have a larger $\sigma^2$) than normal distributions used to predict $Y$ at large $X$ values, but linear models do not permit this.

**2. The residuals do not follow a normal distribution with constant variance across all values of** $X$. The variance of the residuals' distribution changes as a function of $X$ (see the spread of the data points around the trendline!).

**3. The predicted values do not make sense, given the observations.** Our response variable is abundance, which can only take integer values. Yet, at water content = 300, the abundance value that our model predicts to be the most probable is 1.63! We know that the probability of observing 1.63 individuals at water content = 300 is actually zero, as is the probability of observing any fraction (non-integers). Our predicted values should be modeled using a distribution that only predicts integers, rather than a continuous distribution like the normal distribution. This is a very common problem, as biological data often follows one of the myriad other statistical distributions besides the normal distribution.

### So, what do we do now? Transform our data?

Very often, data will not "behave" and will violate the assumptions we have seen, showing evidence for **non-normality** and/or **heteroskedasticity**.

We have been told to **transform** our data using logarithmic, square-root, and cosine transformations to get around these problems. Unfortunately, transformations not always work and come with a few drawbacks:

**1.**  They change the response variable (!), making interpretation challenging;
**2.**  They may not simulateneously improve linearity and homogeneity of variance;
**3.**  The boundaries of the sample space change.

For instance, our simple linear model:

$$Y_i = \beta_0 + \beta_1X_i + \varepsilon$$

looks like this under a log-transform:

$$E(\log{Y_i}) = \beta_0 + \beta_1X_i $$

It is, of course, much less intuitive to interpret that for every $300$ units increase in water content, Galumna abundance takes the form of $\log(1.63)$...

Thankfully, the **normal distribution is not our only option!**
