# Reviewing linear models

Much of our research focuses on investigating how patterns we observe can be explained by predictive variables.

We are often looking for a function $f$ that can explain a response variable ( $Y$ ) *in terms of* one ( $X_1$ ) or many other predictors ( $X_2$, $X_3$, $...$ , $X_n$ ):

$$Y = f(X_1)$$
The combination of predictive variables we have sampled will *never* fully explain $Y$. Because of this, there is always *unpredictable disturbance* in our models, *i*.*e*. the error $\epsilon$. As such, the error is an irrevocable part of our function:

$$Y = f(X_1, \epsilon)$$
In [Workshop 4](https://qcbsrworkshops.github.io/workshop04/pres-en/workshop04-pres-en.html#1), we have learned how to use **general linear models** as $f(\cdot)$ to describe the relationship between variables. They were: the $t$-test, the analysis of variance (or, ANOVA), the linear regression (both simple, with one predictor, and multiple, with more than one predictor), and the analysis of covariance (ANCOVA).

## General linear models

### Definition 

The general form of our function $Y = f(X_1)$ as a linear function can be represented by:

$$Y = \beta_0 + \beta_1X_i + \varepsilon$$

where:

$Y_i$ is thepredicted value of a response variable

$\beta_0$ is the *unknown coefficient* **intercept**

$\beta_1$ is the *unknown coefficient* **slope**

$X_i$ is the value for the explanatory variable

$\varepsilon_i$ is the model residual drawn from a normal distribution with a varying mean but a constant variance.

### Assumptions

**Linear models** only produce unbiased estimators (i.e. are only reliable) if they follow certain assumptions. Most importantly:

1\. The population can be described by a linear relationship:

$$Y = \beta_0 + \beta_1X_i + \varepsilon$$

2\. The error term $\varepsilon$ has the same variance given any value of the explanatory variable (i.e. homoskedasticity), and the error terms are not correlated across observations (i.e. no autocorrelation):

$$\mathbb{V}{\rm ar} (\epsilon_i | \mathbf{X} ) = \sigma^2_\epsilon,\ \forall i = 1,..,N$$
and, 

$$\mathbb{C}{\rm ov} (\epsilon_i, \epsilon_j) = 0,\ i \neq j$$

3\. And, the residuals are normal:

$$\boldsymbol{\varepsilon} | \mathbf{X} \sim \mathcal{N} \left( \mathbf{0}, \sigma^2_\epsilon \mathbf{I} \right)$$


```{r echo = FALSE, fig.width = 8, fig.height = 8}
# Set the coefficients:
N = 50
beta_0 = 1
beta_1 = 0.5

# Generate sample data:
x <- 0:N
e <- rnorm(mean = 0, sd = 1.5, n = length(x))
y <- beta_0 + beta_1 * x + e

# Plot the data
plot(x, y)

# The regression equation:
y_dgp <- beta_0 + beta_1 * x

# Plot regression:
lines(x = x, y = y_dgp, col = "darkgreen", lty = 2)

legend(x = 0, y = 25,
       legend = c(expression(paste("Y = ", beta[0] + beta[1] * X))),
       lty = c(2, 1), lwd = c(1, 1), pch = c(NA, NA), col = c("darkgreen", "blue"))
```


# An example with general linear models

Let us use our prior knowledge on general linear models to explore the relationship between variables within the *Oribatid mite dataset*. 

Let us begin by loading this dataset into `R`:

```{r echo=TRUE}
# Use setwd() to set your working directory

mites <- read.csv('data/mites.csv', 
                  stringsAsFactors = TRUE)
```

The dataset that you just loaded is a subset from the classic 'Oribatid
mite dataset', which has been used in numerous texts (e.g. Borcard,
Gillet & Legendre, *Numerical Ecology with R*), and which is available
in the `vegan` library. 

The Oribatid mite dataset has **70 observations with moss and mite samples** collected Station de Biologie des
Laurentides from the Université de Montréal, within the municipality of Saint-Hippolyte, Québec (Canada). Each sample includes **5** variables of **environmental measurements** and abundance for *Galumna* sp. for each site.

We can peek into the structure and the first six rows of the dataset using the `head()` and `str()` functions:

```{r, echo = TRUE, eval = TRUE}
head(mites)

str(mites)
```

Our first glance into the dataset already allows us to separate potential response variables from potential predictors:

<div class = "split">
<div class = "split-left">

Response variables:

1. Occurrence: `pa` 
2. Abundance: `Galumna`
3. Relative Frequency or Proportion: `prop`
</div>

<div class = "split-right">

Predictive variables:

1. Substract Density: `SubsDens`
2. Water Content: `WatrCont`
3. Substrate: `Substrate`
4. Shrubs Nearby: `Shrub`
5. Topography: `Topo`

</div>
</div>

We can also already ellaborate an initial question: *Could the abundance, occurrence or proportion of Galumna sp. be predicted by environmental features?*

To answer this question, we can think of a variety of functions:

<div class = "split">
<div class = "split-left">

$\text{Abundance} = f(\text{Water content}, \epsilon)$

$\text{Proportion} = f(\text{Water content}, \epsilon)$

$\text{Occurrence} = f(\text{Substrate}, \epsilon)$

$\text{Abundance} = f(\text{Topography}, \epsilon)$

</div>

<div class = "split-right">

$\text{Occurrence} = f(\text{Shrubs Nearby}, \epsilon)$

$\text{Relative Frequency} = f(\text{Topography}, \epsilon)$

$\text{Occurrence} = f(\text{Substract Density}, \epsilon)$

$\text{Abundance} = f(\text{Substrate}, \epsilon)$
]

</div>
</div>

Let us attempt to be more specific and ask **wether *Galumna*'s community values (abundance, occurrence and relative frequency) vary as a function of water content**.

We can begin by representing all three response variables against the predictor:

```{r, fig.width=17, fig.height=6}
par(mfrow = c(1, 3), cex = 1.4)

plot(Galumna ~ WatrCont,
     data = mites, 
     xlab = 'Water content', 
     ylab = 'Abundance')

boxplot(WatrCont ~ pa,
        data = mites, 
        xlab='Presence/Absence',
        ylab = 'Water content')

plot(prop ~ WatrCont, 
     data = mites, 
     xlab = 'Water content', 
     ylab = 'Proportion')
```

Indeed, `Galumna` seems to vary negatively as a function of `WatrCont`, *i*.*e*.
*Galumna* sp. seems to prefer dryer sites.

We can go step further and fit general linear models to test whether `Galumna`, `pa`, and/or `prop` vary as a function of `WatrCont`

```{r, eval = -c(2, 5, 8)}
lm.abund <- lm(Galumna ~ WatrCont, data = mites)
summary(lm.abund)

lm.pa <- lm(pa ~ WatrCont, data = mites)
summary(lm.pa)

lm.prop <- lm(prop ~ WatrCont, data = mites)
summary(lm.prop)
```

```{r}
# Extracting the Pr(>|t|)

summary(lm.abund)$coefficients[, 4]
summary(lm.pa)$coefficients[, 4]
summary(lm.prop)$coefficients[, 4]
```

We can see a strong and significant relationship for all three models, concerning each one of the three response variables!

Nevertheless, we cannot forget the most important step: verifying if the assumptions for these general linear models have not been violated. 

Let us begin by validating these models to make sure that we respect assumptions of linear models, starting with the abundance model.

```{r, echo = TRUE, eval = TRUE, fig.width = 7, fig.height = 7, fig.align = "center"}
plot(Galumna ~ WatrCont, data = mites)
abline(lm.abund)
```

The model does not fit well. It predicts negative abundance values when
WatrCont exceeds 600, which does not make any sense. Also, the model
does poorly at predicting high abundance values at low values of
WatrCont.

```{r, echo = TRUE, eval = TRUE, fig.width = 7, fig.height = 7, fig.align = "center"}
par(mfrow = c(2, 2), cex = 1.4)
plot(lm.abund)
```

![](images/6_diagplots.jpeg)

Diagnostic plots show that the data/model violate assumptions of
homogeneity of variance (the graph on the left shows that residuals are
larger at higher fitted values) and normality (the graph on the right
indicates that residuals are not distributed as expected from a normal
distribution, i.e. many points are far from predicted values given by
the dotted line). Therefore, we need to reject this model, and can't
use it to conclude that Galumna abundance varies as a function of water
content. Looking at diagnostic plots for the presence-absence model and
the proportion model also indicate that these models are inappropriate:

```{r, echo = TRUE, eval = TRUE, fig.width = 7, fig.height = 7, fig.align = "center"}
#Proportion
plot(prop ~ WatrCont, data = mites)
abline(lm.prop)
par(mfrow = c(2, 2), cex = 1.4)
plot(lm.prop)
#Présence/Absence
par(mfrow = c(1, 1), cex = 1.4)
plot(pa ~ WatrCont, data = mites)
abline(lm.pa)
par(mfrow = c(2, 2), cex = 1.4)
plot(lm.pa)
```

It is quite common with biological datasets that assumptions of
homogeneity of variance (homoscedasticity) and normality are not met.
These two assumptions are the main problem with linear models, and the
main reason why we need generalized linear models. Lets revisit the
basic equation for a linear model to better understand where these
assumptions come from. The equation for a linear model is:

y~i~ = β~0~ + β~1~x~i~ + ε~i~, where:

-   y~i~ is the predicted value for the response variable,
-   β~0~ is the intercept of the regression line between y and x,
-   β~1~ is the slope of the regression line between y and x,
-   x~i~ is the value for the explanatory variable,
-   ε~i~ are the residuals of the model, which are drawn from a normal
    distribution with a varying mean but a constant variance.

This last point about ε~i~ is important. This is where assumptions of
normality and homoscedasticity originate. It means that the residuals
(the distance between each observation and the regression line) can be
predicted by drawing random values from a normal distribution. Recall
that all normal distributions have two parameters, μ (the mean of the
distribution) and σ^2^ (the variance of the distribution):

![](images/6_normal_params.jpeg)

In a linear model, μ changes based on values of x (the predictor
variable), but σ^2^ has the same value for all values of Y. Indeed,
another equation to represent linear models is:

y~i~ \~ *N*(μ = β~0~ + β~1~x~i~, σ^2^),

which literally means that any given observation (y~i~) is drawn from a
normal distribution with parameters μ (which depends on the value of
x~i~) and σ^2^.

Predict Galumna abundance at a water content = 300 using this equation
and the linear model that we fitted earlier. You will need values for
β~0~ and β~1~ (regression coefficients) and ε~i~ (the deviation of
observed values from the regression line)

```{r, echo = TRUE, eval = TRUE}
coef(lm.abund)
```

This model predicts that for a water content value of, say, 300, we
should obtain a Galumna abundance of 1.63:

3.44 + (-0.006 x 300) = 1.63.

That is the expected abundance if there was no residual. To get our
predicted values, we need to add ε~i~. This is where we use the normal
distribution. For x = 300, our model predicts that ε~i~ should follow a
normal distribution with mean = 1.63. We can find the σ^2^ value for our
abundance model by extracting it from the model summary:

```{r, echo = TRUE, eval = TRUE}
summary(lm.abund)$sigma
```

We find that sigma is roughly 1.51. We now have all the coefficients
that we need to model Galumna abundance. At a water content of 300,
residuals should follow a normal distribution with parameters μ = 1.63
and σ^2^ = 1.51.

At a water content of 400, residuals ε~i~ should follow a normal
distribution with parameters μ = 3.44 + (-0.006 x 400) = 1.02 and σ^2^ =
1.51, etc. Each y~i~ value is modeled using a different normal
distribution, with a mean that depends on x~i~. The variance of all of
those normal distributions (σ^2^), however, is the same. Function lm()
finds the optimal σ^2^ value that minimizes the total residual sum of
square and uses it for all normal distributions used to model y.
Graphically, this looks like this:

![](images/6_lm_assump.jpeg)

The four normal distributions on this graph represent the probability of
observing a given Galumna abundance for 4 different water content
values. The mean of the normal distribution varies as a function of
water content (hence μ decreases with water content), but σ^2^ is always
1.51 (i.e. the variance is homogeneous across all values of x). This
model is inappropriate for at least two reasons:

1\. Values are on average farther from the regression line at low water
content values. That is, there is more residual variance around the
predicted values for low values of x, such that ε~i~ varies as a
function of x, thus violating the assumption of homoscedasticity. It
makes no sense to use a constant value of σ^2^: the normal distributions
used to predict y at low values of x should ideally be wider (have a
larger σ^2^) than normal distributions used to predict y at large x
values, but linear models do not permit this.

2\. It makes no sense to use a normal distribution to predict y based on
x. Our response variable is abundance, which can only take integer
values. Yet, at water content = 300, the abundance value that our model
predicts to be the most probable is 1.63! We know that the probability
of observing 1.63 individuals at water content = 300 is actually zero,
as is the probability of observing any fraction (non-integers). Our
predicted values should be modeled using a distribution that only
predicts integers, rather than a continuous distribution like the normal
distribution. This is a very common problem, as biological data often
follows one of the myriad other statistical distributions besides the
normal distribution.

Generalized linear models can solve these two problems. Keep reading!