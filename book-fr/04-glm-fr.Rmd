# Modèle linéaire généralisé

Afin d'éviter les biais reliés aux modèles linéaires de base, nous avons besoin de spécifier deux choses : 
1\. une **distribution statistique pour les résidus du modèle**\
2\. **une fonction de lien** pour les valeurs prédites par ce même modèle.\

Pour une régression linéaire d'une variable réponse continue distribuée normalement, l'équation suivante nous permet d'obtenir les
valeurs prédites de la variable réponse :

$μ = βx$

où:\
-$μ$ est la valeur prédite de la réponse variable\
-$x$ est la matrice du modèle (*i.e.* ça représente les variables explicatives)\
-$β$ correspond aux paramètres estimés à partir des données (*i.e.*
l'ordonnée à l'origine et la pente). Le terme $βx$ est appelé le **prédicteur linéaire**. En termes mathématiques, c'est le produit matriciel de la matrice du modèle $x$ et du vecteur des paramètres estimés $β$.

Lorsqu'on crée un modèle linéaire simple avec une variable réponse
continue distribuée normalement, le prédicteur linéaire $βx$ est égal aux valeurs attendues de la variable réponse. *Ceci n'est pas exact si la variable réponse n'est pas distribuée normalement*. Si c'est le cas, il faut appliquer une transformation sur les valeurs prédites $μ$, *i.e.* **une fonction de lien**, pour obtenir une **relation linéaire** entre celles-ci et le prédicteur linéaire :

$$g(\mu_i) = ηi$$

où $g(μ)$ est la fonction de lien des valeurs prédites. Ceci permet
d'enlever la contrainte de distribution normale des résidus. 

Cela transforme les valeurs prédite sur une échelle de 0 à `+Inf`. Par exemple, si la probabilité que je poche le cours est de 0.6, les chances que je ne passe pas le cours est $0.6/(1 − 0.6) = 1.5$. Ceci indique que la probabilité de m'observer couler le cours est 1.5 fois plus grandes que la probabilté que je passe le cours (soit $1.5 × 0.4 = 0.6$). 

Pour compléter notre modèle linéaire, il nous faut une fonction de **variance** qui décrit comment la variance $\text{var}(Y_i)$ dépend de la moyenne:

$$\text{var}(Y_i) = \phi V(\mu)$$

où le **paramètre de dispersion** $phi$ est une constante.

Avec le prédicteur linéaire $βx$, notre modèle linéaire généralisé serait donc:

$$\eta_i = \underbrace{g(\mu)}_{Fonction~lien}  = \underbrace{\beta_0 + \beta_1X_1~+~...~+~\beta_pX_p}_{Composant~linéaire}$$

Alors que pour notre **modèle linéaire général** avec $\epsilon ∼ N(0, σ^2)$ et $Y_i \sim{} N(\mu_i, \sigma^2)$, nous aurions:

1. la fonction lien d'**identité**: 

$$g(\mu_i) = \mu_i$$

2. et, la fonction de variance:

$$V(\mu_i) = 1$$
qui résulte en un modèle linéaire général lorsqu'on ajoute le prédciteur linéaire:

$$\underbrace{g(\mu)}_{Fonction~lien}  = \underbrace{\beta_0 + \beta_1X_1~+~...~+~\beta_pX_p}_{Composant~linéaire}$$

Lorsque la variable réponse est une variable binaire, la fonction de lien est la fonction logit et est représentée par l'équation suivante:

$$\eta_i = \text{logit}(\mu_i) = \log(\frac{\mu_i}{1-\mu_i})$$

Le ratio $μ / 1-μ$ représente la cote (ou les chances) qu'un événement se produise. La transformation log (on appelle maintenant ce ratio le log odds) permet aux valeurs d'être distribuées de `-Inf` à `+Inf`. Les valeurs prédites peuvent ainsi être reliées à un prédicteur linéaire. C'est pour cette raison qu'on appelle ce modèle un modèle **linéaire** généralisé même si la relation ne ressemble pas nécessairement à une \«ligne droite\» !

Dans `R`, on peut estimer un modèle linéaire généralisé avec la fonction `glm()`, qui ressemble beaucoup à la fonction `lm()`, avec l'argument `family` prenant le nom de la **fonction lien** et, la **fonction de variance**:

```{r, eval = FALSE, echo = TRUE}
# This is what the glm() function syntax looks like (don't run this)
glm(formula,
    family = gaussian(link = "identity"),
    data,
    ...)
```

Cette approche s'applique à d'autres distributions!

| Distribution de $Y$ | Nom du fonction lien | Fonction lien | Modèle      | `R` |
|---------------------|--------------------|---------------|------------|-----|
| Normale              | Identité           | $g(\mu) = \mu$  | $\mu = \mathbf{X} \boldsymbol{\beta}$ | `gaussian(link="identity")` | 
| Binomiale            | Logit              | $g(\mu) = \log\left(\dfrac{\mu}{1-\mu}\right)$  | $\log\left(\dfrac{\mu}{1-\mu}\right) = \mathbf{X} \boldsymbol{\beta}$ | `binomial(link="logit")`| 
| Poisson              | Log           | $g(\mu) = \log(\mu)$ | $-\mu^{-1} = \mathbf{X} \boldsymbol{\beta}$ | `poisson(link="log")`|
| Exponentielle              | Inverse négative          | $g(\mu) = -\mu^{-1}$  | $\log(\mu) = \mathbf{X} \boldsymbol{\beta}$ | `Gamma(link="inverse")` |
