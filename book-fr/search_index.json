[["index.html", "Atelier 6: Modèles linéaires généralisés en R Série d’ateliers R du CSBQ Préface 0.1 Code de conduite 0.2 Contributeurs et contributrices 0.3 Contribuez à la série!", " Atelier 6: Modèles linéaires généralisés en R Série d’ateliers R du CSBQ Développé et entretenu par les contributeurs et les contributrices de la Série d’ateliers R du CSBQ1. 2022-03-17 17:18:40 Préface La Série d’ateliers R du CSBQ est une série de 10 ateliers qui guide les participants à travers les étapes nécessaires à l’utilisation de R pour un large éventail d’analyses statistiques pertinentes pour la recherche en biologie et en écologie. Ces ateliers en accès libre ont été créés par des membres du CSBQ, à la fois pour les membres du CSBQ et pour la communauté au sens large. Le contenu de cet atelier a été revu par plusieurs membres du CSBQ. Si vous souhaitez suggérer des modifications, veuillez contacter les coordinateurs de la série actuelle, dont la liste figure sur la page principale de Github 0.1 Code de conduite La Série d’ateliers R du CSBQ et le Symposium R du CSBQ sont des lieux dédiés à fournir un environnement accueillant et favorable à toutes les personnes, indépendamment de leurs origines ou de leur identité. Les participants, les présentateurs et les organisateurs de la série d’ateliers et d’autres activités connexes acceptent le présent code de conduite lorsqu’ils assistent à des activités liées aux ateliers. Nous ne tolérons pas les comportements irrespectueux ou qui excluent, intimident ou gênent les autres. Nous ne tolérons pas la discrimination ou le harcèlement fondés sur des caractéristiques telles que, mais sans s’y limiter, l’identité et l’expression du genre, l’orientation sexuelle, le handicap, l’apparence physique, la taille du corps, la citoyenneté, la nationalité, les origines ethniques ou sociales, la grossesse, le statut familial, les informations génétiques, la religion ou les convictions (ou l’absence de celles-ci), l’appartenance à une minorité nationale, la propriété, l’âge, l’éducation, le statut socio-économique, les choix techniques et le niveau d’expérience. Il s’applique à tous les espaces gérés par l’atelier ou affiliés à celui-ci, y compris, mais sans s’y limiter, les ateliers, les listes de diffusion et les forums en ligne tels que GitHub, Slack et Twitter. 0.1.1 Comportement attendu Tous les participants sont tenus de faire preuve de respect et de courtoisie envers les autres. Toutes les interactions doivent être professionnelles, quelle que soit la plateforme utilisée : en ligne ou en personne. Afin de favoriser un environnement d’apprentissage positif et professionnel, nous encourageons les types de comportements suivants dans tous les événements et plates-formes des ateliers : Utiliser un langage accueillant et inclusif ; Respecter les différents points de vue et expériences ; Accepter avec grâce les critiques constructives ; Se concentrer sur ce qui est le mieux pour la communauté ; Faire preuve de courtoisie et de respect envers les autres membres de la communauté. 0.1.2 Comportements inacceptables Voici quelques exemples de comportements inacceptables de la part des participants à tout événement ou plateforme d’atelier : les commentaires écrits ou verbaux qui ont pour effet d’exclure des personnes sur la base de leur appartenance à un groupe spécifique ; faire craindre à quelqu’un pour sa sécurité, par exemple en le harcelant ou en l’intimidant ; des menaces ou des propos violents dirigés contre une autre personne ; l’affichage d’images sexuelles ou violentes ; l’attention sexuelle non désirée ; les contacts physiques non consensuels ou non désirés ; des insultes ou des rabais ; les blagues sexistes, racistes, homophobes, transphobes, incapables ou d’exclusion ; l’incitation à la violence, au suicide ou à l’automutilation ; la poursuite de l’interaction (y compris la photographie ou l’enregistrement) avec une personne après qu’on - lui a demandé d’arrêter ; la publication d’une communication privée sans consentement. 0.2 Contributeurs et contributrices Développé à l’origine par : Cédric Frenette Dussault, Vincent Fugère, Thomas Lamy, Zofia Taranu A contribué à modifier la présentation : Contribution avec des changements à la documentation écrite : Contribution en signalant des problèmes et en suggérant des modifications: 2022 - 2021 - 2020 Laurie Maynard Pedro Henrique P. Braga Katherine Hébert Alex Arkilanian Mathieu Vaillancourt Esteban Góngora 2019 - 2018 - 2017 Azenor Bideault Willian Vieira Pedro Henrique P. Braga Marie Hélène Brice Kevin Cazelles 2016 - 2015 - 2014 Cédric Frenette Dussault Thomas Lamy Zofia Taranu Vincent Fugère 0.3 Contribuez à la série! En construction La Série d’ateliers R du CSBQ fait partie du Centre de la science de la biodiversité du Québec, et est maintenue par les coordonnateurs et les coordonnatrices de la série, et les membres étudiants diplômés, postdoctoraux et professionnels de la recherche. La liste des contributeurs et des contributrices de cet atelier sont accessiblesici↩︎ "],["objectifs-dapprentissage.html", "Chapitre 1 Objectifs d’apprentissage", " Chapitre 1 Objectifs d’apprentissage Résumé: Les modèles linéaires généralisés sont des outils importants afin de surmonter un problème récurrent des modèles linéaires, c.-à-d. les variables de réponse n’ayant pas une distribution normale des résidus. Dans cet atelier, vous apprendrez les distributions principales en fonction de la nature de la variable de réponse, le concept de fonction de lien, et comment vérifier les suppositions de base de ces modèles. Faire la distinction entre les modèles linéaires généralisés et les modèles généraux linéaires (incluant plusieurs de leurs équations!) Identifier les situations où il est approprié d’utiliser des modèles linéaires généralisés. Tester les hypothèses des modèles linéaires généralisés. Implémenter et executer des modèles linéaires généralisés avec des données binaires, de proportion, et d’abondance. Valider, interpreter and visualiser les résultats de modèles linéaires généralisés. "],["préparez-vous-pour-cet-atelier.html", "Chapitre 2 Préparez-vous pour cet atelier", " Chapitre 2 Préparez-vous pour cet atelier Téléchargez les données pour cet atelier: Mites Faramea Nous utiliserons ces paquets R pour cet atelier: ggplot2 lme4 MASS vcdExtra bbmle DescTools GlmSimulatoR cplm Télechargez les paquets R: install.packages(c(&quot;ggplot2&quot;, &quot;MASS&quot;, &quot;vcdExtra&quot;, &quot;bbmle&quot;, &quot;DescTools&quot;, &quot;GlmSimulatoR&quot;, &quot;cplm&quot;)) ## also installing the dependencies &#39;assertthat&#39;, &#39;statmod&#39;, &#39;tweedie&#39;, &#39;biglm&#39;, &#39;reshape2&#39; Ensuite, chargez les: library(ggplot2) library(MASS) library(vcdExtra) library(bbmle) library(DescTools) library(GlmSimulatoR) library(cplm) "],["révision-des-modèles-linéaires.html", "Chapitre 3 Révision des modèles linéaires 3.1 Modèles linéaires généraux 3.2 Un exemple avec les modèles linéaires généraux 3.3 Un exemple avec des vrais données des modèles linéaires généraux 3.4 Les conditions d’application d’un modèle linéaire", " Chapitre 3 Révision des modèles linéaires La plupart de nos recherches tentent d’expliquer des tendances dans nos observations à l’aide de variables prédictives. Nous cherchons souvent une fonction \\(f\\) qui explique une variable réponse ( \\(Y\\) ) en fonction d’une ( \\(X_1\\) ) ou de plusieurs variables prédictives ( \\(X_2\\), \\(X_3\\), \\(...\\), \\(X_n\\) ): \\[Y = f(X_1)\\] L’ensemble de variables prédictives que nous avons mesuré ne pourra jamais complètement expliquer notre variable \\(Y\\). Il y a une variation imprévisible dans nos modèles, i.e. l’erreur \\(\\epsilon\\), qui fera toujours partie de notre fonction: \\[Y = f(X_1, \\epsilon)\\] Dans l’atelier 4, nous avons appris comment utiliser les modèles linéaires généraux pour décrire la relation entre variables. Ces modèles comportent les test de \\(t\\), les analyses de variances (ANOVA), les régressions linéaires (simple ou avec plusieurs variables prédictrices) et les analyses de covariance (ANCOVA). 3.1 Modèles linéaires généraux 3.1.1 Définition La formule générale de notre fonction linéaire \\(Y = f(X_1)\\) serait représentée comme: \\[Y = \\beta_0 + \\beta_1X_i + \\varepsilon\\] où: \\(Y_i\\) est la valeur prédite de la variable réponse \\(\\beta_0\\) est le coefficient inconnu de l’ordonnée à l’origine \\(\\beta_1\\) est le coefficient inconnu de la pente \\(X_i\\) est la valeur de la variable explicative \\(\\varepsilon_i\\) représente les résidus du modèle obtenus d’une distribution normale de moyenne 0 et de variance constante (qui est à estimer). 3.1.2 Conditions d’utilisation Nous avons aussi appris que les modèles linéaires produisent seulement des estimateurs non-biaisés (c’est-à-dire, sont seulement fiables) si ils suivent quelques conditions. Notamment: 1. La population peut être décrite par une relation linéaire: \\[Y = \\beta_0 + \\beta_1X_i + \\varepsilon\\] 2. Le terme d’erreur \\(\\varepsilon\\) a la même variance quelque soit la valeur de la variable explicative (c’est-à-dire, l’homoscédasticité), et les termes d’erreur ne sont pas corrélés entre les observations (donc, il n’y a pas d’autocorrélation). \\[\\mathbb{V}{\\rm ar} (\\epsilon_i | \\mathbf{X} ) = \\sigma^2_\\epsilon,\\ \\forall i = 1,..,N\\] et, \\[\\mathbb{C}{\\rm ov} (\\epsilon_i, \\epsilon_j) = 0,\\ i \\neq j\\] 3. Les résidus suivent une distribution normale: \\[\\boldsymbol{\\varepsilon} | \\mathbf{X} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\sigma^2_\\epsilon \\mathbf{I} \\right)\\] Les estimations d’un modèle général linéaire telles que \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\) assumemt que les données sont générées selon les conditions présentées. 3.2 Un exemple avec les modèles linéaires généraux Simulons 250 unités d’observation qui satisfaient nos conditions d’application: \\(\\epsilon_i \\sim \\mathcal{N}(0, 2^2), i = 1,...,250\\). nSamples &lt;- 250 ID &lt;- factor(c(seq(1:nSamples))) PredVar &lt;- runif(nSamples, min = 0, max = 50) simNormData &lt;- data.frame(ID = ID, PredVar = PredVar, RespVar = (2 * PredVar + rnorm(nSamples, mean = 0, sd = 2))) lm.simNormData &lt;- lm(RespVar ~ PredVar, data = simNormData) layout(matrix(c(1, 2, 3, 4), 2, 2)) plot(lm.simNormData) Ces graphiques permettent de vérifier les conditions d’application de la linéarité et l’homoscédasticité. Le graphique QQ permet la comparaison des résidus avec une distribution normal. Scale-location plot (la racine carré des résidus standardisés vs. valeur prédite) est utile pour vérifier l’homoscédasticité; La distance de Cook est une mesure d’influence des observations sur le coefficient de la régression linéaire et permet d’identifier des données aberrantes. Les résidus équivaut \\(Y-\\widehat{Y}\\), soit la valeur observée par la valeur prédite par le modèle. Les données aberrantes sont des observations avec des résidus larges, i.e. la valeur observée (\\(Y\\)) pour un point (\\(x\\)) est très différente de la valeur prédite par le modèle linéaire (\\(\\widehat{Y}\\)). Un point de levier est défini comme une observation \\(Y\\) qui a une valeur de \\(x\\) qui est très éloignée de la moyenne de \\(x\\). Une observation influente est défini par une observation \\(Y\\) qui change la pente de la relation \\(\\beta_1\\). Donc, un point influent va avoir une influence élevée sur la prédiction du modèle. Une méthode pour trouver des observations influentes est de comparer un modèle avec et sans la dite observation. 3.3 Un exemple avec des vrais données des modèles linéaires généraux Utilisons nos connaissances des modèles linéaires généraux pour explorer la relation entre les variables dans le jeu de données de mites Orbatid. Commençons par charger les données dans R: # Use setwd() to set your working directory mites &lt;- read.csv(&quot;data/mites.csv&quot;, stringsAsFactors = TRUE) Le jeu de données que vous avez chargé est un échantillon du jeu de données mites Oribatid (Acari,Oribatei), qui a été utilsé pour plusieurs textes (e.g. Borcard, Gillet &amp; Legendre, Numerical Ecology with R), et est disponible avec le package vegan. Le jeu de données mites contient 70 échantillons de mousses et mites récoltés par la Station de Biologie de l’Université de Montréal. ] provenant de la municipalité de Saint-Hippolyte, Québec (Canada). Il contient 5 variables environmentales, l’abondance de la mite Galumna sp., et l’abondance totale des mites. Nous pouvons examiner la structure du jeu de données avec les fonctions head() and str() functions: head(mites) ## Galumna pa totalabund prop SubsDens WatrCont Substrate Shrub Topo ## 1 8 1 140 0.057142857 39.18 350.15 Sphagn1 Few Hummock ## 2 3 1 268 0.011194030 54.99 434.81 Litter Few Hummock ## 3 1 1 186 0.005376344 46.07 371.72 Interface Few Hummock ## 4 1 1 286 0.003496503 48.19 360.50 Sphagn1 Few Hummock ## 5 2 1 199 0.010050251 23.55 204.13 Sphagn1 Few Hummock ## 6 1 1 209 0.004784689 57.32 311.55 Sphagn1 Few Hummock str(mites) ## &#39;data.frame&#39;: 70 obs. of 9 variables: ## $ Galumna : int 8 3 1 1 2 1 1 1 2 5 ... ## $ pa : int 1 1 1 1 1 1 1 1 1 1 ... ## $ totalabund: int 140 268 186 286 199 209 162 126 123 166 ... ## $ prop : num 0.05714 0.01119 0.00538 0.0035 0.01005 ... ## $ SubsDens : num 39.2 55 46.1 48.2 23.6 ... ## $ WatrCont : num 350 435 372 360 204 ... ## $ Substrate : Factor w/ 7 levels &quot;Barepeat&quot;,&quot;Interface&quot;,..: 4 3 2 4 4 4 4 2 3 4 ... ## $ Shrub : Factor w/ 3 levels &quot;Few&quot;,&quot;Many&quot;,&quot;None&quot;: 1 1 1 1 1 1 1 2 2 2 ... ## $ Topo : Factor w/ 2 levels &quot;Blanket&quot;,&quot;Hummock&quot;: 2 2 2 2 2 2 2 1 1 2 ... Notre première vue su jeu de données nous permet déjà de séparer les variables potentielles en variables réponses ou variables prédictrices: Variables réponses: Occurrence: pa Abondance: Galumna Fréquence relative ou Proportions: prop Variables prédictrices: Densité du substrat: SubsDens Contenu en eau (du sol): WatrCont Substrat: Substrate Arbustes: Shrub Topographie: Topo Quelles questions pouvons-nous poser avec ces variables? Est-ce que l’environnement permet de prédire l’abondance, l’occurrence, ou la proportion de Galumna sp.? Pour répondre à ces questions nous pouvons élaborer plusieurs fonctions: \\(\\text{Abondance} = f(\\text{Contenu en eau}, \\epsilon)\\) \\(\\text{Proportion} = f(\\text{Contenu en eau}, \\epsilon)\\) \\(\\text{Occurrence} = f(\\text{Substrat}, \\epsilon)\\) \\(\\text{Abondance} = f(\\text{Topographie}, \\epsilon)\\) \\(\\text{Occurrence} = f(\\text{Arbustes}, \\epsilon)\\) \\(\\text{Fréquence relative} = f(\\text{Topographie}, \\epsilon)\\) \\(\\text{Occurrence} = f(\\text{Densité du substrat}, \\epsilon)\\) \\(\\text{Abondance} = f(\\text{Substrat}, \\epsilon)\\) ] Pouvons-nous voir une relation entre Galumna et une ou plusieurs des cinq variables environnementales? Essayons en cherchant si la communauté de Galumna (abondance, occurrence and fréquence relative) varie en fonction du contenu en eau. Nous pouvons commencer en représentant les trois varaibles réponses avec la variable prédictrice: plot(Galumna ~ WatrCont, data = mites, xlab = &quot;Water content&quot;, ylab = &quot;Abundance&quot;) boxplot(WatrCont ~ pa, data = mites, xlab = &quot;Presence/Absence&quot;, ylab = &quot;Water content&quot;) plot(prop ~ WatrCont, data = mites, xlab = &quot;Water content&quot;, ylab = &quot;Proportion&quot;) En effet, Galumna semble varier négativement avec la fonction de WatrCont, i.e. Galumna sp. préfèrerait des sites plus secs. Nous pouvons aller plus loin encore en testant un modèle linéaire avec Galumna, pa, ou prop en fonction de WatrCont # Fit the models ## # Abundance model lm.abund &lt;- lm(Galumna ~ WatrCont, data = mites) ## # Presence-absence model lm.pa &lt;- lm(pa ~ WatrCont, data = mites) ## # Proportion model lm.prop &lt;- lm(prop ~ WatrCont, data = mites) Nous pouvons vérifier si la relation est significative avec sa sortie du modèle: # Check the model output with the summary() function summary(lm.abund) ## ## Call: ## lm(formula = Galumna ~ WatrCont, data = mites) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7210 -0.8236 -0.3270 0.3910 6.6772 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.439349 0.555825 6.188 3.98e-08 *** ## WatrCont -0.006045 0.001280 -4.723 1.21e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.514 on 68 degrees of freedom ## Multiple R-squared: 0.247, Adjusted R-squared: 0.2359 ## F-statistic: 22.31 on 1 and 68 DF, p-value: 1.206e-05 summary(lm.pa) ## ## Call: ## lm(formula = pa ~ WatrCont, data = mites) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61320 -0.30889 -0.05498 0.30247 0.80073 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1892142 0.1431306 8.309 6.03e-12 *** ## WatrCont -0.0020263 0.0003296 -6.148 4.68e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3897 on 68 degrees of freedom ## Multiple R-squared: 0.3573, Adjusted R-squared: 0.3478 ## F-statistic: 37.8 on 1 and 68 DF, p-value: 4.677e-08 summary(lm.prop) ## ## Call: ## lm(formula = prop ~ WatrCont, data = mites) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.010208 -0.004927 -0.002056 0.003240 0.049252 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.020e-02 3.294e-03 6.133 4.98e-08 *** ## WatrCont -3.516e-05 7.586e-06 -4.635 1.67e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.008971 on 68 degrees of freedom ## Multiple R-squared: 0.2401, Adjusted R-squared: 0.2289 ## F-statistic: 21.49 on 1 and 68 DF, p-value: 1.665e-05 # Extracting the Pr(&gt;|t|) summary(lm.abund)$coefficients[, 4] ## (Intercept) WatrCont ## 3.981563e-08 1.206117e-05 summary(lm.pa)$coefficients[, 4] ## (Intercept) WatrCont ## 6.030252e-12 4.676755e-08 summary(lm.prop)$coefficients[, 4] ## (Intercept) WatrCont ## 4.977432e-08 1.665437e-05 En effet, il y a une forte relation significative avec les 3 variables réponses! Mais attendez… Nous avons oublier de faire la chose la plus important! Soit de vérifier les conditions d’application du modèle linéaire! 3.4 Les conditions d’application d’un modèle linéaire Validons nos modèles pour s’assurer qu’ils suivent les conditions d’application des modèles linéaires, en commençant avec le modèle d’abondance. # Plot the abundance model plot(Galumna ~ WatrCont, data = mites) abline(lm.abund) Le modèle ne suit pas bien les données observées. Il prédit une abondance négative lorsque WatrCont dépasse 600, ce qui n’est pas réaliste pour notre jeu de données qui ne peut pas avoir de données négatives. Le modèle performe aussi très mal quand il en vient à prédire les valeurs d’abondance à hautes valeurs de WatrCont. Examinons les graphiques de diagnostique: # Diagnostic plots plot(lm.abund) Les graphiques montre que le modèle viole les conditions d’homogénéité de la variance. En effet, le graphique en haut à gauche montre que les résidus sont plus larges lorsque les valeurs prédites sont élevés. Le modèle ne suit pas non plus la conditons de normalité; le graphique en haut à droite indique que les résidus ne suivre pas une courbe normale aux extrémités et beaucoup de points sont très éloignés de la valeur prédite (ligne pointillée). Nous devons rejeter ce modèle et ne pouvons conclure quoi que ce soit sur l’abondace de Galumma selon le contenu en eau. Nous pouvons regarder les graphiques de diagnostique du modèle de fréquence relative et de présence-absence, mais nous observons des problèmes similaires: # Plot the proportion model plot(prop ~ WatrCont, data = mites) abline(lm.prop) # Diagnostic plots plot(lm.prop) # Plot the presence/absence model plot(pa ~ WatrCont, data = mites) abline(lm.pa) # Diagnostic plots plot(lm.pa) Reculons un peu et révisons les conditions d’application des modèles linéaires pour mieux comprendre d’où viennent ces suppositions. Cette équation est: \\[Y_i = \\beta_0 + \\beta_1X_i + \\varepsilon\\] La dernière variable \\(\\varepsilon_i\\) est très importante. C’est de là que les conditions d’application du modèle prennent origine. Pour les modèles linéaires, les résidus \\(\\varepsilon_i\\) (la distance entre une observation et la droite de régression) peuvent être prédits en dessinant une variable aléatoire provenant d’une distribution normale. Rappelez-vous que les distributions normales ont deux paramètres: \\(\\mu\\) (la moyenne de la distribution) et \\(\\sigma^2\\) (la variance de la distribution). Pour un modèle linéaire, \\(\\mu\\) change selon la valeur de \\(X\\) (variable prédictrice), mais \\(\\sigma^2\\) a la même valeurs pour toutes les valeurs de \\(Y\\). Notre modèle linéaire simple peut aussi être écrit de cette façon: \\[Y_i \\sim N(\\mu = \\beta_0 + \\beta_1 X_i +\\varepsilon, \\sigma^2)\\] où \\(N(\\cdot)\\) indique que \\(Y_i\\) provient d’une distribution normale avec le paramètre \\(\\mu\\) (moyenne; qui dépend de \\(x_i\\)) et \\(\\sigma\\) (variance; qui a la même valeur pour toutes les valeurs de \\(Y_i\\)). Qu’arrive-t-il si on fait varier les valeurs de \\(\\mu\\) et \\(\\sigma\\). En faisant varier \\(\\mu\\) alors \\(\\sigma = 5\\) fait changer la moyenen de la distribution. Si nous gardons \\(\\mu = 25\\), en faisant varier \\(\\sigma\\) , la forme e la distribution change, où un petit \\(\\sigma\\) (variance basse) indique que la probabilité est plus élevé autour de la moyenne, alors qu’un \\(\\sigma\\) élevé diffuse la probabilité à traver l’étendue des données. 3.4.1 Prédiction du modèle Lorsque les conditions d’application du modèle linéairene sont pas rencontrées, les prédiction du modèle deviennent problématiques. Regardons un exemple pour démontrer les problèmes associés avec un modèle mal estimé. Rappel: nous voulons estimer les coefficients inconnus \\(\\beta_0\\) et \\(\\beta_1\\), pour tracer une ligne droite qui prédit chaque valeur de \\(Y\\) en fonction de \\(X\\)! \\[Y_i \\sim N(\\mu = \\beta_0 + \\beta_1 X_i +\\varepsilon, \\sigma^2)\\] Prédisons l’abondance de Galumna pour un contenu en eau = 300 avec notre modèle linéaire général. Quels sont les paramètres de la distribution normale utilisée pour modéliser \\(Y\\) quand le contenu en eau est \\(300\\)? Nous commençons par obtenir les valeurs de \\(\\mu\\) and \\(\\sigma^2\\) pour une distribution correspondant à notre modèle. Pour obtenir les coefficient de nos modèles, on peut utiliser la fonction coef(): # Extract model coefficients coef(lm.abund) ## (Intercept) WatrCont ## 3.439348672 -0.006044788 Ces coéfficients nous permettrais de prédire l’abondance de Galumna s’il n’y avait pas d’erreur. Cepenant, nous savons que l’erreur est irrévocable pour notre modèle. Pour avoir nos valeurs prédites, nous avons donc besoin d’ajouter . C’est ici que nous utilisons la distribution normale! Pour \\(X\\) = 300, notre modèle prédit que devrait suivre une distribution normale avec une moyenne = 1.63. Nous pouvons extraire la variance (\\(\\sigma^2\\)) avec le sommaire du modèle: # Extract variance from the model summary summary(lm.abund)$sigma ## [1] 1.513531 Nous pouvons intégrer ces valeurs avec l’équation du modèle: \\[Y_i \\sim N(\\mu = \\beta_0 + \\beta_1 X_i +\\varepsilon, \\sigma^2)\\] \\(\\mu = 3.44 + (-0.006 \\times 300) = 1.63\\) \\(\\sigma^2 = 1.51\\) Ceci nous indique que des valeurs de \\(Y\\) générées aléatoirement lorsque le contenu en eau = \\(300\\) devrait être \\(1.63\\) en moeynne et avoir un variance de \\(1.51\\). À \\(x = 300\\), résidus devrait suivre une distribution normale avec \\(\\mu = 1.63\\) et \\(\\sigma^2 = 1.51\\). À \\(x = 400\\), nous avons \\(\\mu = 1.02\\) et \\(\\sigma^2 = 1.51\\), etc. Lorsque le contenu en eau = 400, résidus devrait suivre une distribution normale dont les paramètres \\(\\mu = 3.44 + (-0.006 x 400) = 1.02\\) et \\(\\sigma^2 = 1.51\\), etc. Chaque valeur de \\(Y\\) est modélisé selon une distribution normale avec une moyenne qui dépend de \\(X_i\\), mais avec la variance qui est constante \\(\\sigma^2 = 1.51\\) pour toutes les valeurs de \\(X_i\\). Sur un graphique, cela ressemblerait à: Les quatre distributions normales (en orange) sur ce graphique représentent la probabilité d’observer une valeur d’abondance de Galumna donnée pour quatre valeurs différentes de contenu en eau. La moyenne de la distribution normale varie selon une fonction du contenu en eau (donc \\(\\mu\\) diminue avec le contenu en eau), mais \\(\\sigma^2\\) est toujorus = 1.51 (i.e. la variance est homogène pour toutes les valeurs de \\(X\\)). Ce modèle est innaproprié pour au moins deux raisons: 1. Les valeurs sont en moyenne, plus éloignée de la pente à une valeur de X basse qu’à une valeur de X élevée, ce qui indique que la variance (σ2) n’est pas homogène. Il y a plus de variance résiduelle autour des valeurs prédites à une valeurs basse de \\(X\\), d’une façon que \\(\\varepsilon\\) varie en fonction de \\(X\\), et ainsi violant la condition de l’homoscédasticité. Ce n’est pas réaliste d’utiliser une valeur de \\(\\sigma^2\\) constante: la distribution normale utilisée pour prédire \\(Y\\) à de faibles valeurs de \\(X\\) devrait idéalement être plus large (une variance \\(\\sigma^2\\) plus large) que la distribution normale utilisée pour prédire \\(Y\\) pour de grandes valeurs de \\(X\\), mais le modèle linéaire ne permet pas cela. 2. Les résidus ne suivent pas une distribution normale avec une variance constante pour toutes les valeurs de \\(X\\). La distribution de la variance des résidus changent selon une fonction de \\(X\\) (observer l’étendue des données aoutut de la ligne de tendance!). 3. Les valeurs prédites ne font pas de sens, selon les observations données. Notre variable réponse est l’abondance, qui est une variable discrète. Pourtant, pour un contenu en eau = 300, la valeur d’abondance que notre modèle prédit comme étant la plus probable d’observer est 1.63! Nous savons que la probabilité d’observer 1.63 individuals pour une contenu en eau = 300 est actuellement de 0, puisque la probabilité d’observer n’importe quelle fraction (non-discrète) est impossible. Nos valeurs prédites devraient être modélisées en utilisant une distribution qui prédit seulement avec des variables discrètes, plutôt que continus. Ceci est un problème commun, puisque les données biologiques suivent souvent une myriades d’autres distributions statistiques autre que la distribution normale. 3.4.2 Que faire? Transformer nos données? Très souvent, nos données ne vont pas se comporter adéquatement et vont violer les conditions d’applications, ce qui indique la non-normalité et hétéroscédasticité. Certains vous dirons de transformer vos données avec un logarithme, la racine carré ou un cosine remédier à ce problème. Malheureusement, les transformations ne marhcent pas toujours et viennent souvent avec des inconvénients: 1. Cela change la variable réponse (!), rendant l’interprétation difficile; 2. Les transformations ne vont pas toujours améliorer la linéarité et l’homogénéité de la variance; 3. Les limites spatiales de l’échantillon changent. Par exemple, notre modèle linéaire simple: \\[Y_i = \\beta_0 + \\beta_1X_i + \\varepsilon\\] ressemble à ceci lorsqu’on le transforme avec un logarithme: \\[E(\\log{Y_i}) = \\beta_0 + \\beta_1X_i \\] C’est, de toute évidence, moins intuitif à interpréter pour chaque augmentation de \\(300\\) unités en contenu en eau, l’abondance de Galumna prend la forme de \\(\\log(1.63)\\)… Heureusement, la distribution normale n’est pas notre seule option! "],["les-distributions-des-données-biologiques.html", "Chapitre 4 Les distributions des données biologiques", " Chapitre 4 Les distributions des données biologiques Les statisticiens ont développé une foule de lois de probabilité (ou distributions) correspondant à divers types de données. Une loi de probabilité donne la probabilité d’observer chaque issue possible d’une expérience ou campagne d’échantillonage (par. ex. abondance = 8 Galumna est un issu d’un échantillonage). Les lois «discrètes» n’incluent que des nombres entiers dans leur ensemble d’issus, alors que les lois «continues» incluent aussi des fractions (par. ex. la loi normale). Toutes les lois ont des paramètres qui déterminent la forme de la loi/distribution (par. ex. μ et σ2 pour la loi normale). Pour un excellent survol des lois de probabilités utiles en écologie, nous vous recommandons le chapitre 4 du livre de Ben Bolker Ecological Models and Data in R. Ici nous ne présenterons que brièvement quelques distributons utiles pour les GLMs. Nous avons déjà vu que notre variable «abondance de Galumna» ne peut prendre comme valeur que des nombres entiers, et serait donc mieux modélisée par une loi discrète qu’une loi continue. Une loi utile pour modéliser les données d’abondance est la loi de «Poisson», nommé en l’honneur du statisticien Siméon Denis Poisson. La loi de Poisson est une loi discrète avec un seul paramètre: \\(\\lambda\\) (lambda), qui détermine et la moyenne et la variance de la distribution (la moyenne et la variance d’une loi de Poisson sont donc égales). Voici 3 exemples de lois de Poisson avec des valeurs différentes de \\(\\lambda\\), ce qui correspond ici au nombre moyen de Galumna retrouvé dans un ensemble fictif d’échantillons: # examples of Poisson distributions with different values # of lambda par(cex = 2) x = seq(1, 50, 1) plot(x, dpois(x, lambda = 1), type = &quot;h&quot;, lwd = 3, xlab = &quot;Frequency of Galumna&quot;, ylab = &quot;Probability&quot;, main = &quot;lambda = 1&quot;) plot(x, dpois(x, lambda = 10), type = &quot;h&quot;, lwd = 3, xlab = &quot;Frequency of Galumna&quot;, ylab = &quot;Probability&quot;, main = &quot;lambda = 10&quot;) plot(x, dpois(x, lambda = 30), type = &quot;h&quot;, lwd = 3, xlab = &quot;Frequency of Galumna&quot;, ylab = &quot;Probability&quot;, main = &quot;lambda = 30&quot;) Remarquez que lorsque \\(\\lambda\\) est faible (c.-à-d. lorsque la moyenne s’approche de zéro), la distribution est décalée vers la gauche, alors que lorsque \\(\\lambda\\) est élevée, la distribution est symmétrique. La variance augmente aussi avec la moyenne (puisque les deux ont la même valeur), les valeurs prédites sont toujours des nombres entiers, et l’ensemble d’issus d’une loi de Poisson est strictement positif. Toutes ces propriétés sont utiles pour modéliser les données de dénombrement, p.ex. l’abondance d’un taxon, le nombre de graines dans une parcelle, etc. Notre variable mites Galumna semble suivre une loi de Poisson avec une basse valeur de \\(\\lambda\\) (en effet, si nous calculons l’abondance moyenne de Galumna pour l’ensemble des échantillons avec la fonction mean(), nous observons que cette valeur est proche de 0): hist(mites$Galumna) mean(mites$Galumna) ## [1] 0.9571429 La variable mites $pa (présence-absence) prend une autre forme. Cette variable n’inclut que des 0s et des 1s, de telle sorte que la loi de Poisson ne serait pas plus appropriée pour cette variable que la loi normale. hist(mites$pa) Nous avons besoin d’une distribution qui n’inclut dans son ensemble que deux issues possibles: 0 ou 1. La loi de «Bernoulli» est une distribution de la sorte. C’est souvent la première loi de probabilité qu’on nous présente dans les cours de statistiques, pour prédire la probabilité d’obtenir «pile» ou «face» en tirant à... pile ou face. Cette loi n’a qu’un paramètre: p, la probabilité de succès. Nous pouvons utiliser la loi de Bernouilli pour calculer la probabilité d’obtenir l’issue «Galumna présent» (1) vs. «Galumna absent» (0). Voici des exemples de distributions de Bernoulli avec différentes probabilités de présence (\\(p\\)): Nous pouvons calculer le nombre de sites où Galumna est présent par rapport au nombre total de sites pour obtenir un estimé de ce que \\(p\\) pourrait être dans cet exemple: sum(mites$pa)/nrow(mites) ## [1] 0.3571429 \\(p\\) pour la variable mites ‘$pa’ est plus ou moins 0.36, de telle sorte qu’environ deux fois plus de sites montrent l’issu «Galumna absent (0) que l’issu «Galumna présent» (1). Lorsqu’il y a plusieurs épreuves (chacune avec un succès/échec), la loi de Bernoulli se transforme en loi binomiale, qui inclue le paramètre additionel \\(n\\), le nombre d’épeuves. La loi binomiale prédit la probabilité d’observer une certaine proportion de succès, \\(p\\), sur le nombre total d’épreuves, \\(n\\). Un «succès» pourrait être, par exemple, la présence d’un taxon à un site (comme pour Galumna), le nombre d’individus qui survit pendant une année, etc. Voici des exemples de lois binomiales avec \\(n\\) = 50 et trois valeurs différentes de \\(p\\): Remarquez que la loi binomiale est assymétrique et décalée à gauche lorsque \\(p\\) est faible, mais elle est décalée à droite lorsque \\(p\\) est élevé. C’est la différence principale avec la loi de Poisson: l’étendu de la loi binomial a une limite supérieure, correspondant à \\(n\\). Conséquemment, la loi binomiale est utilisée pour modéliser des données lorsque le nombre de succès est donné par un nombre entier, et lorsque le nombre d’épreuves est connu. Nous pouvons utiliser la distribution binomiale pour modéliser nos données de proportion, où chaque mite échantillonée pourrait être considérée comme une épreuve. Si la mite est du genre Galumna, l’épreuve est un succès (1), sinon, c’est un échec (0). Dans ce cas, le nombre d’épreuves \\(n\\) varie pour nos 70 échantillons selon l’abondance totale de mites dans l’échantillon, alors que \\(p\\), la probabilité de succès, est donné par la proportion de Galumna dans chaque échantillon. Pourquoi tout cette discussion à propos des lois de distribution? Parce que n’importe quelle loi peut être utilisée pour remplacer la loi normale lorsqu’on calcule les valeurs estimées dans un modèle linéaire. Par exemple, nous pouvons utiliser la loi de Poisson pour modéliser nos valeurs d’abondance en utilisant l’équation suivante: \\[Y_i \\sim Poisson(\\lambda = \\beta_0 + \\beta_1X_i)\\] Remarquez que \\(\\lambda\\) varie selon \\(x\\) (contenu d’eau), ce qui signifie que la variance dans les résidus variera aussi selon \\(x\\) (car pour la loi de Poisson variance = aussi \\(\\lambda\\)). Ceci veut dire que nous venons de nous défaire de la supposition d’homogénéité de la variance! Aussi, les valeurs estimées seront désormais des nombres entiers plutôt que des nombres décimaux car ils seront tirés de lois de Poisson avec différentes valeurs de \\(\\lambda\\). Ce modèle ne prédiera jamais de valeurs négatives car l’ensemble d’une loi de Poisson est toujours strictement positif. En changeant la distribution des résidus (\\(\\varepsilon\\)) de normale à Poisson, nous avons corrigé presque tous les problèmes de notre modèle linéaire pour l’abondance de Galumna. Ce modèle est presque un GLM de Poisson, qui ressemble à ça: Remarquez que les probabilités d’observer différentes valeurs estimées (en orange) sont maintenant des nombres entiers, et qu’autant la variance que la moyenne de la distribution declinent lorsque \\(\\lambda\\) diminue avec le contenu d’eau. Pourquoi la droite de régression est-elle courbée? Pourquoi est-ce que ce modèle se nomme un «modèle linéaire généralisé»? Continuez votre lecture! "],["modèle-linéaire-généralisé.html", "Chapitre 5 Modèle linéaire généralisé", " Chapitre 5 Modèle linéaire généralisé Afin d’éviter les biais reliés aux modèles linéaires de base, nous avons besoin de spécifier deux choses : 1. une distribution statistique pour les résidus du modèle 2. une fonction de lien pour les valeurs prédites par ce même modèle. Pour une régression linéaire d’une variable réponse continue distribuée normalement, l’équation suivante nous permet d’obtenir les valeurs prédites de la variable réponse : \\(μ = βx\\) où: -\\(μ\\) est la valeur prédite de la réponse variable -\\(x\\) est la matrice du modèle (i.e. ça représente les variables explicatives) -\\(β\\) correspond aux paramètres estimés à partir des données (i.e. l’ordonnée à l’origine et la pente). Le terme \\(βx\\) est appelé le prédicteur linéaire. En termes mathématiques, c’est le produit matriciel de la matrice du modèle \\(x\\) et du vecteur des paramètres estimés \\(β\\). Lorsqu’on crée un modèle linéaire simple avec une variable réponse continue distribuée normalement, le prédicteur linéaire \\(βx\\) est égal aux valeurs attendues de la variable réponse. Ceci n’est pas exact si la variable réponse n’est pas distribuée normalement. Si c’est le cas, il faut appliquer une transformation sur les valeurs prédites \\(μ\\), i.e. une fonction de lien, pour obtenir une relation linéaire entre celles-ci et le prédicteur linéaire : \\[g(\\mu_i) = ηi\\] où \\(g(μ)\\) est la fonction de lien des valeurs prédites. Ceci permet d’enlever la contrainte de distribution normale des résidus. Cela transforme les valeurs prédite sur une échelle de 0 à +Inf. Par exemple, si la probabilité que je poche le cours est de 0.6, les chances que je ne passe pas le cours est \\(0.6/(1 − 0.6) = 1.5\\). Ceci indique que la probabilité de m’observer couler le cours est 1.5 fois plus grandes que la probabilté que je passe le cours (soit \\(1.5 × 0.4 = 0.6\\)). Pour compléter notre modèle linéaire, il nous faut une fonction de variance qui décrit comment la variance \\(\\text{var}(Y_i)\\) dépend de la moyenne: \\[\\text{var}(Y_i) = \\phi V(\\mu)\\] où le paramètre de dispersion \\(phi\\) est une constante. Avec le prédicteur linéaire \\(βx\\), notre modèle linéaire généralisé serait donc: \\[\\eta_i = \\underbrace{g(\\mu)}_{Fonction~lien} = \\underbrace{\\beta_0 + \\beta_1X_1~+~...~+~\\beta_pX_p}_{Composant~linéaire}\\] Alors que pour notre modèle linéaire général avec \\(\\epsilon ∼ N(0, σ^2)\\) et \\(Y_i \\sim{} N(\\mu_i, \\sigma^2)\\), nous aurions: la fonction lien d’identité: \\[g(\\mu_i) = \\mu_i\\] et, la fonction de variance: \\[V(\\mu_i) = 1\\] qui résulte en un modèle linéaire général lorsqu’on ajoute le prédciteur linéaire: \\[\\underbrace{g(\\mu)}_{Fonction~lien} = \\underbrace{\\beta_0 + \\beta_1X_1~+~...~+~\\beta_pX_p}_{Composant~linéaire}\\] Lorsque la variable réponse est une variable binaire, la fonction de lien est la fonction logit et est représentée par l’équation suivante: \\[\\eta_i = \\text{logit}(\\mu_i) = \\log(\\frac{\\mu_i}{1-\\mu_i})\\] Le ratio \\(μ / 1-μ\\) représente la cote (ou les chances) qu’un événement se produise. La transformation log (on appelle maintenant ce ratio le log odds) permet aux valeurs d’être distribuées de -Inf à +Inf. Les valeurs prédites peuvent ainsi être reliées à un prédicteur linéaire. C’est pour cette raison qu’on appelle ce modèle un modèle linéaire généralisé même si la relation ne ressemble pas nécessairement à une «ligne droite» ! Dans R, on peut estimer un modèle linéaire généralisé avec la fonction glm(), qui ressemble beaucoup à la fonction lm(), avec l’argument family prenant le nom de la fonction lien et, la fonction de variance: # This is what the glm() function syntax looks like (don&#39;t # run this) glm(formula, family = gaussian(link = &quot;identity&quot;), data, ...) Cette approche s’applique à d’autres distributions! Distribution de \\(Y\\) Nom du fonction lien Fonction lien Modèle R Normale Identité \\(g(\\mu) = \\mu\\) \\(\\mu = \\mathbf{X} \\boldsymbol{\\beta}\\) gaussian(link=\"identity\") Binomiale Logit \\(g(\\mu) = \\log\\left(\\dfrac{\\mu}{1-\\mu}\\right)\\) \\(\\log\\left(\\dfrac{\\mu}{1-\\mu}\\right) = \\mathbf{X} \\boldsymbol{\\beta}\\) binomial(link=\"logit\") Poisson Log \\(g(\\mu) = \\log(\\mu)\\) \\(-\\mu^{-1} = \\mathbf{X} \\boldsymbol{\\beta}\\) poisson(link=\"log\") Exponentielle Inverse négative \\(g(\\mu) = -\\mu^{-1}\\) \\(\\log(\\mu) = \\mathbf{X} \\boldsymbol{\\beta}\\) Gamma(link=\"inverse\") "],["glm-avec-une-distribution-binomiale.html", "Chapitre 6 GLM avec une distribution binomiale 6.1 GLM avec des données binomiales: lien logit 6.2 Exemple 6.3 Défi 1 6.4 Interpréter la sortie d’une régression logistique 6.5 Pouvoir prédictif et validation du modèle 6.6 Représentation graphique des résultats", " Chapitre 6 GLM avec une distribution binomiale Les variables binaires sont fréquentes en écologie : on observe un phénomène \\(Y\\) ou son «absence». Par exemple, on note souvent la présence ou l’absence d’espèces lors d’études de terrain. Le but est généralement de déterminer si la présence d’une espèce est influencée par différentes variables environnementales. D’autres exemples courants sont la présence/absence d’une maladie au sein d’une population sauvage, l’observation/non-observation d’un comportement spécifique et la survie/mort d’un individu. Habituellement, nous sommes interessés à une question telle que: comment l’occurence d’une espèce varie en fonction des conditions de son environnement? \\[Occurrences = f(Environment)\\] Sous un modèle linéaire, les valeurs attendues peuvent se trouver en dehors de l’étendu de [0, 1] avec la fonction ‘lm()’: # set up some binary data Pres &lt;- c(rep(1, 40), rep(0, 40)) rnor &lt;- function(x) rnorm(1, mean = ifelse(x == 1, 12.5, 7.5), sd = 2) ExpVar &lt;- sapply(Pres, rnor) # linear model with binary data... lm(Pres ~ ExpVar) Lorsqu’on prédit la probabilité d’observer un phénomène Y qui est une variable binaire, la valeur prédite doit se trouver entre 0 et 1 : c’est l’étendue possible d’une probabilité ! 6.1 GLM avec des données binomiales: lien logit Tel que vu dans la section précdente, un modèle de régression qui utilise une variable binaire comme variable réponse est l’un de plusieurs modèles linéaires généralisés (GLM) et est appelé régression logistique ou modèle logit. Un modèle linéaire généralisé est composé de prédicteurs linéaires: \\[\\underbrace{g(\\mu_i)}_{Link~~function} = \\underbrace{\\beta_0 + \\beta_1X_1~+~...~+~\\beta_pX_p}_{Linear~component}\\] Les chances mettre nos valeurs attendues sur une échelle de 0 à +Inf. On prend ensuite les logarithmes, et calcule le logit: \\[\\eta_i = \\text{logit}(\\mu_i) = \\log(\\frac{\\mu_i}{1-\\mu_i})\\] avec \\(\\mu\\) étant la valeur attendue (probabilité que \\(Y = 1\\)), et les valeurs attendues s’étendant de -Inf à +Inf. Dans ‘R’, la présence (ou succès, survie…) est habituellement codée par un 1 et une absence (ou échec, mort…) par un 0. On effectue une régression logistique (ou n’importe quel GLM) à l’aide de la fonction glm(). Cette fonction diffère un peu de la fonction de base lm(), car elle permet de spécifier une distribution statistique autre que la distribution normale. glm(formula, family = ???, # this argument allows us to set a probability distribution! data, ...) Nous avons déjà vu que les variables binaires ne sont pas distribuées normalement (i.e. on observe un pic à 0 et un pic à 1 et rien entre les deux). Dans ‘R’, on spécifie la distribution statistique du modèle avec l’argument family. Pour la régression logistique, on l’indique de la façon suivante : family = 'binomial’. Distribution de \\(Y\\) Nom du fonction lien Fonction lien Modèle R Normale Identité \\(g(\\mu) = \\mu\\) \\(\\mu = \\mathbf{X} \\boldsymbol{\\beta}\\) gaussian(link=\"identity\") Binomiale Logit \\(g(\\mu) = \\log\\left(\\dfrac{\\mu}{1-\\mu}\\right)\\) \\(\\log\\left(\\dfrac{\\mu}{1-\\mu}\\right) = \\mathbf{X} \\boldsymbol{\\beta}\\) binomial(link=\"logit\") Poisson Log \\(g(\\mu) = \\log(\\mu)\\) \\(-\\mu^{-1} = \\mathbf{X} \\boldsymbol{\\beta}\\) poisson(link=\"log\") Exponentielle Inverse négative \\(g(\\mu) = -\\mu^{-1}\\) \\(\\log(\\mu) = \\mathbf{X} \\boldsymbol{\\beta}\\) Gamma(link=\"inverse\") Dans R, nous pouvons donc construire notre GLM binomial avec un lien logit de cette façon: # This is the syntax for a binomial GLM with a logit link glm(formula, family = binomial(link = &quot;logit&quot;), # this is also known as logistic data, ...) 6.2 Exemple Construisons notre premier modèle linéaire généralisé! Ici, nous voulons construire notre modèle en utilisant le jeu de données mites: # setwd(&#39;...&#39;) mites &lt;- read.csv(&quot;data/mites.csv&quot;, header = TRUE) str(mites) ## &#39;data.frame&#39;: 70 obs. of 9 variables: ## $ Galumna : int 8 3 1 1 2 1 1 1 2 5 ... ## $ pa : int 1 1 1 1 1 1 1 1 1 1 ... ## $ totalabund: int 140 268 186 286 199 209 162 126 123 166 ... ## $ prop : num 0.05714 0.01119 0.00538 0.0035 0.01005 ... ## $ SubsDens : num 39.2 55 46.1 48.2 23.6 ... ## $ WatrCont : num 350 435 372 360 204 ... ## $ Substrate : chr &quot;Sphagn1&quot; &quot;Litter&quot; &quot;Interface&quot; &quot;Sphagn1&quot; ... ## $ Shrub : chr &quot;Few&quot; &quot;Few&quot; &quot;Few&quot; &quot;Few&quot; ... ## $ Topo : chr &quot;Hummock&quot; &quot;Hummock&quot; &quot;Hummock&quot; &quot;Hummock&quot; ... Nous pouvons modéliser notre régression logistique avec les doonées de présence de Galumna sp. en relation avec le contenu en eau et la topographie comme s’en suit, en utilisant glm() et l’argument family: logit.reg &lt;- glm(pa ~ WatrCont + Topo, data = mites, family = binomial(link = &quot;logit&quot;)) Pour la sortie du modèle: summary(logit.reg) ## ## Call: ## glm(formula = pa ~ WatrCont + Topo, family = binomial(link = &quot;logit&quot;), ## data = mites) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0387 -0.5589 -0.1594 0.4112 2.0252 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.464402 1.670622 2.672 0.007533 ** ## WatrCont -0.015813 0.004535 -3.487 0.000489 *** ## TopoHummock 2.090757 0.735348 2.843 0.004466 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 91.246 on 69 degrees of freedom ## Residual deviance: 48.762 on 67 degrees of freedom ## AIC: 54.762 ## ## Number of Fisher Scoring iterations: 6 Est-ce que la sortie vous dit quelque chose? Ressemble-t-elle à la sortie de summary.lm()? En effet! Cependant, vous remarquerez qu’il ya des petites différences (p.ex. le paramètre de dispersion) qui sont discutées plus loin dans ce guide! 6.3 Défi 1 Avec le jeu de données bacteria (provenant du package MASS), modélisez la présence de H. influenzae en relation avec le traitement et la semaine du test. Commençez avec le modèle complet, puis réduisez-le pour obtenir le modèle le plus parcimonieux. Chargez le package MASS et le jeu de données bacteria: ## &#39;data.frame&#39;: 220 obs. of 6 variables: ## $ y : Factor w/ 2 levels &quot;n&quot;,&quot;y&quot;: 2 2 2 2 2 2 1 2 2 2 ... ## $ ap : Factor w/ 2 levels &quot;a&quot;,&quot;p&quot;: 2 2 2 2 1 1 1 1 1 1 ... ## $ hilo: Factor w/ 2 levels &quot;hi&quot;,&quot;lo&quot;: 1 1 1 1 1 1 1 1 2 2 ... ## $ week: int 0 2 4 11 0 2 6 11 0 2 ... ## $ ID : Factor w/ 50 levels &quot;X01&quot;,&quot;X02&quot;,&quot;X03&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ trt : Factor w/ 3 levels &quot;placebo&quot;,&quot;drug&quot;,..: 1 1 1 1 3 3 3 3 2 2 ... Ce jeu de données a été créée pour tester la présence de la bactérie H. influenzae chez les enfants avec otitis media dans les territoire nordique de l’Australie. Dr A. Leach a testé les effets d’un un médicament sur 50 enfants ayant des antécédents d’otite moyenne dans le Territoire du Nord de l’Australie. Les enfants ont été choisis aléatoirement pour recevoir le médicament ou un placebo. La présence de H. influenzae a été contrôlée aux semaines 0, 2, 4, 6 et 11 : 30 de ces contrôles étaient manquants et ne sont pas inclus dans ce jeu de données. Cliquez pour voir la solution au Défi 1! # Challenge 1 - Solution # Fit models (full to most parsimonious) model.bact1 &lt;- glm(y ~ trt * week, data = bacteria, family = binomial) model.bact2 &lt;- glm(y ~ trt + week, data = bacteria, family = binomial) model.bact3 &lt;- glm(y ~ week, data = bacteria, family = binomial) # Let&#39;s compare these models using a likelihood ratio test # (LRT). anova(model.bact1, model.bact2, model.bact3, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: y ~ trt * week ## Model 2: y ~ trt + week ## Model 3: y ~ week ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 214 203.12 ## 2 216 203.81 -2 -0.6854 0.70984 ## 3 218 210.91 -2 -7.1026 0.02869 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Which model is the best candidate? En se basant sur ces résultats, on doit choisir le modèle 2 comme celui représentant le mieux le jeu de données. 6.4 Interpréter la sortie d’une régression logistique La sortie du modèle de régression logistique indique que les deux variables explicatives (WatrCont et Topo) sont significatives: # Extracting model coefficients summary(logit.reg)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.46440199 1.670622482 2.672299 0.0075333598 ## WatrCont -0.01581255 0.004535069 -3.486728 0.0004889684 ## TopoHummock 2.09075654 0.735348234 2.843220 0.0044660283 Mais comment interprète-on les coefficients estimés? Rappelez-vous que nous avons effectué une transformation des valeurs prédites par le modèle (i.e. la probabilité que \\(Y = 1\\)), alors il faut utiliser une fonction inverse pour pouvoir interpréter correctement les résultats. 6.4.1 Un exemple avec le lien d’identité Si nous avions utilisé une fonction de lien d’identité, l’interprétation serait beaucoup plus facile. En assumant un résultat binaire \\(y\\) et deux covariables \\(x_1\\) et \\(x_2\\) et une constante, la probabilité d’avoir un résultat favorable ( \\(y = 1\\) ) est donné par: \\[Pr(y_i = 1) = p = g^{-1(\\beta_0 + x_{1i}\\beta_1 + x_{2i}\\beta_2)}\\] où \\(g^{-1}()\\) est la fonction lien d’identité inverse. Pour un lien d’identité, l’interprétation du coefficient \\(\\beta_1\\) est plutôt simple: Pour un incrément d’une unité de \\(x_1\\), \\(\\beta_1\\) donne une différence constante du résultat. \\[\\Delta{y_i} = (\\beta_0 +\\beta_1(\\color{red}{x_{1i} + 1}) + \\beta_2x_{2i}) - (\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})\\] \\[\\Delta{y_i} = \\beta_1\\] 6.4.2 Interprétation avec un lien logit Pour un modèle linéaire logistic avec deux covariables \\(x_1\\) et \\(x_2\\), nous avons: \\[log({\\dfrac{p}{1-p}})=\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i}\\] Ce qui correspong à notre ratio du log de nos cotes! Nous pouvons ainsi utiliser une fonction exponentielle pour réécrire notre modèle et obtenir le ratio des cotes: \\[\\dfrac{p}{1-p}=exp(\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})\\] Si nous voulons ainsi convertir les cotes en probabilité, selon le coefficient \\(\\alpha\\) nous pouvons utiliser la fonction de lien logit inverse (aussi appelé la fonction logistic): \\[ Pr(y_i = 1) = logit^{-1}(\\alpha) = \\dfrac{1}{1 + exp(-\\alpha)} = (\\dfrac{1}{1 + exp(-\\alpha)}) * (\\dfrac{exp(\\alpha)}{exp(\\alpha)}) = \\dfrac{exp(\\alpha)}{exp(\\alpha) + 1}\\] Pour en revenir à notre mode`le, cela nous donne: \\[Pr(y_i = 1) = \\dfrac{exp(\\beta_0 + \\beta_1x_{1i} +\\beta_2 x_{2i})}{1 + exp{(\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})}}\\] Puisque la fonction de lien inverse n’Est pas linéaire, cela reste difficile d’interpréter le coefficient. Cependant, nous pouvons observer ce qui arrive à la différence avec un incrément d’une unité de \\(x_1\\): \\[\\Delta{y_i} = \\dfrac{\\exp(\\beta_0 + \\beta_1(\\color{red}{x_{1i} + 1}) + \\beta_2x_{2i})}{1 + \\exp{(\\beta_0 + \\beta_1(\\color{red}{x_{1i} + 1}) + \\beta_2x_{2i})}} - \\dfrac{\\exp(\\beta_0 + x_{1i}\\beta_1 + \\beta_2x_{2i})}{1 + \\exp{(\\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i})}}\\] \\[\\Delta{y_i} = \\exp(\\beta_1)\\ \\] Lorsque \\(x_1\\) augmente d’une unité, les cotes augmentes par un facteur de \\(\\exp(\\beta_1)\\). à noter que les valeurs des cotes ici sont considérées dans une situation où les autres paramètres sont constants. Avec ceci, nous pouvons finalement interpréter nos coefficients et les résultats de nos modèles! exp(logit.reg$coefficients[2]) ## WatrCont ## 0.9843118 Pour obtenir l’intervalle de confiance sur l’échelle des cotes : exp(confint(logit.reg)[2, ]) ## 2.5 % 97.5 % ## 0.9741887 0.9919435 Lorsque la cote est inférieure à 1, l’interprétation est un peu plus compliquée. Si c’est le case, il faut prendre la valeur inverse de la cote (i.e. 1 divisé par la cote) pour faciliter l’interprétation. L’interprétation revient à dire comment l’observation d’un phénomène est MOINS probable. Pour le contenu en eau du sol, la cote est de 0.984. L’inverse est: \\[\\dfrac{1}{0.984} = 1.0159\\]. Ceci signifie que l’augmentation d’une unité en contenu en eau diminue la vraisemblance d’observer la présence de Galumna sp. de 1.0159. On peut aussi l’exprimer en pourcentage en soustrayant 1 à cette valeur : \\((1.0159 - 1) * 100 = 1.59%\\). Il est 1.59 % moins vraisemblable d’observer Galumna sp. avec une augmentation d’une unité de contenu en eau. Pour se convaincre qu’on fait la bonne interprétation, on peut représenter graphiquement les résultats de la présence de Galumna sp. en fonction du contenu en eau du sol. On voit qu’en moyenne la présence de Galumna sp. est plus élevée lorsque le contenu en eau est faible. Lorsqu’un paramètre estimé est entre 0 et 1 sur l’échelle des cotes, la relation entre la variable réponse et la variable explicative est négative. Si la valeur est supérieure à 1, la relation est positive. Si l’intervalle de confiance inclut la valeur 1, la relation n’est pas significative. Rappelez-vous qu’une valeur de 1 sur l’échelle des cotes signifie que la probabilité d’observer un phénomène Y est la même que celle de ne pas observer ce phénomène (i.e. quand \\(p\\) = 0.5, 0.5/(1-0.5) = 1). Faisons de même avec la topographie: exp(logit.reg$coefficients[1]) ## (Intercept) ## 86.86907 Le paramètre estimé pour la topographie est de 2.091 sur l’échelle des cotes en log. Donc, la probabilité est donnée par : \\(1/(1+1/exp(2.091)) = 0.89\\) ce qui équivaut à \\(1/(1+1/8.09)\\). Prenez note que la cote pour une variable explicative est calculée lorsque les autres variables sont gardées constantes. La topographie a une cote de 8.09. Ceci signifie que la probabilité d’observer Galumna sp. est 8.09 fois plus vraisemblable lorsque la topographie est de type hummock plutôt que blanket. Calculons cette valeur de cote sans utiliser la fonction exp(): On commence avec la valeur de cote pour la topographie du modèle logit.reg: \\(µ/ (1 - µ) = 8.09\\) On réarrange pour isoler \\(µ\\) : \\(µ\\) = 8.09(1 - \\(µ\\)) = 8.09 - 8.09\\(µ\\) 8.09\\(µ\\) + \\(µ\\) = 8.09 \\(µ\\)(8.09 + 1) = 8.09 \\(µ\\) = 8.09 / (8.09 + 1) \\(µ\\) = 1 / (1 + (1 / 8.09)) = 0.89 On obtient le même résultat sans utiliser la fonction logit inverse ! 6.5 Pouvoir prédictif et validation du modèle Une façon simple et intuitive d’estimer le pouvoir explicatif d’un GLM est de comparer la déviance du modèle à celle d’un modèle nul. La déviance peut être vue comme une généralisation du concept de la somme des carrés résiduelle lorsque le modèle est estimé par maximisation de la vraisemblance (i.e. la méthode par laquelle on estime les paramètres d’un GLM). Ceci nous permet de calculer un pseudo-R2, une statistique similaire au R2 dans une régression des moindres carrés (i.e. la méthode utilisée pour estimer les paramètres d’une régression linéaire de base). La forme générique pour calculer un pseudo-R2 est : \\[\\text{pseudo-R}^2 = \\dfrac{\\text{déviance nulle} - \\text{déviance résiduelle}}{\\text{déviance nulle}}\\] où «déviance du modèle nul» est la déviance du modèle nul et «déviance résiduelle» est la déviance résiduelle du modèle d’intérêt.La différence est divisée par la déviance du modèle nul afin de contraindre le pseudo-R2 entre 0 et 1. Le modèle nul correspond à un modèle sans variable explicative. Dans R, on l’indique de la façon suivante : null.model &lt;- glm(Response.variable ~ 1, family = binomial). Une unité de déviance est la mesure de distance entre \\(y\\) et \\(μ\\). \\[{\\displaystyle d(y,y)=0}\\] \\[{\\displaystyle d(y,\\mu )&gt;0\\quad \\forall y\\neq \\mu }\\] La déviance totale \\({\\displaystyle D(\\mathbf {y} ,{\\hat {\\boldsymbol {\\mu }}})}\\) d’une modèle avec ses prédictions \\({\\hat {\\boldsymbol {\\mu }}}\\) de l’observation \\(\\mathbf {y}\\) est la somme de ses unités de déviances : \\[{\\displaystyle D(\\mathbf {y} ,{\\hat {\\boldsymbol {\\mu }}})=\\sum _{i}d(y_{i},{\\hat {\\mu }}_{i})}\\] Maintenant, la déviance d’un modèle estimé \\({\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]\\) peut être défini par sa vraisemblance: \\[D(y,{\\hat {\\mu }})=2{\\Big (}\\log {\\big (}p(y\\mid {\\hat {\\theta }}_{s}){\\big )}-\\log {\\big (}p(y\\mid {\\hat {\\theta }}_{0}){\\big )}{\\Big )}\\] avec \\(\\hat \\theta_0\\) dénotant la valeurs ajustées du paramètre dans le modèle réduit, alors que \\({\\displaystyle {\\hat {\\theta }}_{s}}\\hat \\theta_s\\) dénotes les paramètres ajustés pour le modèle saturé. La déviance résiduelle est définie comme 2 fois le ratio de la vraisemblence en log du modèle saturé comparé au modèle réduit: \\[D(y,{\\hat {\\mu }})=2{\\Big (}\\log {\\big (}p(\\text{modèle saturé}){\\big )}-\\log {\\big (}p(\\text{modèle réduit}){\\big )}{\\Big )}\\] Enfin, la déviance nulle est définie comme 2 fois le ratio de la vraisemblence en log du modèle saturé comparé au modèle réduit (i.e. les variables prédictrices = 1). \\[D(y,{\\hat {\\mu }})=2{\\Big (}\\log {\\big (}p(\\text{modèle saturé}){\\big )}-\\log {\\big (}p(\\text{modèle nul}){\\big )}{\\Big )}\\] Maintenant, nous pouvons le faire dans R. Comparons la déviance de notre modèle (déviance résiduelle) à la déviance d’un modèle nul (déviance nulle). null.model &lt;- glm(response.variable ~ 1, family = binomial) Le modèle saturé (ou complet) contient l’ensemble des variables prédisctrices: full.model &lt;- glm(response.variable ~ ., family = binomial) Les déviances résiduelles et nulles sont déjà stockées dans glm object: # Les déviances résiduelle et nulle sont déjà enregistrées # dans un objet de type glm. objects(logit.reg) ## [1] &quot;aic&quot; &quot;boundary&quot; &quot;call&quot; ## [4] &quot;coefficients&quot; &quot;contrasts&quot; &quot;control&quot; ## [7] &quot;converged&quot; &quot;data&quot; &quot;deviance&quot; ## [10] &quot;df.null&quot; &quot;df.residual&quot; &quot;effects&quot; ## [13] &quot;family&quot; &quot;fitted.values&quot; &quot;formula&quot; ## [16] &quot;iter&quot; &quot;linear.predictors&quot; &quot;method&quot; ## [19] &quot;model&quot; &quot;null.deviance&quot; &quot;offset&quot; ## [22] &quot;prior.weights&quot; &quot;qr&quot; &quot;R&quot; ## [25] &quot;rank&quot; &quot;residuals&quot; &quot;terms&quot; ## [28] &quot;weights&quot; &quot;xlevels&quot; &quot;y&quot; Nous pouvons mainteant utiliser ces déviances pour calculer la valeur du pseudo-R2: # calcule pseudo-R2 pseudoR2 &lt;- (logit.reg$null.deviance - logit.reg$deviance)/logit.reg$null.deviance pseudoR2 ## [1] 0.4655937 Les variables explicatives du modèle expliquent 46.6% de la variabilité de la variable réponse. Un pseudo-R2 de McFadden ajusté, qui pénalise pour le nombre de prédicteurs, peut être calculé comme suit: où K correspond au nombre supplémentaire de prédicteurs par rapport au modèle nul. La qualité d’ajustement des modèles de régression logistique peut être exprimée par des variantes de statistiques pseudo-R2, telles que les mesures de Maddala (1983) ou de Cragg et Uhler (1970). Lorsqu’on parle de régressions logistiques, les valeurs faibles de R2 sont courantes. La fonction R DescTools::PseudoR2() permet de calculer plusieurs pseudo-R2. En spécifiant which = all, calculez toutes les statistiques en même temps. logit.reg &lt;- glm(pa ~ WatrCont + Topo, data = mites, family = binomial(link = &quot;logit&quot;)) DescTools::PseudoR2(logit.reg, which = &quot;all&quot;) ## McFadden McFaddenAdj CoxSnell Nagelkerke AldrichNelson ## 0.4655937 0.3998373 0.4549662 0.6245898 0.3776866 ## VeallZimmermann Efron McKelveyZavoina Tjur AIC ## 0.6674318 0.5024101 0.7064093 0.5114661 54.7623962 ## BIC logLik logLik0 G2 ## 61.5078819 -24.3811981 -45.6229593 42.4835224 fit &lt;- Rsq(object = logit.reg) HLtest(object = fit) # La valeur de p est de 0.9051814. Donc, on ne rejète pas # notre modèle. L&#39;ajustement du modèle est bon. 6.5.1 Défi 2 Évaluez l’ajustement et le pouvoir prédictif du modèle model.bact2. Comment pouvez-vous améliorer le pouvoir prédictif du modèle ? Cliquez pour voir la solution au Défi 2! null.d &lt;- model.bact2$null.deviance resid.d &lt;- model.bact2$deviance bact.pseudoR2 &lt;- (null.d - resid.d)/null.d bact.pseudoR2 ## [1] 0.0624257 C’est très faible! Le pouvoir prédictif pourrait être augmenté en incluant plus de variables explicatives. 6.6 Représentation graphique des résultats Lorsque le modèle a été validé, il peut être utile de représenter les résultats graphiquement. Voici un exemple avec le paquet ggplot2. Revoir l’atelier 3 pour plus d’informations sur ce paquet. ggplot(mites, aes(x = WatrCont, y = pa)) + geom_point() + stat_smooth(method = &quot;glm&quot;, method.args = list(family = binomial), se = TRUE) + xlab(&quot;Contenu en eau&quot;) + ylab(&quot;Probabilité de présence&quot;) + ggtitle(&quot;Probabilité de présence de Galumna sp. en fonction du contenu en eau&quot;) + theme_classic() "],["glm-binomial-avec-des-proportions.html", "Chapitre 7 GLM binomial avec des proportions", " Chapitre 7 GLM binomial avec des proportions Parfois, les données de proportions sont plus similaires à une régression logistique que ce que vous pensez… En comptes discrets, nous pouvons, par exemple, mesurer le nombre de présences d’individus par rapport au nombre total de populations échantillonnées. Nous obtiendrons ainsi un nombre proportionnel de “succès” dans l’observation des individus en divisant les comptes par les comptes totaux. Dans glm(), nous devons fournir des poids a priori si la variable de réponse est la proportion de succès. Les proportions peuvent être codées en fournissant le nombre de succès et des poids a priori dans la fonction: prop.reg &lt;- glm(cbind(Galumna, totalabund - Galumna) ~ Topo + WatrCont, data = mites, family = binomial) summary(prop.reg) ## ## Call: ## glm(formula = cbind(Galumna, totalabund - Galumna) ~ Topo + WatrCont, ## family = binomial, data = mites) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4808 -0.9699 -0.6327 -0.1798 4.1688 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.288925 0.422109 -7.792 6.61e-15 *** ## TopoHummock 0.578332 0.274928 2.104 0.0354 * ## WatrCont -0.005886 0.001086 -5.420 5.97e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 140.702 on 69 degrees of freedom ## Residual deviance: 85.905 on 67 degrees of freedom ## AIC: 158.66 ## ## Number of Fisher Scoring iterations: 5 Les poids peuvent aussi être spécifiés dans glm(): prop.reg2 &lt;- glm(prop ~ Topo + WatrCont, data = mites, family = binomial, weights = totalabund) summary(prop.reg2) ## ## Call: ## glm(formula = prop ~ Topo + WatrCont, family = binomial, data = mites, ## weights = totalabund) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4808 -0.9699 -0.6327 -0.1798 4.1688 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.288925 0.422109 -7.792 6.61e-15 *** ## TopoHummock 0.578332 0.274928 2.104 0.0354 * ## WatrCont -0.005886 0.001086 -5.420 5.97e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 140.702 on 69 degrees of freedom ## Residual deviance: 85.905 on 67 degrees of freedom ## AIC: 158.66 ## ## Number of Fisher Scoring iterations: 5 "],["que-faire-avec-des-données-dabondance.html", "Chapitre 8 Que faire avec des données d’abondance? 8.1 GLM avec une distribution de Poisson 8.2 GLM avec une distribution quasi-Poisson 8.3 GLM avec une distribution binomiale négative 8.4 Représentation graphique du modèle final 8.5 Conclusion sur les GLM avec des données d’abondance", " Chapitre 8 Que faire avec des données d’abondance? Les données discrètes (ou d’abondance) sont caractérisées par: Des valeurs positives: on ne peut compter -7 individus Valeurs discrètes: on ne peut compter 7.56 individus Démontre une variance élevée pour des valeurs élevées Afin d’illustrer l’utilisation des GLMs avec des données d’abondance, nous allons utiliser un nouveau jeux de données: faramea. faramea &lt;- read.csv(&quot;faramea.csv&quot;, header = TRUE) Ce jeux de données s’intéresse à l’espèce d’arbre Faramea occidentalis sur l’île Barro Colorado au Panama. 43 transects ont été utilisés afin de mesurer le nombre d’arbre le long d’un gradient environnemental. Des caractéristiques environnementales, comme l’élévation du terrain et la précipitation, ont aussi été mesurées au niveau de chaque transect. Examinons maintenant à quoi ressemble la distribution du nombre d’arbres par transect. # Histogram of F. occidentalis count data hist(faramea$Faramea.occidentalis, breaks = seq(0, 45, 1), xlab = expression(paste(&quot;Nombre de &quot;, italic(Faramea ~ occidentalis))), ylab = &quot;Fréquence&quot;, main = &quot;&quot;, col = &quot;grey&quot;) Nous pouvons remarquer qu’il n’y a que des valeurs entières et positives. Pour cet exemple, nous voulons tester si l’élévation (un prédicteur à valeurs continues) influence l’abondance de Faramea occidentalis. Etant donné cette spécificité propre aux données de dénombrement, la distribution de Poisson, semble un choix approprié pour modéliser ces données avec l’élévation. 8.1 GLM avec une distribution de Poisson 8.1.1 La distribution de Poisson La distribution de Poisson specifie que la probabilité d’une variable discrète aléatoire \\(Y\\) est donné par: \\[f(y, \\,\\mu)\\, =\\, Pr(Y = y)\\, =\\, \\frac{\\mu^y \\times e^{-\\mu}}{y!}\\] \\[E(Y)\\, =\\, Var(Y)\\, =\\, \\mu\\] où \\(\\mu\\) est le paramètre de la distribution de Poisson La distribution de Poisson est particulièrement appropriée pour modéliser des données de dénombrement car : elle ne spécifie des probabilités que pour des valeurs entières \\(P(y&lt;0) = 0\\), en d’autres termes la probabilité d’observer une valeur négative est nulle la relation entre la moyenne et la variance permet de manipuler des données hétérogènes (e.g. quand la variance dans les données augmente avec la moyenne) Un GLM de Poisson va modéliser la valeur de \\(\\mu\\) comme un fonction de différente variables prédictrices: Étape 1. Nous assumons que \\(Y_i\\) suit une distribution de Poisson avec un moyenne et une variance \\(\\mu_i\\). \\[Y_i ∼ Poisson(\\mu_i)\\] \\[E(Y_i) = Var(Y_i) = \\mu_i\\] \\[f(y_i, \\, \\mu_i) = \\frac{\\mu^{y_i}_i \\times e^{-\\mu_i}}{y!}\\] \\(\\mu_i\\) est le nombre attendu d’individus. Étape 2. Nous spécifions le prédicteur linéaire du modèle en tant qu’un modèle linéaire. \\[\\underbrace{\\alpha}_\\text{Interception} + \\underbrace{\\beta_1}_\\text{pente de &#39;Élévation&#39;} \\times \\text{Élévation}_i + \\underbrace{\\beta_2}_\\text{pente de &#39;Précipitation&#39;} \\times \\text{Précipitation}_i\\] Étape 3. La fonction de lien entre la moyenne \\(Y_i\\) et le prédicteur linéaire est une fonction logarithmique et peut être écrit de cette façon: \\[log(\\mu_i) = \\alpha + \\beta_1 \\times \\text{Élévation}_i + \\beta_2 \\times \\text{Précipitation}_i\\] ou écrit tel que: \\[\\mu_i = e^{ \\alpha + \\beta_1 \\times \\text{Élévation}_i + \\beta_2 \\times \\text{Précipitation}_i}\\] Ceci démontre que l’impact de chaque variable prédictrice est multiplicatif. En augmentant l’élévation de 1, on augmente \\(μ\\) par un incrément de exp( \\(\\beta_\\text{Élévation}\\)) Nous pouvons aussi l’écrire de cette façon: \\[\\mu_i = e^{\\alpha} \\times e^{\\beta_1^{\\text{Élévation}_i}} \\times e^{\\beta_2^{\\text{Précipitation}_i}}\\] Si \\(β_j = 0\\) alors \\(exp(β_j) = 1\\) et \\(μ\\) n’est pas lié à \\(x_j\\). Si \\(β_j &gt; 0\\) alors \\(μ\\) augmente aussi si \\(x_j\\) augmente; si \\(β_j &lt; 0\\) alors \\(μ\\) diminue si \\(x_j\\) augmente. 8.1.2 GLM de Poisson dans R Pour ajuster un GLM avec une distribution de Poisson sous R, il suffit de spécifier family = poisson dans la fonction glm(). Par défaut la fonction de lien est la fonction logarithmique. # Fit a Poisson GLM glm.poisson = glm(Faramea.occidentalis ~ Elevation, data = faramea, family = poisson) # this is what makes it a Poisson GLM! Note the default link is log. summary(glm.poisson) ## ## Call: ## glm(formula = Faramea.occidentalis ~ Elevation, family = poisson, ## data = faramea) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3319 -2.7509 -1.5451 0.1139 11.3995 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.7687001 0.1099136 16.092 &lt; 2e-16 *** ## Elevation -0.0027375 0.0006436 -4.253 2.11e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 414.81 on 42 degrees of freedom ## Residual deviance: 388.12 on 41 degrees of freedom ## AIC: 462.01 ## ## Number of Fisher Scoring iterations: 10 Ordonnée à l’origine = \\(\\alpha\\) Élévation = \\(\\beta\\) Le résumé est similaire à celui de la fonction lm ( voir l’atelier 4) et donne les estimations des paramètres. Vous pouvez aussi récupérer les estimations des paramètres à l’aide des fonctions suivantes : # Ordonnée à l&#39;origine summary(glm.poisson)$coefficients[1, 1] ## [1] 1.7687 # pente de elevation summary(glm.poisson)$coefficients[2, 1] ## [1] -0.002737509 À partir d’ici, nous pouvons estimer les déviance résiduelle et nulle. Dans notre modèle, la paramètre inconnu est l’ordonnée à l’origine (\\(\\alpha\\)) et la pente de l’élévation (\\(\\beta\\)): \\[log(\\mu_i) = 1.769 - 0.0027 \\times \\text{Élévation}_i\\] qui peut aussi être écrit comme: \\[\\mu_i = e^{1.769 - 0.0027 \\times \\text{Élévation}_i}\\] Rappelez-vous que pour estimer une paramètre inconnu, l’estimation de la vraisemblance maximale est utilisée. La déviance rédiuelle est défini par: \\[\\text{residual deviance} = 2 \\, log(L(y;\\,y)) - 2 \\, log(L(y;\\, \\mu))\\] Pour un GLM de Poisson, la déviance résiduelle devrait être proche du degré de liberté résiduel. Cependant, notre déviance résiduelle est beaucoup plus élevée que le degré de liberté de notre modèle! \\[388.12 &gt;&gt; 41\\] 8.1.3 La validation du modèle et le problème de la surdispersion Un aspect important du résumé se trouve dans les dernières lignes : ## Null deviance: 414.81 on 42 degrees of freedom Residual ## deviance: 388.12 on 41 degrees of freedom L’estimation du maximum de vraisemblance est utilisé afin d’estimer les paramètres. Nous avons déjà mentionné que la déviance est l’équivalent en maximum de vraisemblance des sommes des carrés dans un modèle linéaire. Ici vous pouvez considérer la déviance nulle et la déviance résiduelle comme les équivalents de la somme totale des carrés et de la somme des carrés résiduelle. La déviance résiduelle correspond à deux fois la différence entre la log-vraisemblance de deux modèles : un modèle qui s’ajuste parfaitement aux données (i.e. un modèle saturé) et le modèle que nous voulons tester. Si notre modèle est correct, la distribution de la déviance résiduelle est estimée selon une distribution du χ² avec \\(n\\)-\\(p\\)-1 degrés de liberté (où \\(n\\) correspond au nombre d’observations et \\(p\\) correspond au nombre de variables explicatives). Une implication très importante en ce qui nous concerne est que la déviance résiduelle doit être égale au nombre de degrés de liberté résiduels. Dans notre example, la déviance résiduelle vaut 388.12, tandis que nous avons 41 (43-1-1) degrés de liberté. La déviance est 9.5 fois supérieure au nombre de dégrés de liberté. Le modèle peut alors être qualifié de surdispersé. La surdispersion La surdispersion peut être évaluée à l’aide du paramètre de surdispersion φ qui se mesure donc de la facon suivante : φ = déviance résiduelle / dégrés de liberté résiduels * φ &lt; 1 indique qu&#39;il y a sousdispersion * φ = 1 indique que la dispersion est conforme aux attendus * φ &gt; 1 indique qu&#39;il y a surdispersion Mais pourquoi un GLM présente-il de la surdispersion ? En fait, des modèles GLM sont surdispersés quand la variance dans les données est encore plus grande que ce qu’autorise la distribution de Poisson. Par exemple, cela peut se produire lorsque les données contiennent de nombreux zeros ou beaucoup de très grosses valeurs. Si nous revenons sur la distribution de nos données (ci-dessus) nous pouvons remarquer que ces deux problèmes sont présents et que la distribution de Poisson n’était peut être pas le choix idéal. La surdispersion peut aussi survenir lorsque des variables explicatives ou des termes d’intéractions sont absentes ou bien encore lorsque qu’il y a des problèmes de valeurs aberrantes. La distribution de Poisson peut tenir compte de l’hétérogénéité présente dans des données grace à la relation entre sa moyenne et sa variance. Toutefois dans certains cas la variance augmente bien plus rapidement par rapport à la moyenne si bien que la distribution de Poisson n’est plus appropriée. Pour nous convaincre une dernière fois d’abandonner la distribution de Poisson pour modéliser l’abondance de l’espèce faramea nous pouvons rapidement calculer la moyenne et la variance dans notre jeux de données : mean(faramea$Faramea.occidentalis) ## [1] 3.883721 var(faramea$Faramea.occidentalis) ## [1] 60.24806 Dans la pratique, les GLMs basés sur la distribution de Poisson sont très pratique pour décrire la moyenne µi mais vont sous-estimer la variance dans les données dès qu’il y a de la surdispersion. Par conséquent, les tests qui découlent du modèle seront trop laxistes. Il y a deux moyens de traiter les problèmes de surdispersion que nous allons détailler ci-dessous : corriger la surdispersion en utilisant un GLM quasi-Poisson choisir une nouvelle distribution comme la binomiale négative 8.2 GLM avec une distribution quasi-Poisson Le principe d’un GLM avec une distribution «quasi» Poisson est très simple; le paramètre de surdispersion (φ) est ajouté dans l’équation qui spécifie la variance du modèle : \\[E(Y_i) = \\mu_i\\] \\[Var(Y_i) = φ.\\mu_i\\] Le prédicteur linéaire, ainsi que la fonction de lien (log) restent les mêmes. La seule différence est que \\(φ\\) va être estimé afin de corriger le modèle. Les estimations des paramètres seront eux aussi inchangés, mais leurs écarts-types seront multiplés par \\(√φ\\). Ainsi, certains paramètres qui étaient marginalement significatifs peuvent ne plus le rester. Dans R, la famille quasipoisson peut être utilisée pour traiter ces problèmes de surdispersion (de la même manière la famille quasibinomial peut être utilisée). L’estimation de \\(φ\\) sera donné dans le résumé du modèle GLM quasi-Poisson. Nous pouvons ajuster ce modèle de deux manières différentes : # Option 1, nous ajustons un nouveau modèle GLM # quasi-Poisson glm.quasipoisson = glm(Faramea.occidentalis ~ Elevation, data = faramea, family = quasipoisson) # Option 2, nous actualisons le modèle précédent : glm.quasipoisson = update(glm.poisson, family = quasipoisson) # regardons le résumé summary(glm.quasipoisson) ## ## Call: ## glm(formula = Faramea.occidentalis ~ Elevation, family = quasipoisson, ## data = faramea) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3319 -2.7509 -1.5451 0.1139 11.3995 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.768700 0.439233 4.027 0.000238 *** ## Elevation -0.002738 0.002572 -1.064 0.293391 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 15.96936) ## ## Null deviance: 414.81 on 42 degrees of freedom ## Residual deviance: 388.12 on 41 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 10 En examinant le résumé du modèle, nous pouvons voir que \\(φ\\) est estimé à 15.97. Nous avons donc eu raison de corriger le modèle afin de prendre en compte la surdispersion. Par contre, si nous regardons la significativité du coefficient de regression associé à l’élévation, nous remarquons qu’il n’est plus significatif. Cependant, 15.97 indique que la surdispersion est forte et en général un GLM quasi-Poisson est favorisé lorsque \\(φ\\) est compris entre 1 et 15. Lors que la surdispersion se trouve netre 15 et 20, il est recommandé d’utiliser une distribution binomiale négative à nos données. Deux points sont importants à garder en tête lorsque vous utilisez un GLM quasi-Poisson afin de corriger la surdispersion : Les GLMs quasi-Poisson n’ont pas d’AIC. En effet, la vraisemblance d’un modèle GLM quasi-Poisson ne peut pas être spécifiée et s’appuie sur une procédure de pseudo-maximum de vraisemblance. Par conséquence les GLMs quasi-Poisson n’ont pas d’AIC, et ce critère ne peut pas être utilisé afin de comparer différents modèles. Toutefois des alternatives ont été developpées pour gérer cette situation (e.g. quasi-AIC). La surdispersion influence la comparaison de modèles. En effet, la surdispersion influence la comparaison de deux modèles emboités et doit donc être prise en considération. Par exemple, considérons que nous voulons comparer le modèle GLM1, qui contient \\(p_1\\) paramètres avec le modèle GLM2, qui contient \\(p_2\\) paramètres. GLM1 est emboité dans GLM2 et \\(p2 &gt; p1\\). La comparaison des deux modèles est basées sur le test du rapport des vraisemblances des deux modèles, \\(D_1\\) et \\(D_2\\) respectivement. Si la surdispersion est connue, les déviances doivent être corrigées de manière approprié selon \\(D^* = D/φ\\), et le test final sera basé sur le critère \\(D^*_1 - D^*_2\\) qui est supposé être distributé selon une distribution du \\(χ²\\) avec \\(p_1-p_2\\) degrés de liberté lorsque le modèle GLM1 est correct. Mais dans certain cas \\(φ\\) n’est pas connu. Par exemple, lorsque vous spécifiez un GLM avec un distribution normale. Dans ce cas, φ peut être estimé a posteriori en utilisant la déviance résiduelle du plus gros modèle de telle sorte que le critière de comparaison devienne: \\[\\frac{(D_1-D_2)/(p_2-p_1)}{D_2(n-p_2)}\\] Ce critère est supposé suivre une distribution F avec \\(p_1-p_2\\) et \\(n-p_2\\) degrés de liberté. Testons l’effet de l’élévation par une analyse de déviance. null.model &lt;- glm(Faramea.occidentalis ~ 1, data = faramea, family = quasipoisson) anova(null.model, glm.quasipoisson, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: Faramea.occidentalis ~ 1 ## Model 2: Faramea.occidentalis ~ Elevation ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 42 414.81 ## 2 41 388.12 1 26.686 0.1961 Paramètre de dispesion: 8.3 GLM avec une distribution binomiale négative Un GLM avec une distribution binomiale négative (BN) est utilisé lorsque la surdispersion est très forte. La distribution BN contient un paramètre supplémentaire, \\(k\\), qui va être très utile pour gérer les problèmes de surdispersion. Avant de rentrer dans les détails sur R, voyons rapidement ce qui se cache derrière la distribution BN. En fait, la distribution BN est la combinaison de deux distributions; une distribution de Poisson et une distribution Gamma. La distribution BN définie la distribution d’une variable aléatoire discrète de la même manière qu’une distribution de Poisson mais autorise la variance à être différente de la moyenne. Le mélange entre la distribution de Poisson et la distribution Gamma peut se résumer à l’aide de deux paramètres, \\(µ\\) et \\(k\\) qui spécifie la distribution de la facon suivante : \\[Y \\sim NB(µ, k)\\] \\[E(Y) = µ~et~Var(Y) = µ + µ²/k\\] De cette manière nous pouvons voir comment cette distribution va gérer la surdispersion dans les modèles GLM. Le deuxième terme de la variance de la distribution BN va déterminer le degré de surdispersion. En effet, la surdispersion est indirectement déterminée par \\(k\\), que représente le paramètre de dispersion. Si \\(k\\) est grand (par rapport à \\(μ²\\)), la deuxième partie de la variance, \\(µ²/k\\) va s’approcher de 0, et la variance de Y sera \\(μ\\). Dans ce cas la distribution BN converge vers la distribution de Poisson et vous pourriez tout aussi bien utiliser cette dernière. Par contre, plus \\(k\\) sera petit et plus la surdispersion sera grande. Comme avec toutes les autres distributions, un GLM avec une distribution BN se spécifie en trois étapes. Tout d’abord le modèle fait l’hypothèse que les Yi suivent une distribution BN de moyenne \\(μ_i\\) et de paramètre \\(k\\). \\[Y_i \\sim NB(µ_i, k)\\] \\[E(Y_i) = µ_i~and~Var(Y_i) = µ_i + µ_i²/k\\] Les deux dernières étapes définissent le prédicteur linéaire ainsi que la fonction de lien entre la moyenne des \\(Y_i\\) et le prédicteur linéaire. La fonction de lien utilisée par les GLMs avec une distribution BN est le logarithme ce qui permet de s’assurer que les valeurs prédites soient toujours positives. \\[log(µ_i) = β_0 + βX_i\\] ou \\[µ_i = exp(β_0 + βX_i)\\] Vous pouvez ajuster un GLM avec une distribution BN à l’aide de la fonction glm.nb() du package MASS: glm.negbin = glm.nb(Faramea.occidentalis ~ Elevation, data = faramea) summary(glm.negbin) ## ## Call: ## glm.nb(formula = Faramea.occidentalis ~ Elevation, data = faramea, ## init.theta = 0.2593107955, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.36748 -1.17564 -0.51338 -0.05226 2.25716 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.369226 0.473841 5.00 5.73e-07 *** ## Elevation -0.007038 0.002496 -2.82 0.00481 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(0.2593) family taken to be 1) ## ## Null deviance: 41.974 on 42 degrees of freedom ## Residual deviance: 36.343 on 41 degrees of freedom ## AIC: 182.51 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.2593 ## Std. Err.: 0.0755 ## ## 2 x log-likelihood: -176.5090 Le résumé du modèle et similaire à celui des autres GLMs (e.g. GLMs Poisson). Cependant vous avez maintenant un nouveau paramètre, theta, qui est le paramètre \\(k\\) de la variance de votre distribution. L’écart-type de ce paramètre est aussi fourni, mais attention à son interprétation car l’intervalle n’est pas symétrique. 8.4 Représentation graphique du modèle final Le GLM avec une distribution BN semble être le meilleur modèle pour modéliser nos données. Nous voulons maintenant représenter la relation entre le nombre de Faramea occidentalis et l’élévation. Utilisez summary pour obtenir les paramètres. summary(glm.negbin)$coefficients[1, 1] ## [1] 2.369226 summary(glm.negbin)$coefficients[2, 1] ## [1] -0.007038124 Utilisez les écarts-types pour construire l’intervalle de confiance. summary(glm.negbin)$coefficients[1, 2] ## [1] 0.4738409 summary(glm.negbin)$coefficients[2, 2] ## [1] 0.002496143 pp &lt;- predict(glm.negbin, newdata = data.frame(Elevation = 1:800), se.fit = TRUE) linkinv &lt;- family(glm.negbin)$linkinv ## inverse-link function pframe &lt;- as.data.frame(pp$fit) names(pframe) &lt;- &quot;pred0&quot; pframe$pred &lt;- linkinv(pp$fit) sc &lt;- abs(qnorm((1 - 0.95)/2)) ## Normal approx. to likelihood pframe &lt;- transform(pframe, lwr = linkinv(pred0 - sc * pp$se.fit), upr = linkinv(pred0 + sc * pp$se.fit)) plot(faramea$Elevation, faramea$Faramea.occidentalis, ylab = &quot;Nombre de F. occidentalis&quot;, xlab = &quot;Élévation(m)&quot;) lines(pframe$pred, lwd = 2) lines(pframe$upr, col = 2, lty = 3, lwd = 2) lines(pframe$lwr, col = 2, lty = 3, lwd = 2) Nous pouvons voir que le nombre de Faramea occidentalis diminue de manière significative avec l’élévation. Toutefois, l’intervalle de confiance autour de notre modèle est assez large, notamment à faible élévation. 8.4.1 Défi 3 Utilisez le jeu de données mites! Modélisez l’abondance de l’espèce Galumna en fonction des caractéristiques du substrat (son contenu en eau WatrCont et sa densité SubsDens). Faut-il contrôler pour la surdispersion? Quelles variables explicatives ont un effet significatif? Selectionnez le meilleur modèle! mites &lt;- read.csv(&quot;mites.csv&quot;, header = TRUE) Retirez une variable à la fois et comparez le modèle imbriqué au modèle saturé (ou complet): drop1(MyGLM, test = &quot;Chi&quot;) Spécifiez un modèle imbriqué manuellement, appelez le MyGLM2, et utilisez la fonction anova(): anova(MyGLM, MyGLM2, test = &quot;Chi&quot;) Cliquez pour voir la solution au Défi 3! # GLM Poisson glm.p = glm(Galumna ~ WatrCont + SubsDens, data = mites, family = poisson) # GLM quasi-Poisson glm.qp = update(glm.p, family = quasipoisson) # sélection du modèle drop1(glm.qp, test = &quot;Chi&quot;) ## Single term deletions ## ## Model: ## Galumna ~ WatrCont + SubsDens ## Df Deviance scaled dev. Pr(&gt;Chi) ## &lt;none&gt; 101.49 ## WatrCont 1 168.10 31.711 1.789e-08 *** ## SubsDens 1 108.05 3.125 0.07708 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # ou glm.qp2 = glm(Galumna ~ WatrCont, data = mites, family = quasipoisson) anova(glm.qp2, glm.qp, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: Galumna ~ WatrCont ## Model 2: Galumna ~ WatrCont + SubsDens ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 68 108.05 ## 2 67 101.49 1 6.5657 0.07708 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.5 Conclusion sur les GLM avec des données d’abondance Tous les GLM que nous venons de voir pour modéliser des données de d’abondance (Poisson, quasi-Poisson et BN) utilisent la même relation log-linéaire entre moyenne et prédicteur linéaire (\\(log(µ) = βx\\)). Toutefois ils vont autoriser différentes relations entre la moyenne et la variance et vont aussi se reposer sur des méthodes d’estimation de la vraisemblance différentes. Les GLMs Quasi-Poisson ou BN sont privilégiés afin de traiter la surdispersion. Malheureusement dans certains cas les données peuvent contenir trop de zeros et d’autres modèles seront plus éfficaces pour traiter ces situations. C’est par exemple le cas des «zero-augmented models» (e.g. zero-inflated Poisson; ZIP) qui vont traiter les zéros indépendamment des autres valeurs. "],["autres-distributions.html", "Chapitre 9 Autres distributions", " Chapitre 9 Autres distributions Lorsque la variable réponse est constituée de pourcentages ou de proportions qui ne surviennent pas de succès et échecs de \\(n\\) oui/non (expérience de Bernouilli) il n’est pas possible d’utiliser une distribution binomiale. Dans ce cas, il est souvent conseillé d’effectuer une transformation logit des données et d’utiliser un modèle linéaire (mixte). Voir cet article. Pour des données qui semblent distribuées normalement après une transformation log, log-transformation, il est parfois préférable d’utiliser une distribution log-normale dans un GLM, au lieu de transformer les données en log. Une distribution gamma peut également être utilisée. Elle est similaire à une distribution log-normale, mais est plus polyvalente. La distribution Tweedie est une famille de distributions polyvalentes qui est utile pour les données comportant un mélange de zéros et de valeurs positives (pas nécessairement des abondances). Voir le paquet R Tweedie. Lorsque les données comportent un nombre excessif de zéros, qui proviennent d’un processus différent de celui qui génère les comptages, il convient d’utiliser des distributions «zero-inflated» Poisson ou binomial négatif «zero-inflated». Ces méthodes sont disponibles, dans le paquet glmmADMB, parmi d’autres. "],["sommaire.html", "Chapitre 10 Sommaire", " Chapitre 10 Sommaire Les GLMs constituent une puissante méthode d’analyse pour les données avec des distributions non-normales, telles que des données d’abondance, binomiales et des proportions. Ces modèles ne viennent cependant pas sans leur défi et n’apporte pas une solution à tous les problèmes de distribution de vos données. Lors du prochain atelier, vous verrez les modèles linéaires mixtes (généraux et généralisés). Ces modèles permettent de surmonter un certain nombre de limitations liées aux modèles linéaires traditionnels en modélisant une partie de la variance avec des variables aléatoires. Dans cet atelier, vous apprendrez à déterminer si vous devez utiliser un modèle à effets mixtes pour analyser vos données. "],["ressources-additionnelles.html", "Chapitre 11 Ressources additionnelles", " Chapitre 11 Ressources additionnelles Livres: B. Bolker (2009) Ecological Models and Data in R. Princeton University Press. A. Zuur et al. (2009) Mixed Effects Models and Extensions in Ecology with R. Springer. Articles : Harrison et al. (2018), PeerJ, DOI 10.7717/peerj.4794 Sites internet: GLMM for Ecologists "],["references.html", "Chapitre 12 References", " Chapitre 12 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
